# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from textwrap import dedent

import json
import logging
import uuid
import pathlib

import pandas as pd

from pyrit.common.path import DATASETS_PATH
from pyrit.exceptions.exception_classes import (
    InvalidJsonException,
    pyrit_json_retry,
    remove_markdown_json,
)
from pyrit.models import PromptDataType, PromptRequestPiece, PromptRequestResponse, PromptTemplate
from pyrit.prompt_converter import PromptConverter, ConverterResult, config, utils, prompt_openai, exemplars, components
from pyrit.prompt_target import PromptChatTarget

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

config_path = pathlib.Path(__file__).parent / '_default.yaml'
config = config.load_config(config_path)
sections = [
    "utterances_to_claims",
    "claims_to_inferences",
    "inferences_to_generations",
]
utterances = utils.read_json(pathlib.Path(__file__).parent / config["utterances"])
# load few shot exemplars
few_shot_sources = {}
for section in sections:
    sources = config["few_shot"][section]
    few_shot_sources[section] = {}
    for source in sources:
        # `load_few_shot_sources` is wrapped with experimental singleton so 
        # there will be no duplicated data in memory
        data = exemplars.load_few_shot_source(
            source=source,
            few_shot_dir=pathlib.Path(__file__).parent / config["few_shot"]["data_dir"],
            max_n=500,
            premise_first=section != "inferences_to_generations", # premises are usually longer
            max_hypothesis_toks=6,
            max_sent_toks=50,
        )
        if data is not None:
            few_shot_sources[section][source] = data

class ClaimConverter(PromptConverter):
    def __init__(self, *, converter_target: PromptChatTarget, prompt_template: PromptTemplate = None):
        self.converter_target = converter_target

        # set to default strategy if not provided
        prompt_template = (
            prompt_template
            if prompt_template
            else PromptTemplate.from_yaml_file(
                pathlib.Path(DATASETS_PATH) / "prompt_converters" / "claim_converter.yaml"
            )
        )
        self.number_variations = 1

        self.system_prompt = str(
            prompt_template.apply_custom_metaprompt_parameters(number_iterations=str(self.number_variations))
        )

    async def convert_async(self, *, prompt: str, input_type: PromptDataType = "text") -> ConverterResult:
        """
        Generates variations of the input prompts using the converter target.
        Parameters:
            prompts: list of prompts to convert
        Return:
            target_responses: list of prompt variations generated by the converter target
        """
        if not self.input_supported(input_type):
            raise ValueError("Input type not supported")

        conversation_id = str(uuid.uuid4())

        self.converter_target.set_system_prompt(
            system_prompt=self.system_prompt,
            conversation_id=conversation_id,
            orchestrator_identifier=None,
        )

        # prompt = dedent(
        #     f"Create {self.number_variations} variation of the seed prompt given by the user between the "
        #     "begin and end tags"
        #     "=== begin ==="
        #     f"{prompt}"
        #     "=== end ==="
        # )

        # request = PromptRequestResponse(
        #     [
        #         PromptRequestPiece(
        #             role="user",
        #             original_value=prompt,
        #             converted_value=prompt,
        #             conversation_id=conversation_id,
        #             sequence=1,
        #             prompt_target_identifier=self.converter_target.get_identifier(),
        #             original_value_data_type=input_type,
        #             converted_value_data_type=input_type,
        #             converter_identifiers=[self.get_identifier()],
        #         )
        #     ]
        # )

        produced_claims = prompt_openai.run_pipeline_per_source(
            instance=prompt, #utterances[0],
            target_n=20,
            few_shot_sources=few_shot_sources["utterances_to_claims"],
            engine=config["openai_engine"],
        )

        # select initial claim
        initial_claim = produced_claims[0][0]

        # Generate from GPT-3
        inference_methods = ["pragmatic", "entailment", "paraphrase"]
        inference_sources = []
        if "pragmatic" in inference_methods:
            inference_sources.extend([
                "internal-claims_to_inferences", "internal-hyponym_inferences",
                "imppres-implicature", "imppres-presupposition"]
            )
        if "entailment" in inference_methods:
            inference_sources.extend(["entailmentbank"])
        if "paraphrase" in inference_methods:
            inference_sources.extend(["glue-mrpc", "glue-stsb"])

        inferences = prompt_openai.run_pipeline_per_source(
            instance=initial_claim,
            target_n=20,
            few_shot_sources={
                k: few_shot_sources["claims_to_inferences"][k] for k in inference_sources
                if k in few_shot_sources["claims_to_inferences"]
            },
            engine=config["openai_engine"],
        )

        inferences = [i.capitalize().rstrip(".") if i[0].islower() else i.rstrip(".") for i, _ in inferences]
        inferences = [initial_claim] + inferences
        inferences = list(dict.fromkeys(inferences)) # maintains order while turning into a set

        inferences_selected = inferences[0]
        inf_to_gen_cfg = config["interface"]["inferences_to_generations"]

        generations = []
        num_gen_samples = 20

        result = prompt_openai.run_pipeline_per_source(
            instance=inferences_selected,
            few_shot_sources=few_shot_sources["inferences_to_generations"],
            target_n=num_gen_samples, #//len(inferences_selected),
            one_output_per_exemplar=False,
            exemplars_per_prompt=3,
            engine=config["openai_engine"],
            # **sampling_kwargs,
        )
        generations.extend([(inferences_selected, out) for out, _ in result])
            # pb_gen.progress((i+1)/len(inferences_selected))
        generations = [g for g in generations if "->" not in g[0]+g[1]] # infrequent bug
        generations_df = pd.DataFrame(generations, columns=["claim", "inst"]).drop_duplicates(subset="inst")
        logger.info(f"TestGenie generated {len(generations_df)} statements.")

        tokenizer = components.load_tokenizer("gpt2")
        spacy_model = components.load_spacy()

        insts = generations_df["inst"]
        claims = generations_df['claim']

        truncation_strategies = {
            "half": lambda insts, _: components.truncate_text_by_length(insts, tokenizer, n=0.5),
            "3_toks": lambda insts, _: components.truncate_text_by_length(insts, tokenizer, n=-3),
            "root": lambda insts, _: components.truncate_text_by_root(insts, spacy_model),
            "gpt3": lambda insts, claims: components.truncate_text_with_gpt3(insts, claims, engine=config["openai_engine"]),
            # TODO: truncate based on classifier probability
        }

        selected_truncation = ["half", "3_toks", "root", "gpt3"]
        test_data = []
        for name in selected_truncation:
            truncator = truncation_strategies[name]
            prompts_and_targets = truncator(insts, claims)
            test_data.extend(zip(claims, prompts_and_targets))

        logger.info(f"Created {len(test_data)} tests/prompts. We show 10 random prompts below.")
        test_prompts = [prompt for _, (prompt, _) in test_data]

        target_model = "gpt3/"+config["openai_engine"]
        num_generations = 1
        completion_data = []
        if target_model.startswith("gpt3"):
            engine = target_model.split("/")[1]
            completions = components.generate_from_prompts_gpt3(test_prompts, engine, num_generations)
        else: # implies a huggingface model
            generator = components.load_hf_generator(target_model)
            logger.info("Using target model to complete the prompts...")
            completions = components.generate_from_prompts_hf(test_prompts, generator, num_generations)

        # Build dataframe to label and retrieve existing annotations
        completion_df = components.build_completion_df(test_data, completions, target_model)

        response_msg = completion_df["completion"][0]

        # response_msg = await self.send_variation_prompt_async(request)

        return ConverterResult(output_text=response_msg, output_type="text")

    @pyrit_json_retry
    async def send_variation_prompt_async(self, request):
        response = await self.converter_target.send_prompt_async(prompt_request=request)

        response_msg = response.request_pieces[0].converted_value
        response_msg = remove_markdown_json(response_msg)
        try:
            response = json.loads(response_msg)

        except json.JSONDecodeError:
            raise InvalidJsonException(message=f"Invalid JSON response: {response_msg}")

        try:
            return response[0]
        except KeyError:
            raise InvalidJsonException(message=f"Invalid JSON response: {response_msg}")

    def input_supported(self, input_type: PromptDataType) -> bool:
        return input_type == "text"
