# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from textwrap import dedent

import json
import logging
import uuid
import pathlib

import numpy as np
import pandas as pd

from pyrit.common.path import DATASETS_PATH
from pyrit.exceptions.exception_classes import (
    InvalidJsonException,
    pyrit_json_retry,
    remove_markdown_json,
)
from pyrit.models import PromptDataType, PromptRequestPiece, PromptRequestResponse
from pyrit.prompt_converter import PromptConverter, ConverterResult, config, utils, prompt_openai, exemplars, components, classifiers
from pyrit.prompt_target import PromptChatTarget

logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)

config_path = pathlib.Path(__file__).parent / '_default.yaml'
config = config.load_config(config_path)

clf_config = config["classifier"]

claim_classifier = components.load_classifier(
    classifier_type=clf_config["type"],
    classifier_encoder_model=clf_config["encoder_model"],
    use_differentiable_head=clf_config["setfit"]["use_differentiable_head"],
    num_iterations=clf_config["setfit"]["num_iterations"],
    body_learning_rate=clf_config["setfit"]["body_learning_rate"],
    batch_size=clf_config["setfit"]["batch_size"],
)

sections = [
    "utterances_to_claims",
    "claims_to_inferences",
    "inferences_to_generations",
]
utterances = utils.read_json(pathlib.Path(__file__).parent / config["utterances"])
# load few shot exemplars
few_shot_sources = {}
for section in sections:
    sources = config["few_shot"][section]
    few_shot_sources[section] = {}
    for source in sources:
        # `load_few_shot_sources` is wrapped with experimental singleton so 
        # there will be no duplicated data in memory
        data = exemplars.load_few_shot_source(
            source=source,
            few_shot_dir=pathlib.Path(__file__).parent / config["few_shot"]["data_dir"],
            max_n=500,
            premise_first=section != "inferences_to_generations", # premises are usually longer
            max_hypothesis_toks=6,
            max_sent_toks=50,
        )
        if data is not None:
            few_shot_sources[section][source] = data

class ClaimConverter(PromptConverter):
    def __init__(self, *, converter_target: PromptChatTarget, prompt_template=None):
        self.converter_target = converter_target

        # set to default strategy if not provided
        # prompt_template = (
        #     prompt_template
        #     if prompt_template
        #     else PromptTemplate.from_yaml_file(
        #         pathlib.Path(DATASETS_PATH) / "prompt_converters" / "claim_converter.yaml"
        #     )
        # )
        # self.number_variations = 1

        # self.system_prompt = str(
        #     prompt_template.apply_custom_metaprompt_parameters(number_iterations=str(self.number_variations))
        # )

    async def convert_async(self, *, prompt: str, input_type: PromptDataType = "text") -> ConverterResult:
        """
        Generates variations of the input prompts using the converter target.
        Parameters:
            prompts: list of prompts to convert
        Return:
            target_responses: list of prompt variations generated by the converter target
        """
        if not self.input_supported(input_type):
            raise ValueError("Input type not supported")

        conversation_id = str(uuid.uuid4())

        # self.converter_target.set_system_prompt(
        #     system_prompt=self.system_prompt,
        #     conversation_id=conversation_id,
        #     orchestrator_identifier=None,
        # )

        # prompt = dedent(
        #     f"Create {self.number_variations} variation of the seed prompt given by the user between the "
        #     "begin and end tags"
        #     "=== begin ==="
        #     f"{prompt}"
        #     "=== end ==="
        # )

        # request = PromptRequestResponse(
        #     [
        #         PromptRequestPiece(
        #             role="user",
        #             original_value=prompt,
        #             converted_value=prompt,
        #             conversation_id=conversation_id,
        #             sequence=1,
        #             prompt_target_identifier=self.converter_target.get_identifier(),
        #             original_value_data_type=input_type,
        #             converted_value_data_type=input_type,
        #             converter_identifiers=[self.get_identifier()],
        #         )
        #     ]
        # )

        ######################
        # Statements to claims
        ######################
        produced_claims = prompt_openai.run_pipeline_per_source(
            instance=prompt, #utterances[0],
            target_n=20,
            few_shot_sources=few_shot_sources["utterances_to_claims"],
            engine=config["openai_engine"],
        )

        options = [p.capitalize() for p, _ in produced_claims]
        options = [f"[{i}] " + " " + o for i, o in enumerate(options)]
        selected = input("Select a claim from automatically extracted claims from the example statement.\n" + "\n".join(options))

        # select initial claim        
        # initial_claim = st.text_input(
        #     label=(
        #         "If needed, you can use this field to edit the claim you just selected. "
        #         "For example, you may want to change `People think COVID is a conspiracy` to "
        #         "`COVID is a conspiracy.`"
        #     ),
        #     value=initial_claim,
        #     on_change=_reset_to_step(CURRENT_STEP),
        # )
        initial_claim = produced_claims[int(selected)][0]

        ########################
        # Claims to inferences
        ########################
        # Generate from GPT-3
        inference_methods = ["pragmatic", "entailment", "paraphrase"]
        inference_sources = []
        if "pragmatic" in inference_methods:
            inference_sources.extend([
                "internal-claims_to_inferences", "internal-hyponym_inferences",
                "imppres-implicature", "imppres-presupposition"]
            )
        if "entailment" in inference_methods:
            inference_sources.extend(["entailmentbank"])
        if "paraphrase" in inference_methods:
            inference_sources.extend(["glue-mrpc", "glue-stsb"])

        inferences = prompt_openai.run_pipeline_per_source(
            instance=initial_claim,
            target_n=20,
            few_shot_sources={
                k: few_shot_sources["claims_to_inferences"][k] for k in inference_sources
                if k in few_shot_sources["claims_to_inferences"]
            },
            engine=config["openai_engine"],
        )

        inferences = [i.capitalize().rstrip(".") if i[0].islower() else i.rstrip(".") for i, _ in inferences]
        inferences = [initial_claim] + inferences
        inferences = list(dict.fromkeys(inferences)) # maintains order while turning into a set

        options = [i.capitalize() for i in inferences]
        options = [f"[{i}] " + " " + o for i, o in enumerate(options)]
        selected = input("Select an inference from automatically extracted inferences from the example claim.\n" + "\n".join(options))

        inferences_selected = inferences[int(selected)]
        inf_to_gen_cfg = config["interface"]["inferences_to_generations"]

        generations = []
        num_gen_samples = 20

        result = prompt_openai.run_pipeline_per_source(
            instance=inferences_selected,
            few_shot_sources=few_shot_sources["inferences_to_generations"],
            target_n=num_gen_samples, #//len(inferences_selected),
            one_output_per_exemplar=False,
            exemplars_per_prompt=3,
            engine=config["openai_engine"],
            # **sampling_kwargs,
        )
        generations.extend([(inferences_selected, out) for out, _ in result])
            # pb_gen.progress((i+1)/len(inferences_selected))
        generations = [g for g in generations if "->" not in g[0]+g[1]] # infrequent bug
        generations_df = pd.DataFrame(generations, columns=["claim", "inst"]).drop_duplicates(subset="inst")
        logger.info(f"TestGenie generated {len(generations_df)} statements.")

        sampled_df = generations_df.sample(len(generations), random_state=config["global_seed"])
        unsampled_df = generations_df.drop(sampled_df.index)
        
        sampled_df["label"] = None
        sampled_df["label"][0] = True
        sampled_df["label"][1] = False
        # verified_df = st.data_editor(sampled_df)
        verified_df = pd.DataFrame(sampled_df)
    
        generations_labeled = pd.concat([verified_df, unsampled_df]).sort_index()
        generations_estimated = classifiers.fit_and_predict(claim_classifier, generations_labeled, True)
        generations_estimated = generations_estimated.loc[generations_estimated.pred == 1]

        tokenizer = components.load_tokenizer("gpt2")
        spacy_model = components.load_spacy()

        insts = generations_df["inst"]
        claims = generations_df['claim']

        truncation_strategies = {
            "half": lambda insts, _: components.truncate_text_by_length(insts, tokenizer, n=0.5),
            "3_toks": lambda insts, _: components.truncate_text_by_length(insts, tokenizer, n=-3),
            "root": lambda insts, _: components.truncate_text_by_root(insts, spacy_model),
            "gpt3": lambda insts, claims: components.truncate_text_with_gpt3(insts, claims, engine=config["openai_engine"]),
            # TODO: truncate based on classifier probability
        }

        selected_truncation = ["half", "3_toks", "root", "gpt3"]
        test_data = []
        for name in selected_truncation:
            truncator = truncation_strategies[name]
            prompts_and_targets = truncator(insts, claims)
            test_data.extend(zip(claims, prompts_and_targets))

        logger.info(f"Created {len(test_data)} tests/prompts. We show 10 random prompts below.")
        test_prompts = [prompt for _, (prompt, _) in test_data]

        target_model = "gpt3/"+config["openai_engine"]
        num_generations = 1
        completion_data = []
        if target_model.startswith("gpt3"):
            engine = target_model.split("/")[1]
            completions = components.generate_from_prompts_gpt3(test_prompts, engine, num_generations)
        else: # implies a huggingface model
            generator = components.load_hf_generator(target_model)
            logger.info("Using target model to complete the prompts...")
            completions = components.generate_from_prompts_hf(test_prompts, generator, num_generations)

        # Build dataframe to label and retrieve existing annotations
        completion_df = components.build_completion_df(test_data, completions, target_model)
       
        # Predict which completions are failures
        completion_df["label"] = None
        completion_df["label"][0] = True
        completion_df["label"][1] = False
        completions_estimated = classifiers.fit_and_predict(claim_classifier, completion_df, do_fit=False)

        # calculate margin from random (closer to 0.5->more uncertain)
        completions_estimated["uncertainty"] = 1 - np.abs(0.5 - completions_estimated["prob"])*2
        # ignore already-labeled data
        completions_estimated = completions_estimated.loc[completions_estimated["label"].isna()]

        options = [f"[{i}] " + " " + p for i, p in enumerate(completions_estimated["inst"])]
        # selected = input("Select a generation from automatically extracted generations from the example inference.\n" + "\n".join(options))

        response_msg = "\n".join(options)

        # response_msg = await self.send_variation_prompt_async(request)

        return ConverterResult(output_text=response_msg, output_type="text")

    @pyrit_json_retry
    async def send_variation_prompt_async(self, request):
        response = await self.converter_target.send_prompt_async(prompt_request=request)

        response_msg = response.request_pieces[0].converted_value
        response_msg = remove_markdown_json(response_msg)
        try:
            response = json.loads(response_msg)

        except json.JSONDecodeError:
            raise InvalidJsonException(message=f"Invalid JSON response: {response_msg}")

        try:
            return response[0]
        except KeyError:
            raise InvalidJsonException(message=f"Invalid JSON response: {response_msg}")

    def input_supported(self, input_type: PromptDataType) -> bool:
        return input_type == "text"
