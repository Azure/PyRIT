transfer: True
logfile: ""
progressive_goals: False
stop_on_success: False
tokenizer_paths: ["meta-llama/Llama-2-7b-chat-hf", "mistralai/Mistral-7B-Instruct-v0.1", "meta-llama/Meta-Llama-3-8B-Instruct", "lmsys/vicuna-7b-v1.5"]
tokenizer_kwargs: [{"use_fast": False}, {"use_fast": False}, {"use_fast": False}, {"use_fast": False}]
model_paths: ["meta-llama/Llama-2-7b-chat-hf", "mistralai/Mistral-7B-Instruct-v0.1", "meta-llama/Meta-Llama-3-8B-Instruct", "lmsys/vicuna-7b-v1.5"]
model_kwargs: [{"low_cpu_mem_usage": True, "use_cache": False}, {"low_cpu_mem_usage": True, "use_cache": False}, {"low_cpu_mem_usage": True, "use_cache": False}, {"low_cpu_mem_usage": True, "use_cache": False}]
conversation_templates: ["llama-2", "mistral", "llama-3-8b", "vicuna"]
devices: ["cuda:0", "cuda:1", "cuda:2", "cuda:3"]
