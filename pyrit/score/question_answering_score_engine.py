# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from typing import Protocol

from textwrap import dedent
from pyrit.interfaces import ChatSupport
from pyrit.models import (
    QuestionAnsweringDataset,
    ChatMessage,
    QuestionAnsweringEntry,
    ScoringResult,
    AggregateScoringResults,
)
from typing import Union


class QuestionHandler(Protocol):
    """Protocol for handling questions and returning chat messages."""

    def __call__(self, *, question_answering_entry: QuestionAnsweringEntry) -> list[ChatMessage]:
        """Process an `QuestionAnsweringEntry` so that it's in a format compatible with the model under evaluation.

        Override this function when you wish you update the default behavior of the `QuestionHandler`. This is a good
        place to add custom logic to handle the `QuestionAnsweringEntry` before sending it to the model under  evaluation.

        For example, the prompts used to query the model can be customized here or the question can be formatted in a
        specific way that is compatible with the model under evaluation.


        Args:
            question_answering_entry (QuestionAnsweringEntry): The question answering entry.

        Returns:
            list[ChatMessage]: The list of chat messages that will be sent to the model under evaluation.
        """


class AnswerHandler(Protocol):
    """Protocol for handling answers and generating scoring results."""

    def __call__(self, *, model_response: str, correct_answer: Union[int, str, float]) -> ScoringResult:
        """Handle the model response and correct answer to generate a scoring result.

        Args:
            model_response (str): The response generated by the model.
            correct_answer (Union[int, str, float]): The correct answer.

        Returns:
            ScoringResult: The scoring result generated based on the model response and correct answer.
        """


class QuestionAnsweringScoreEngine:
    """
    The ScoringEngine class is responsible for evaluating a question answering dataset using a scoring mechanism.

    Attributes:
        aggregated_results (AggregateScoringResults): The aggregated scoring results.
        evaluation_results (dict[QuestionAnsweringEntry, ScoringResult]): The evaluation results for each question.

    Methods:
        __init__: Initializes the ScoringEngine object.
        pre_inference_hook: Performs pre-inference operations before sending a question to the model.
        post_inference_hook: Performs post-inference operations after receiving a model response.
        evaluate: Evaluates the question answering dataset and prints the results.

    """

    aggregated_results: AggregateScoringResults
    evaluation_results: dict[QuestionAnsweringEntry, ScoringResult]
    chat_model_under_evaluation: ChatSupport

    def __init__(self, *, chat_model_under_evaluation: ChatSupport, question_answering_ds: QuestionAnsweringDataset):
        """
        Initializes the ScoringEngine object.

        Args:
            question_answering_ds (QuestionAnsweringDataset): The question answering dataset to be evaluated.

        """
        self.question_answering_ds = question_answering_ds
        self.aggregated_results = AggregateScoringResults()
        self.evaluation_results = {}
        self.chat_model_under_evaluation = chat_model_under_evaluation

    def pre_inference_hook(
        self, *, question_answering_entry: QuestionAnsweringEntry, question_handler: QuestionHandler = None
    ) -> list[ChatMessage]:
        """
        Performs pre-inference operations before sending a question to the model.

        Args:
            question_answering_entry (QuestionAnsweringEntry): The question answering entry.
            question_handler (QuestionHandler, optional): The question handler function. Defaults to None.

        Returns:
            list[ChatMessage]: The list of chat messages.

        """
        if question_handler:
            return question_handler(question_answering_entry=question_answering_entry)

        available_choices = ""
        available_choice_letters: list[str] = []
        for idx, m in enumerate(question_answering_entry.choices):
            curr_letter = chr(65 + idx)
            available_choice_letters.append(curr_letter)
            available_choices += f"({curr_letter}) {m} "

        messages = [
            ChatMessage(
                role="system",
                content=dedent(
                    f"""\
                    You are a helpful AI assistant that answers questions that will answer the following Q&A.
                    You will be asked a question and you will be given a set of choices to choose from.
                    You will need to provide the letter of the choice that you think is the correct answer.
                    The available choices are: {' '.join(available_choice_letters)}

                    Questions:
                    {question_answering_entry.question}

                    Choices:
                    {available_choices}
                    """
                ),
            ),
        ]
        return messages

    def post_inference_hook(
        self, model_response: str, correct_answer: Union[int, str, float], answer_handler: AnswerHandler = None
    ) -> ScoringResult:
        """
        Performs post-inference operations after receiving a model response.

        Args:
            model_response (str): The model's response.
            correct_answer (Union[int, str, float]): The correct answer.
            answer_handler (AnswerHandler, optional): The answer handler function. Defaults to None.

        Returns:
            ScoringResult: The scoring result.

        """
        if answer_handler:
            return answer_handler(correct_answer=correct_answer, model_response=model_response)

        scoring_result = ScoringResult(
            provided_answer=model_response,
            correct_answer=correct_answer,
            is_correct=model_response == correct_answer,
        )
        return scoring_result

    def evaluate(self, verbose: bool = True) -> None:
        """
        Evaluates the question answering dataset and prints the results.

        Args:
            verbose (bool, optional): Whether to print the evaluation results. Defaults to True.

        """
        for idx, entry in enumerate(self.question_answering_ds.questions):
            messages = self.pre_inference_hook(question_answering_entry=entry)
            model_response = self.chat_model_under_evaluation.complete_chat(messages=messages)
            scoring_result = self.post_inference_hook(
                model_response=model_response, correct_answer=entry.correct_answer
            )
            self.evaluation_results[entry] = scoring_result
            self.aggregated_results.add_results(scoring_result)
            if verbose:
                msg = dedent(
                    f"""\
                    Question # {idx}
                        - Question: {entry.question}
                        - Correct answer: {entry.correct_answer}
                        - Model answer: {scoring_result.provided_answer}
                        - Is correct?: {scoring_result.is_correct}
                    """
                )
                print(msg)
