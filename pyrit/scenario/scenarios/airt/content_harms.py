# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import logging
import os
from typing import Any, Dict, List, Optional, Sequence, Type, TypeVar

from pyrit.common import apply_defaults
from pyrit.common.deprecation import print_deprecation_message
from pyrit.executor.attack import (
    AttackAdversarialConfig,
    AttackScoringConfig,
    AttackStrategy,
    ManyShotJailbreakAttack,
    PromptSendingAttack,
    RolePlayAttack,
    RolePlayPaths,
    TreeOfAttacksWithPruningAttack,
)
from pyrit.models import SeedAttackGroup, SeedGroup
from pyrit.prompt_target import OpenAIChatTarget, PromptChatTarget
from pyrit.scenario.core.atomic_attack import AtomicAttack
from pyrit.scenario.core.dataset_configuration import DatasetConfiguration
from pyrit.scenario.core.scenario import Scenario
from pyrit.scenario.core.scenario_strategy import (
    ScenarioCompositeStrategy,
    ScenarioStrategy,
)
from pyrit.score import SelfAskRefusalScorer, TrueFalseInverterScorer, TrueFalseScorer

logger = logging.getLogger(__name__)

AttackStrategyT = TypeVar("AttackStrategyT", bound="AttackStrategy[Any, Any]")


class ContentHarmsDatasetConfiguration(DatasetConfiguration):
    """
    Dataset configuration for content harms that loads seed groups by harm category.

    This subclass overrides the default loading behavior to use harm category pattern
    matching instead of exact dataset name matching. When scenario_composites are provided,
    it filters datasets to only those matching the selected harm strategies.
    """

    def get_seed_groups(self) -> Dict[str, List[SeedGroup]]:
        """
        Get seed groups filtered by harm strategies from stored scenario_composites.

        When scenario_composites are set, this filters to only include datasets
        matching the selected harm strategies and returns harm strategy names as keys.

        Returns:
            Dict[str, List[SeedGroup]]: Dictionary mapping harm strategy names to their
                seed groups, filtered by the selected harm strategies.
        """
        result = super().get_seed_groups()

        if self._scenario_composites is None:
            return result

        # Extract selected harm strategies
        selected_harms = ScenarioCompositeStrategy.extract_single_strategy_values(
            self._scenario_composites, strategy_type=ContentHarmsStrategy
        )

        # Filter to matching datasets and map keys to harm names
        mapped_result: Dict[str, List[SeedGroup]] = {}
        for name, groups in result.items():
            matched_harm = next((harm for harm in selected_harms if harm in name), None)
            if matched_harm:
                mapped_result[matched_harm] = groups

        return mapped_result


class ContentHarmsStrategy(ScenarioStrategy):
    """
    ContentHarmsStrategy defines a set of strategies for testing model behavior
    across several different harm categories. The scenario is designed to provide quick
    feedback on model performance with respect to common harm types with the idea being that
    users will dive deeper into specific harm categories based on initial results.

    Each tag represents a different harm category that the model can be tested for.
    Specifying the all tag will include a comprehensive test suite covering all harm categories.
    Users can define objectives for each harm category via seed datasets or use the default datasets
    provided with PyRIT.

    """

    ALL = ("all", {"all"})

    Hate = ("hate", set[str]())
    Fairness = ("fairness", set[str]())
    Violence = ("violence", set[str]())
    Sexual = ("sexual", set[str]())
    Harassment = ("harassment", set[str]())
    Misinformation = ("misinformation", set[str]())
    Leakage = ("leakage", set[str]())


class ContentHarms(Scenario):
    """

    Content Harms Scenario implementation for PyRIT.

    This scenario contains various harm-based checks that you can run to get a quick idea about model behavior
    with respect to certain harm categories.
    """

    VERSION: int = 1

    @classmethod
    def get_strategy_class(cls) -> Type[ScenarioStrategy]:
        """
        Get the strategy enum class for this scenario.

        Returns:
            Type[ScenarioStrategy]: The ContentHarmsStrategy enum class.
        """
        return ContentHarmsStrategy

    @classmethod
    def get_default_strategy(cls) -> ScenarioStrategy:
        """
        Get the default strategy used when no strategies are specified.

        Returns:
            ScenarioStrategy: ContentHarmsStrategy.ALL
        """
        return ContentHarmsStrategy.ALL

    @classmethod
    def default_dataset_config(cls) -> DatasetConfiguration:
        """
        Return the default dataset configuration for this scenario.

        Returns:
            DatasetConfiguration: Configuration with all content harm datasets.
        """
        return ContentHarmsDatasetConfiguration(
            dataset_names=[
                "airt_hate",
                "airt_fairness",
                "airt_violence",
                "airt_sexual",
                "airt_harassment",
                "airt_misinformation",
                "airt_leakage",
            ],
            max_dataset_size=4,
        )

    @apply_defaults
    def __init__(
        self,
        *,
        adversarial_chat: Optional[PromptChatTarget] = None,
        objective_scorer: Optional[TrueFalseScorer] = None,
        scenario_result_id: Optional[str] = None,
        objectives_by_harm: Optional[Dict[str, Sequence[SeedGroup]]] = None,
    ):
        """
        Initialize the Content Harms Scenario.

        Args:
            adversarial_chat (Optional[PromptChatTarget]): Additionally used for scoring defaults.
                If not provided, a default OpenAI target will be created using environment variables.
            objective_scorer (Optional[TrueFalseScorer]): Scorer to evaluate attack success.
                If not provided, creates a default composite scorer using Azure Content Filter
                and SelfAsk Refusal scorers.
                seed_dataset_prefix (Optional[str]): Prefix of the dataset to use to retrieve the objectives.
                This will be used to retrieve the appropriate seed groups from CentralMemory. If not provided,
                defaults to "content_harm".
            scenario_result_id (Optional[str]): Optional ID of an existing scenario result to resume.
            objectives_by_harm (Optional[Dict[str, Sequence[SeedGroup]]]): DEPRECATED - Use dataset_config
                in initialize_async instead. A dictionary mapping harm strategies to their corresponding
                SeedGroups. If not provided, default seed groups will be loaded from datasets.
        """
        if objectives_by_harm is not None:
            print_deprecation_message(
                old_item="objectives_by_harm parameter",
                new_item="dataset_config in initialize_async",
                removed_in="0.13.0",
            )

        self._objective_scorer: TrueFalseScorer = objective_scorer if objective_scorer else self._get_default_scorer()
        self._scorer_config = AttackScoringConfig(objective_scorer=self._objective_scorer)
        self._adversarial_chat = adversarial_chat if adversarial_chat else self._get_default_adversarial_target()

        super().__init__(
            version=self.VERSION,
            objective_scorer=self._objective_scorer,
            strategy_class=ContentHarmsStrategy,
            scenario_result_id=scenario_result_id,
        )
        self._objectives_by_harm = objectives_by_harm

    def _get_default_adversarial_target(self) -> OpenAIChatTarget:
        return OpenAIChatTarget(
            endpoint=os.environ.get("AZURE_OPENAI_GPT4O_UNSAFE_CHAT_ENDPOINT"),
            api_key=os.environ.get("AZURE_OPENAI_GPT4O_UNSAFE_CHAT_KEY"),
            model_name=os.environ.get("AZURE_OPENAI_GPT4O_UNSAFE_CHAT_MODEL"),
            temperature=1.2,
        )

    def _get_default_scorer(self) -> TrueFalseInverterScorer:
        return TrueFalseInverterScorer(
            scorer=SelfAskRefusalScorer(
                chat_target=OpenAIChatTarget(
                    endpoint=os.environ.get("AZURE_OPENAI_GPT4O_UNSAFE_CHAT_ENDPOINT"),
                    api_key=os.environ.get("AZURE_OPENAI_GPT4O_UNSAFE_CHAT_KEY"),
                    model_name=os.environ.get("AZURE_OPENAI_GPT4O_UNSAFE_CHAT_MODEL"),
                    temperature=0.9,
                )
            ),
        )

    def _resolve_seed_groups_by_harm(self) -> Dict[str, List[SeedAttackGroup]]:
        """
        Resolve seed groups from deprecated objectives_by_harm or dataset configuration.

        Returns:
            Dict[str, List[SeedAttackGroup]]: Dictionary mapping content harm strategy names to their
                seed attack groups.

        Raises:
            ValueError: If both objectives_by_harm and dataset_config are specified.
        """
        if self._objectives_by_harm is not None and self._dataset_config_provided:
            raise ValueError(
                "Cannot specify both 'objectives_by_harm' parameter and 'dataset_config'. "
                "Please use only 'dataset_config' in initialize_async."
            )

        if self._objectives_by_harm is not None:
            return {
                harm: [SeedAttackGroup(seeds=list(sg.seeds)) for sg in groups]
                for harm, groups in self._objectives_by_harm.items()
            }

        # Set scenario_composites on the config so get_seed_attack_groups can filter by strategy
        self._dataset_config._scenario_composites = self._scenario_composites
        return self._dataset_config.get_seed_attack_groups()

    async def _get_atomic_attacks_async(self) -> List[AtomicAttack]:
        """
        Retrieve the list of AtomicAttack instances for harm strategies.

        Returns:
            List[AtomicAttack]: The list of AtomicAttack instances for harm strategies.
        """
        seed_groups_by_harm = self._resolve_seed_groups_by_harm()

        atomic_attacks: List[AtomicAttack] = []
        for strategy, seed_groups in seed_groups_by_harm.items():
            atomic_attacks.extend(self._get_strategy_attacks(strategy=strategy, seed_groups=seed_groups))
        return atomic_attacks

    def _get_strategy_attacks(
        self,
        strategy: str,
        seed_groups: Sequence[SeedAttackGroup],
    ) -> List[AtomicAttack]:
        """
        Create AtomicAttack instances for a given harm strategy.

        Args:
            strategy (str): The harm strategy name to create attacks for.
            seed_groups (Sequence[SeedAttackGroup]): The seed attack groups associated with the harm dataset.

        Returns:
            List[AtomicAttack]: The constructed AtomicAttack instances for each attack type.

        Raises:
            ValueError: If scenario is not properly initialized.
        """
        # objective_target is guaranteed to be non-None by parent class validation
        if self._objective_target is None:
            raise ValueError(
                "Scenario not properly initialized. Call await scenario.initialize_async() before running."
            )

        attacks: List[AtomicAttack] = [
            *self._get_single_turn_attacks(strategy=strategy, seed_groups=seed_groups),
            *self._get_multi_turn_attacks(strategy=strategy, seed_groups=seed_groups),
        ]

        return attacks

    def _get_single_turn_attacks(
        self,
        *,
        strategy: str,
        seed_groups: Sequence[SeedAttackGroup],
    ) -> List[AtomicAttack]:
        """
        Create single-turn AtomicAttack instances: RolePlayAttack and PromptSendingAttack.

        Args:
            strategy (str): The harm strategy name.
            seed_groups (Sequence[SeedAttackGroup]): Seed attack groups for this harm category.

        Returns:
            List[AtomicAttack]: The single-turn atomic attacks.
        """
        prompt_sending_attack = PromptSendingAttack(
            objective_target=self._objective_target,
            attack_scoring_config=self._scorer_config,
        )

        role_play_attack = RolePlayAttack(
            objective_target=self._objective_target,
            adversarial_chat=self._adversarial_chat,
            role_play_definition_path=RolePlayPaths.MOVIE_SCRIPT.value,
        )

        return [
            AtomicAttack(
                atomic_attack_name=strategy,
                attack=prompt_sending_attack,
                seed_groups=list(seed_groups),
                adversarial_chat=self._adversarial_chat,
                objective_scorer=self._objective_scorer,
                memory_labels=self._memory_labels,
            ),
            AtomicAttack(
                atomic_attack_name=strategy,
                attack=role_play_attack,
                seed_groups=list(seed_groups),
                adversarial_chat=self._adversarial_chat,
                objective_scorer=self._objective_scorer,
                memory_labels=self._memory_labels,
            ),
        ]

    def _get_multi_turn_attacks(
        self,
        *,
        strategy: str,
        seed_groups: Sequence[SeedAttackGroup],
    ) -> List[AtomicAttack]:
        """
        Create multi-turn AtomicAttack instances: ManyShotJailbreakAttack and TreeOfAttacksWithPruningAttack.

        Args:
            strategy (str): The harm strategy name.
            seed_groups (Sequence[SeedAttackGroup]): Seed attack groups for this harm category.

        Returns:
            List[AtomicAttack]: The multi-turn atomic attacks.
        """
        many_shot_jailbreak_attack = ManyShotJailbreakAttack(
            objective_target=self._objective_target,
            attack_scoring_config=self._scorer_config,
        )

        tap_attack = TreeOfAttacksWithPruningAttack(
            objective_target=self._objective_target,
            attack_adversarial_config=AttackAdversarialConfig(target=self._adversarial_chat),
        )

        return [
            AtomicAttack(
                atomic_attack_name=strategy,
                attack=many_shot_jailbreak_attack,
                seed_groups=list(seed_groups),
                adversarial_chat=self._adversarial_chat,
                objective_scorer=self._objective_scorer,
                memory_labels=self._memory_labels,
            ),
            AtomicAttack(
                atomic_attack_name=strategy,
                attack=tap_attack,
                seed_groups=list(seed_groups),
                adversarial_chat=self._adversarial_chat,
                objective_scorer=self._objective_scorer,
                memory_labels=self._memory_labels,
            ),
        ]
