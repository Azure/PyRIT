{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {},
      "source": [
        "# Benchmark Orchestrator - optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
        "from pyrit.datasets import fetch_wmdp_dataset\n",
        "from pyrit.models import (\n",
        "    QuestionAnsweringDataset,\n",
        "    QuestionAnsweringEntry,\n",
        "    QuestionChoice,\n",
        ")\n",
        "from pyrit.orchestrator.question_answer_benchmark_orchestrator import (\n",
        "    QuestionAnsweringBenchmarkOrchestrator,\n",
        ")\n",
        "from pyrit.prompt_target import OpenAIChatTarget\n",
        "from pyrit.score.question_answer_scorer import QuestionAnswerScorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize PyRIT (load environment files and set central memory instance)\n",
        "initialize_pyrit(memory_db_type=IN_MEMORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the Azure OpenAI prompt target\n",
        "target = OpenAIChatTarget()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create demo dataset for Q/A Model\n",
        "qa_ds = QuestionAnsweringDataset(\n",
        "    name=\"demo dataset\",\n",
        "    version=\"1.0\",\n",
        "    description=\"A dataset for testing the question answering model\",\n",
        "    author=\"PyRIT Team\",\n",
        "    group=\"\",\n",
        "    source=\"\",\n",
        "    questions=[\n",
        "        QuestionAnsweringEntry(\n",
        "            question=\"What is the capital of France?\",\n",
        "            answer_type=\"str\",\n",
        "            correct_answer=\"Paris\",\n",
        "            choices=[\n",
        "                QuestionChoice(index=0, text=\"Paris\"),\n",
        "                QuestionChoice(index=1, text=\"London\"),\n",
        "                QuestionChoice(index=2, text=\"Berlin\"),\n",
        "                QuestionChoice(index=3, text=\"Madrid\"),\n",
        "            ],\n",
        "        ),\n",
        "        QuestionAnsweringEntry(\n",
        "            question=\"What is the capital of Germany?\",\n",
        "            answer_type=\"str\",\n",
        "            correct_answer=\"Berlin\",\n",
        "            choices=[\n",
        "                QuestionChoice(index=0, text=\"Paris\"),\n",
        "                QuestionChoice(index=1, text=\"London\"),\n",
        "                QuestionChoice(index=2, text=\"Berlin\"),\n",
        "                QuestionChoice(index=3, text=\"Madrid\"),\n",
        "            ],\n",
        "        ),\n",
        "        QuestionAnsweringEntry(\n",
        "            question=\"Which of the following is a prime number?\",\n",
        "            answer_type=\"str\",\n",
        "            correct_answer=\"507961\",\n",
        "            choices=[\n",
        "                QuestionChoice(index=0, text=\"507963\"),\n",
        "                QuestionChoice(index=1, text=\"507962\"),\n",
        "                QuestionChoice(index=2, text=\"507960\"),\n",
        "                QuestionChoice(index=3, text=\"507961\"),\n",
        "            ],\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Create the score for the Q/A Model\n",
        "qa_scorer = QuestionAnswerScorer(\n",
        "    dataset=qa_ds,\n",
        ")\n",
        "\n",
        "# Create the orchestrator with scorer and demo dataset\n",
        "benchmark_orchestrator = QuestionAnsweringBenchmarkOrchestrator(\n",
        "    chat_model_under_evaluation=target, scorer=qa_scorer, verbose=True\n",
        ")\n",
        "\n",
        "# Evaluate the Q/A Model response\n",
        "await benchmark_orchestrator.evaluate()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output if the results are correct\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for idx, (qa_question_entry, answer) in enumerate(benchmark_orchestrator._scorer.evaluation_results.items()):\n",
        "    print(f\"Question {idx+1}: {qa_question_entry.question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"\")\n",
        "\n",
        "    correct_count += 1 if answer.is_correct else 0\n",
        "\n",
        "print(f\"Correct count: {correct_count}/{len(benchmark_orchestrator._scorer.evaluation_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch WMDP dataset for Q/A Model Testing\n",
        "\n",
        "wmdp_ds = fetch_wmdp_dataset()\n",
        "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
        "\n",
        "# Create the score for the Q/A Model\n",
        "qa_scorer_wmdp = QuestionAnswerScorer(\n",
        "    dataset=wmdp_ds,\n",
        ")\n",
        "\n",
        "# Create the orchestrator with scorer and demo dataset\n",
        "benchmark_orchestrator_wmdp = QuestionAnsweringBenchmarkOrchestrator(\n",
        "    chat_model_under_evaluation=target, scorer=qa_scorer_wmdp, verbose=True\n",
        ")\n",
        "\n",
        "# Evaluate the Q/A Model response\n",
        "await benchmark_orchestrator_wmdp.evaluate()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output if the results are correct\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for idx, (qa_question_entry, answer) in enumerate(benchmark_orchestrator_wmdp._scorer.evaluation_results.items()):\n",
        "    print(f\"Question {idx+1}: {qa_question_entry.question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"\")\n",
        "\n",
        "    correct_count += 1 if answer.is_correct else 0\n",
        "\n",
        "print(f\"Correct count: {correct_count}/{len(benchmark_orchestrator_wmdp._scorer.evaluation_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch WMDP dataset for Q/A Model Testing - Chem Subset\n",
        "\n",
        "wmdp_ds = fetch_wmdp_dataset(category=\"chem\")\n",
        "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
        "\n",
        "# Create the score for the Q/A Model\n",
        "qa_scorer_wmdp = QuestionAnswerScorer(\n",
        "    dataset=wmdp_ds,\n",
        ")\n",
        "\n",
        "# Create the orchestrator with scorer and demo dataset\n",
        "benchmark_orchestrator_wmdp = QuestionAnsweringBenchmarkOrchestrator(\n",
        "    chat_model_under_evaluation=target, scorer=qa_scorer_wmdp, verbose=True\n",
        ")\n",
        "\n",
        "# Evaluate the Q/A Model response\n",
        "await benchmark_orchestrator_wmdp.evaluate()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output if the results are correct\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for idx, (qa_question_entry, answer) in enumerate(benchmark_orchestrator_wmdp._scorer.evaluation_results.items()):\n",
        "    print(f\"Question {idx+1}: {qa_question_entry.question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"\")\n",
        "\n",
        "    correct_count += 1 if answer.is_correct else 0\n",
        "\n",
        "print(f\"Correct count: {correct_count}/{len(benchmark_orchestrator_wmdp._scorer.evaluation_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch WMDP dataset for Q/A Model Testing - Bio Subset\n",
        "\n",
        "wmdp_ds = fetch_wmdp_dataset(category=\"bio\")\n",
        "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
        "\n",
        "# Create the score for the Q/A Model\n",
        "qa_scorer_wmdp = QuestionAnswerScorer(\n",
        "    dataset=wmdp_ds,\n",
        ")\n",
        "\n",
        "# Create the orchestrator with scorer and demo dataset\n",
        "benchmark_orchestrator_wmdp = QuestionAnsweringBenchmarkOrchestrator(\n",
        "    chat_model_under_evaluation=target, scorer=qa_scorer_wmdp, verbose=True\n",
        ")\n",
        "\n",
        "# Evaluate the Q/A Model response\n",
        "await benchmark_orchestrator_wmdp.evaluate()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output if the results are correct\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for idx, (qa_question_entry, answer) in enumerate(benchmark_orchestrator_wmdp._scorer.evaluation_results.items()):\n",
        "    print(f\"Question {idx+1}: {qa_question_entry.question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"\")\n",
        "\n",
        "    correct_count += 1 if answer.is_correct else 0\n",
        "\n",
        "print(f\"Correct count: {correct_count}/{len(benchmark_orchestrator_wmdp._scorer.evaluation_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch WMDP dataset for Q/A Model Testing - Cyber Subset\n",
        "\n",
        "wmdp_ds = fetch_wmdp_dataset(category=\"cyber\")\n",
        "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
        "\n",
        "# Create the score for the Q/A Model\n",
        "qa_scorer_wmdp = QuestionAnswerScorer(\n",
        "    dataset=wmdp_ds,\n",
        ")\n",
        "\n",
        "# Create the orchestrator with scorer and demo dataset\n",
        "benchmark_orchestrator_wmdp = QuestionAnsweringBenchmarkOrchestrator(\n",
        "    chat_model_under_evaluation=target, scorer=qa_scorer_wmdp, verbose=True\n",
        ")\n",
        "\n",
        "# Evaluate the Q/A Model response\n",
        "await benchmark_orchestrator_wmdp.evaluate()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output if the results are correct\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for idx, (qa_question_entry, answer) in enumerate(benchmark_orchestrator_wmdp._scorer.evaluation_results.items()):\n",
        "    print(f\"Question {idx+1}: {qa_question_entry.question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"\")\n",
        "\n",
        "    correct_count += 1 if answer.is_correct else 0\n",
        "\n",
        "print(f\"Correct count: {correct_count}/{len(benchmark_orchestrator_wmdp._scorer.evaluation_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    wmdp_ds = fetch_wmdp_dataset(category=\"invalid string\")\n",
        "except ValueError:\n",
        "    print(\"TEST PASS. Value Error Caught.\")\n",
        "else:\n",
        "    print(\"TEST FAILED. No ValueError Caught.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close connection for memory instance\n",
        "from pyrit.memory import CentralMemory\n",
        "\n",
        "memory = CentralMemory.get_memory_instance()\n",
        "memory.dispose_engine()"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}