{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AdvBench PromptSendingOrchestrator - optional\n",
    "\n",
    "This demo is uses prompts from the AdvBench dataset to try against a target. It includes the ways you can send the prompts,\n",
    "and how you can view results. Before starting, import the necessary libraries.\n",
    "\n",
    "Before you begin, ensure you are setup with the correct version of PyRIT installed and have secrets configured as described [here](../../setup/populating_secrets.md).\n",
    "\n",
    "The first example is as simple as it gets.\n",
    "\n",
    "The results and intermediate interactions will be saved to memory according to the environment settings. For details, see the [Memory Configuration Guide](../memory/0_memory.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
    "from pyrit.datasets import fetch_adv_bench_dataset\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
    "target = OpenAIChatTarget()\n",
    "\n",
    "orchestrator = PromptSendingOrchestrator(objective_target=target)\n",
    "\n",
    "adv_bench_prompts = fetch_adv_bench_dataset()\n",
    "prompts = [prompt.value for prompt in adv_bench_prompts.prompts[:3]]\n",
    "\n",
    "start = time.time()\n",
    "await orchestrator.send_prompts_async(prompt_list=prompts)  # type: ignore\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Elapsed time for operation: {end-start}\")\n",
    "\n",
    "await orchestrator.print_conversations_async()  # type: ignore\n",
    "\n",
    "orchestrator.dispose_db_engine()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
