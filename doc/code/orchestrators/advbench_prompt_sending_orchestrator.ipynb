{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AdvBench PromptSendingOrchestrator - optional\n",
    "\n",
    "This demo is uses prompts from the AdvBench dataset to try against a target. It includes the ways you can send the prompts,\n",
    "and how you can view results. Before starting, import the necessary libraries.\n",
    "\n",
    "Before you begin, ensure you are setup with the correct version of PyRIT installed and have secrets configured as described [here](../../setup/populating_secrets.md).\n",
    "\n",
    "The first example is as simple as it gets.\n",
    "\n",
    "The results and intermediate interactions will be saved to memory according to the environment settings. For details, see the [Memory Configuration Guide](../memory/0_memory.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.common import default_values\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.prompt_converter import Base64Converter\n",
    "from pyrit.datasets import fetch_adv_bench_dataset\n",
    "\n",
    "\n",
    "default_values.load_environment_files()\n",
    "target = OpenAIChatTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PromptSendingOrchestrator(prompt_target=target) as orchestrator:\n",
    "    adv_bench_prompts = fetch_adv_bench_dataset()\n",
    "    prompts = [prompt.value for prompt in adv_bench_prompts.prompts[:3]]\n",
    "\n",
    "    start = time.time()\n",
    "    await orchestrator.send_prompts_async(prompt_list=prompts)  # type: ignore\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Elapsed time for operation: {end-start}\")\n",
    "\n",
    "    orchestrator.print_conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Adding Converters\n",
    "\n",
    "Additionally, we can make it more interesting by initializing the orchestrator with different types of prompt converters.\n",
    "This variation takes the original example, but converts the text to base64 before sending it to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PromptSendingOrchestrator(\n",
    "    prompt_target=target, prompt_converters=[Base64Converter()], batch_size=1\n",
    ") as orchestrator:\n",
    "\n",
    "    adv_bench_prompts = fetch_adv_bench_dataset()\n",
    "\n",
    "    # this is run in a Jupyter notebook, so we can use await\n",
    "    await orchestrator.send_prompts_async(prompt_list=prompts)  # type: ignore\n",
    "\n",
    "    await orchestrator.print_conversations()  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pyrit-dev",
   "language": "python",
   "name": "pyrit-kernel"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
