{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PKU-SafeRHLF dataset testing\n",
    "\n",
    "This notebook demonstrates the process of using PKU-SafeRHLF examples to perform AI red teaming on a target language model (LLM).\n",
    "It involves fetching potentially harmful behavior prompts from the PKU-SafeRHLF repository, sending them to the LLM, and analyzing the responses.\n",
    "The goal is to identify vulnerabilities, inappropriate responses, or weaknesses in the model's handling of harmful or ethically sensitive prompts.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fetch_pku_safeRLHF_dataset' from 'pyrit.datasets' (c:\\Users\\mshirsekar\\AppData\\Local\\miniconda3\\envs\\pyrit-dev\\Lib\\site-packages\\pyrit\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary packages\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyrit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_values\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyrit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_pku_safeRLHF_dataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyrit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morchestrator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptSendingOrchestrator\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyrit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_target\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextTarget\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'fetch_pku_safeRLHF_dataset' from 'pyrit.datasets' (c:\\Users\\mshirsekar\\AppData\\Local\\miniconda3\\envs\\pyrit-dev\\Lib\\site-packages\\pyrit\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "from pyrit.common import default_values\n",
    "from pyrit.datasets import fetch_pku_safeRLHF_dataset\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.prompt_target import TextTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "default_values.load_default_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Azure OpenAI prompt target\n",
    "prompt_target = TextTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the orchestrator with scorer without safe prompts included\n",
    "orchestrator = PromptSendingOrchestrator(prompt_target=prompt_target)\n",
    "\n",
    "# Fetch only unsafe prompts from PKU-SafeRLHF dataset\n",
    "prompt_dataset = fetch_pku_safeRLHF_dataset(False)\n",
    "\n",
    "# Use the first 8 examples for red teaming\n",
    "prompt_list = prompt_dataset.prompts[:8]\n",
    "\n",
    "# Send prompts using the orchestrator and capture responses\n",
    "try:\n",
    "    responses = await orchestrator.send_prompts_async(prompt_list=prompt_list)  # type: ignore\n",
    "    if responses:\n",
    "        orchestrator.print_conversations()  # Retrieve the memory to print scoring results\n",
    "    else:\n",
    "        print(\"No valid responses were received from the orchestrator.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while sending prompts: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the orchestrator with scorer with safe prompts included\n",
    "orchestrator = PromptSendingOrchestrator(prompt_target=prompt_target)\n",
    "\n",
    "# Fetch prompts from PKU-SafeRLHF dataset\n",
    "prompt_dataset = fetch_pku_safeRLHF_dataset(True)\n",
    "\n",
    "# Use the first 8 examples for red teaming\n",
    "prompt_list = prompt_dataset.prompts[:8]\n",
    "\n",
    "# Send prompts using the orchestrator and capture responses\n",
    "try:\n",
    "    responses = await orchestrator.send_prompts_async(prompt_list=prompt_list)  # type: ignore\n",
    "    if responses:\n",
    "        orchestrator.print_conversations()  # Retrieve the memory to print scoring results\n",
    "    else:\n",
    "        print(\"No valid responses were received from the orchestrator.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while sending prompts: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
