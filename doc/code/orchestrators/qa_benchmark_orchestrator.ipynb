{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Q&A Benchmark Orchestrator - optional\n",
    "\n",
    "The `QuestionAnsweringBenchmarkOrchestrator` can process Q&A datasets and evaluate how good a target is at answering the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
    "from pyrit.datasets import fetch_wmdp_dataset\n",
    "from pyrit.memory import CentralMemory\n",
    "from pyrit.models import (\n",
    "    QuestionAnsweringDataset,\n",
    "    QuestionAnsweringEntry,\n",
    "    QuestionChoice,\n",
    ")\n",
    "from pyrit.orchestrator import QuestionAnsweringBenchmarkOrchestrator\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score.self_ask_question_answer_scorer import SelfAskQuestionAnswerScorer\n",
    "\n",
    "# Initialize PyRIT (load environment files and set central memory instance)\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mThe orchestrator has completed the conversation and achieved the objective: \n",
      "The objective is to answer the question: What is the capital of France? with the correct answer: \"0: Paris\".\n",
      "Any other response that does not clearly indicate either the correct index or correct answer is incorrect.\n",
      "\n",
      "\u001b[1m\u001b[34muser:\n",
      "Converted value: What is the capital of France?\n",
      "\n",
      "Option 0: Paris\n",
      "Option 1: London\n",
      "Option 2: Berlin\n",
      "Option 3: Madrid\n",
      "\u001b[22m\u001b[33massistant: The capital of France is:\n",
      "\n",
      "**Option 0: Paris**\n",
      "\u001b[22m\u001b[37mobjective score: SelfAskQuestionAnswerScorer: question_answering: True : The response correctly identifies the capital of France as 'Paris' and clearly indicates the correct index, which is Option 0. Both parts of the answer fulfill the criteria laid out in the instructions, ensuring it is precise and accurate.\n"
     ]
    }
   ],
   "source": [
    "# Set up the Azure OpenAI prompt target\n",
    "\n",
    "objective_scorer = SelfAskQuestionAnswerScorer(chat_target=OpenAIChatTarget())\n",
    "\n",
    "# Create the orchestrator with scorer and demo dataset\n",
    "benchmark_orchestrator = QuestionAnsweringBenchmarkOrchestrator(\n",
    "    objective_target=OpenAIChatTarget(),\n",
    "    objective_scorer=objective_scorer,\n",
    ")\n",
    "\n",
    "question = QuestionAnsweringEntry(\n",
    "            question=\"What is the capital of France?\",\n",
    "            answer_type=\"str\",\n",
    "            correct_answer=\"0\",\n",
    "            choices=[\n",
    "                QuestionChoice(index=0, text=\"Paris\"),\n",
    "                QuestionChoice(index=1, text=\"London\"),\n",
    "                QuestionChoice(index=2, text=\"Berlin\"),\n",
    "                QuestionChoice(index=3, text=\"Madrid\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sends prompts associated with dataset\n",
    "result = await benchmark_orchestrator.run_attack_async(question_answering_entry=question)\n",
    "await result.print_conversation_async()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee9dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch WMDP dataset for Q/A Model Testing; this is big and can take a minute\n",
    "\n",
    "wmdp_ds = fetch_wmdp_dataset(category=\"cyber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QuestionAnsweringBenchmarkOrchestrator.run_attack_async() got an unexpected keyword argument 'question_ansering_entry'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate the Q/A Model response\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m benchmark_orchestrator.run_attacks_async(question_answering_entries=wmdp_ds.questions[:\u001b[32m3\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m result.print_conversation_async()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\git\\PyRIT\\pyrit\\orchestrator\\single_turn\\question_answer_benchmark_orchestrator.py:201\u001b[39m, in \u001b[36mQuestionAnsweringBenchmarkOrchestrator.run_attacks_async\u001b[39m\u001b[34m(self, question_answering_entries, prepended_conversations, memory_labels)\u001b[39m\n\u001b[32m    191\u001b[39m batch_items = [\n\u001b[32m    192\u001b[39m     question_answering_entries,\n\u001b[32m    193\u001b[39m     prepended_conversations\n\u001b[32m    194\u001b[39m ]\n\u001b[32m    196\u001b[39m batch_item_keys = [\n\u001b[32m    197\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion_ansering_entry\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    198\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprepended_conversation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    199\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m batch_task_async(\n\u001b[32m    202\u001b[39m     prompt_target=\u001b[38;5;28mself\u001b[39m._objective_target,\n\u001b[32m    203\u001b[39m     batch_size=\u001b[38;5;28mself\u001b[39m._batch_size,\n\u001b[32m    204\u001b[39m     items_to_batch=batch_items,\n\u001b[32m    205\u001b[39m     task_func=\u001b[38;5;28mself\u001b[39m.run_attack_async,\n\u001b[32m    206\u001b[39m     task_arguments=batch_item_keys,\n\u001b[32m    207\u001b[39m     memory_labels=memory_labels\n\u001b[32m    208\u001b[39m )\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [result \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\git\\PyRIT\\pyrit\\prompt_target\\batch_helper.py:85\u001b[39m, in \u001b[36mbatch_task_async\u001b[39m\u001b[34m(prompt_target, batch_size, items_to_batch, task_func, task_arguments, **task_kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arg_index, task_argument \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(task_arguments):\n\u001b[32m     84\u001b[39m         task_kwargs[task_argument] = task_args[arg_index][batch_index]\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     tasks.append(\u001b[43mtask_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtask_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     87\u001b[39m batch_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m     88\u001b[39m responses.extend(batch_results)\n",
      "\u001b[31mTypeError\u001b[39m: QuestionAnsweringBenchmarkOrchestrator.run_attack_async() got an unexpected keyword argument 'question_ansering_entry'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the Q/A Model response\n",
    "results = await benchmark_orchestrator.run_attacks_async(question_answering_entries=wmdp_ds.questions[:3])\n",
    "for result in results:\n",
    "    await result.print_conversation_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460372c",
   "metadata": {},
   "source": [
    "You can also do analysis and benchmarking using these. To see the percentage of correct answers, you can use the `OrchestratorResult` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = sum(r.status == \"success\" for r in results)\n",
    "print(f\"Success rate: {success / len(results)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
