{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a4859",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.common import default_values\n",
    "from pyrit.datasets import fetch_tdc23_redteaming_dataset\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.prompt_target import TextTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0851568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "default_values.load_default_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb04052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Azure OpenAI prompt target\n",
    "prompt_target = TextTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the orchestrator with scorer without safe prompts included\n",
    "orchestrator = PromptSendingOrchestrator(prompt_target=prompt_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch only unsafe prompts from tdc23_redteaming dataset\n",
    "prompt_dataset = fetch_tdc23_redteaming_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6104b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 8 examples for red teaming\n",
    "prompt_list = prompt_dataset.prompts[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send prompts using the orchestrator and capture responses\n",
    "try:\n",
    "    responses = await orchestrator.send_prompts_async(prompt_list=prompt_list)  # type: ignore\n",
    "    if responses:\n",
    "        orchestrator.print_conversations()  # Retrieve the memory to print scoring results\n",
    "    else:\n",
    "        print(\"No valid responses were received from the orchestrator.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while sending prompts: {e}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
