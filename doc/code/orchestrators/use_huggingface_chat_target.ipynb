{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d623d73a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# HuggingFace Chat Target Testing\n",
    "\n",
    "The goal is to optimize the notebook demo with a small yet effective instruction model, or even a chat model, allowing users to experiment with different HuggingFace models. Instruction models can also function as chat models and vice versa, enabling flexible interactions and a variety of use cases.\n",
    "\n",
    "## Key Notes:\n",
    "\n",
    "1. **Additional Dependencies**:\n",
    "   - Some models, such as `\"princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT\"` and `\"princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT\"`, require the `sentencepiece` library to be installed. If you encounter errors related to this, you can install `sentencepiece` via pip:\n",
    "\n",
    "     ```bash\n",
    "     pip install sentencepiece\n",
    "     ```\n",
    "\n",
    "2. **Access Requirements for Certain Models**:\n",
    "   - Some models, like `\"google/gemma-2b\"`, need special access. You must request access on the model's Hugging Face page and agree to the \"Terms of Use.\"\n",
    "   - Ensure your Hugging Face account email matches any email provided on external sites (e.g., Meta) to avoid request denial.\n",
    "   - After requesting, wait for approval from the model's authors.\n",
    "\n",
    "3. **Issues with Some Model Outputs**:\n",
    "   - During testing, it's been observed that some models may output responses that do not make sense or are not coherent. These models might require further adjustments, fine-tuning, or attention to their settings and prompts to improve their performance. It is advisable to take a closer look at these models and assess whether they need to be fixed or optimized for better results.\n",
    "\n",
    "4. **Model Performance and Response Times**:\n",
    "   - The tests were conducted using a CPU, and the following are the average response times for each model:\n",
    "     - `microsoft/Phi-3-mini-4k-instruct`: 4.89 seconds\n",
    "     - `HuggingFaceTB/SmolLM-135M-Instruct`: 3.09 seconds\n",
    "     - `HuggingFaceTB/SmolLM-360M-Instruct`: 3.31 seconds\n",
    "     - `HuggingFaceTB/SmolLM-1.7B-Instruct`: 5.87 seconds\n",
    "     - `Qwen/Qwen2-0.5B-Instruct`: 1.38 seconds\n",
    "     - `Qwen/Qwen2-1.5B-Instruct`: 2.96 seconds\n",
    "     - `stabilityai/stablelm-2-zephyr-1_6b`: 5.31 seconds\n",
    "     - `stabilityai/stablelm-zephyr-3b`: 8.37 seconds\n",
    "     - `google/gemma-2b`: 7.79 seconds\n",
    "     - `TinyLlama/TinyLlama-1.1B-Chat-v1.0`: 4.03 seconds\n",
    "     - `princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT`: 3.36 seconds\n",
    "     - `princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT`: 8.29 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d61a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c70005b72604b9196f089ac72ba3917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for microsoft/Phi-3-mini-4k-instruct: 4.89 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 59de0019-3cf2-44b6-bbdf-261c1d3fd211\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: What is 7*7? 7 multiplied by 7 equals 49.\n",
      "\u001b[22m\u001b[39mConversation ID: 721df2cd-d7d9-4a56-ae44-956b0fb3439e\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: What is 6*6? 6 multiplied by 6 equals 36.\n",
      "\u001b[22m\u001b[39mConversation ID: 73fa053f-b85b-406f-85ef-c60a280338f8\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: What is 5*5? 5 multiplied by 5 equals 25.\n",
      "\u001b[22m\u001b[39mConversation ID: 79c96c19-4256-41bb-8f65-92f67c279db8\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: What is 4*4? 4 multiplied by 4 equals 16.\n",
      "\u001b[22m\u001b[39mConversation ID: 902384b3-b1a7-4d28-a4ef-c69d541e63db\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: What is 3*3? 3 multiplied by 3 equals 9.\n",
      "--------------------------------------------------\n",
      "Running model: HuggingFaceTB/SmolLM-135M-Instruct\n",
      "Average response time for HuggingFaceTB/SmolLM-135M-Instruct: 3.09 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 05168533-b80c-455e-ae99-aab48c88c23d\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 4*4?\n",
      "assistant\n",
      "What a great question!\n",
      "\n",
      "The number 4*4 is a special number because it can be expressed as a product of two numbers, which\n",
      "\u001b[22m\u001b[39mConversation ID: 4c967e6a-b7f0-4264-99e0-c9c48f24ec1a\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 3*3?\n",
      "assistant\n",
      "What a great question!\n",
      "\n",
      "3*3 is a special number because it's a 3-digit number, and it's also a \n",
      "\u001b[22m\u001b[39mConversation ID: a5ace17a-46bf-4820-928a-5dba7ffea7dd\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 5*5?\n",
      "assistant\n",
      "The number 5*5! is a fascinating number that appears in various mathematical and scientific contexts. Here's a breakdown of its properties and significance:\n",
      "\u001b[22m\u001b[39mConversation ID: ca4cc618-2aa2-44a6-a5f3-0c140bfb45f0\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 7*7?\n",
      "assistant\n",
      "7*7 is a mathematical expression that represents the number 7 multiplied by 7. It is a common mathematical expression that can be used to represent\n",
      "\u001b[22m\u001b[39mConversation ID: cd4b72e8-a8f7-446a-bdc5-ff6e8285fbd1\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 6*6?\n",
      "assistant\n",
      "The number 6*6! is a fascinating number that appears in various mathematical and scientific contexts. Here's a breakdown of its properties and significance:\n",
      "--------------------------------------------------\n",
      "Running model: HuggingFaceTB/SmolLM-360M-Instruct\n",
      "Average response time for HuggingFaceTB/SmolLM-360M-Instruct: 3.31 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 04bcbe94-9d33-4fef-b768-f132d7285314\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 4*4?\n",
      "assistant\n",
      "The 4*4 is a mathematical notation that represents a 4-dimensional space. It is a way to describe a space that has four dimensions\n",
      "\u001b[22m\u001b[39mConversation ID: 8ed6f4e8-c89a-4172-8f78-f060c26b6ca3\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 7*7?\n",
      "assistant\n",
      "The 7*7 is a popular exercise routine that involves 7 sets of 7 reps each, with 7 sets of 7 re\n",
      "\u001b[22m\u001b[39mConversation ID: bf2f004f-c002-442d-bdb5-cedaaca27633\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 5*5?\n",
      "assistant\n",
      "The 5*5 is a mathematical notation that represents a 5-dimensional vector. It is a way to express a vector in a 5\n",
      "\u001b[22m\u001b[39mConversation ID: ea44a253-8d19-4608-9006-40477aee2d4e\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 6*6?\n",
      "assistant\n",
      "The 6*6 is a mathematical concept that represents a 6-dimensional space. It's a way to describe a space that has six dimensions\n",
      "\u001b[22m\u001b[39mConversation ID: f41812e6-cdab-44e4-ad02-4a10ca9a9389\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 3*3?\n",
      "assistant\n",
      "The 3*3 matrix!\n",
      "\n",
      "A 3*3 matrix is a square matrix with 3 rows and 3 columns. It is\n",
      "--------------------------------------------------\n",
      "Running model: HuggingFaceTB/SmolLM-1.7B-Instruct\n",
      "Average response time for HuggingFaceTB/SmolLM-1.7B-Instruct: 5.87 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 19b54b54-31eb-4572-8961-a0435cf521a8\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 4*4?\n",
      "assistant\n",
      "4*4 is a popular brand of athletic shoes designed for running, hiking, and other outdoor activities. The \"4\" refers to the number of\n",
      "\u001b[22m\u001b[39mConversation ID: 1f109697-5a33-4096-9449-4d0f282bcbc7\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 6*6?\n",
      "assistant\n",
      "6*6 is a popular card game that combines elements of poker, blackjack, and solitaire. It's a great game for players who\n",
      "\u001b[22m\u001b[39mConversation ID: 71a8fd3d-0db9-4741-bd2d-443c062836ef\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 3*3?\n",
      "assistant\n",
      "3*3 is a popular social media challenge that encourages users to share a photo or video of themselves doing a fun, creative, or humorous activity,\n",
      "\u001b[22m\u001b[39mConversation ID: 82b4fabb-7afd-42a0-9467-1bd036bd54ad\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 7*7?\n",
      "assistant\n",
      "7*7 is a popular phrase that has been used in various contexts, including music, sports, and even everyday conversations. So, what does it\n",
      "\u001b[22m\u001b[39mConversation ID: d6c1b4fb-ed2f-4017-86a7-c65bf55ab059\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: user\n",
      "What is 5*5?\n",
      "assistant\n",
      "5*5 is a popular social media platform that allows users to create and share content, connect with others, and engage in various activities. Here's\n",
      "--------------------------------------------------\n",
      "Running model: Qwen/Qwen2-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for Qwen/Qwen2-0.5B-Instruct: 1.38 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 69ec2b9d-497c-4f6a-a937-4d09e7509132\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 7*7?\n",
      "assistant\n",
      "7 * 7 equals 49.\n",
      "\u001b[22m\u001b[39mConversation ID: a9f7679c-ebe5-4b35-9e03-3009520accc2\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 3*3?\n",
      "assistant\n",
      "27\n",
      "\u001b[22m\u001b[39mConversation ID: c9de603e-2c44-4533-bd0e-2d81f9fe6486\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 4*4?\n",
      "assistant\n",
      "8\n",
      "\u001b[22m\u001b[39mConversation ID: d4a5125c-6156-44ff-b7a8-bbc1ee3e5959\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 6*6?\n",
      "assistant\n",
      "The answer to \\(6 \\times 6\\) is \\(36\\). If you had asked me at the time, I would have replied that\n",
      "\u001b[22m\u001b[39mConversation ID: f7567c1f-42b7-42be-bc5f-821dfd7bccdf\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 5*5?\n",
      "assistant\n",
      "5 multiplied by 5 equals 25.\n",
      "--------------------------------------------------\n",
      "Running model: Qwen/Qwen2-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for Qwen/Qwen2-1.5B-Instruct: 2.96 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 505f3020-2cf5-498a-9ff3-b2097db0e260\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 5*5?\n",
      "assistant\n",
      "The product of 5 and 5 is 25.\n",
      "\u001b[22m\u001b[39mConversation ID: 7aa327d7-efe6-43c5-a89b-5cdef45ed83b\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 4*4?\n",
      "assistant\n",
      "The answer to \"what is 4*4?\" is:\n",
      "\n",
      "16. \n",
      "\n",
      "This is because the multiplication of two numbers in any order will result\n",
      "\u001b[22m\u001b[39mConversation ID: 82ad53d9-39db-4de1-b7ab-22624dedf97f\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 6*6?\n",
      "assistant\n",
      "6*6 equals 36.\n",
      "\u001b[22m\u001b[39mConversation ID: 94d4726c-2696-49c4-b1d7-36e5dc8702c2\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 3*3?\n",
      "assistant\n",
      "9.\n",
      "\u001b[22m\u001b[39mConversation ID: b125c443-30d9-411f-8527-e13fffeac84f\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is 7*7?\n",
      "assistant\n",
      "The answer to 7*7 is 49.\n",
      "--------------------------------------------------\n",
      "Running model: stabilityai/stablelm-2-zephyr-1_6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for stabilityai/stablelm-2-zephyr-1_6b: 5.31 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 192f01a1-5cae-400a-ac82-237ba7236169\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 4*4?\n",
      "<|assistant|>\n",
      "4*4 is equal to 16. When multiplying numbers, you multiply the first number by the second number, and then multiply the result by\n",
      "\u001b[22m\u001b[39mConversation ID: 740b6228-2f9c-44d7-8351-4fc0c18e17d1\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 7*7?\n",
      "<|assistant|>\n",
      "7*7 is equal to 49. It is a number that represents the result of multiplying 7 by itself, 7 times. In\n",
      "\u001b[22m\u001b[39mConversation ID: 75743161-3025-4f41-9aad-3d27a62d8e39\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 3*3?\n",
      "<|assistant|>\n",
      "To calculate 3*3, you can simply multiply the numbers together:\n",
      "3*3 = 3 + 3 = 6.\n",
      "\n",
      "So,\n",
      "\u001b[22m\u001b[39mConversation ID: 9b125a31-eea8-4ebe-8f60-30e4de402f2d\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 6*6?\n",
      "<|assistant|>\n",
      "6*6 is equal to 36. It is a number that can be multiplied by 6 to get the result of 36.\n",
      "\u001b[22m\u001b[39mConversation ID: ff3fc4e6-e2e5-4c80-bb6f-16b3047b9f4a\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 5*5?\n",
      "<|assistant|>\n",
      "The number 5*5 is equal to 25. In other words, 5 multiplied by 5 equals 25. This can\n",
      "--------------------------------------------------\n",
      "Running model: stabilityai/stablelm-zephyr-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for stabilityai/stablelm-zephyr-3b: 8.37 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 4beb3c11-92be-4dfc-99f6-87daf87c3f19\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 4*4?\n",
      "<|assistant|>\n",
      "4*4 is equal to 16.\n",
      "Here's an explanation:\n",
      "When you multiply a number by itself, you're basically saying \"I want\n",
      "\u001b[22m\u001b[39mConversation ID: 9e84a87e-6398-47de-b3b4-11c1dd73bd8d\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 6*6?\n",
      "<|assistant|>\n",
      "6*6 is equal to 36.\n",
      "Here's an explanation for a five-year-old:\n",
      "\n",
      "When we multiply 6 by 6,\n",
      "\u001b[22m\u001b[39mConversation ID: b91a90c2-a944-419f-aeac-a57536b77ed1\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 3*3?\n",
      "<|assistant|>\n",
      "3*3 is equal to 9.\n",
      "Here's an explanation for a five-year-old:\n",
      "\n",
      "When we multiply 3 by 3,\n",
      "\u001b[22m\u001b[39mConversation ID: c57f9440-1a32-4afd-8681-955bfa0acf23\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 5*5?\n",
      "<|assistant|>\n",
      "5*5 is equal to 25.\n",
      "\n",
      "Here's how I arrived at this conclusion:\n",
      "\n",
      "1. The question is asking for the product\n",
      "\u001b[22m\u001b[39mConversation ID: fc2dce08-b0e0-49c1-8746-eaa0c6923883\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 7*7?\n",
      "<|assistant|>\n",
      "7*7 is 49.\n",
      "\n",
      "Here's how I arrived at this answer:\n",
      "\n",
      "1. Start with the number 7.\n",
      "2.\n",
      "--------------------------------------------------\n",
      "Running model: google/gemma-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6fd1c90c4343389bf080da48dd0379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for google/gemma-2b: 7.87 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 1f592b01-a87c-4a16-94ac-5269dca38e70\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: <|im_start|>user\n",
      "What is 4*4?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_im|>im_im_im_im_im_im_im_im_im_im_im_im_\n",
      "\u001b[22m\u001b[39mConversation ID: 26c095a9-b05f-4236-b691-3aa93378f169\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: <|im_start|>user\n",
      "What is 6*6?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>\n",
      "<|im_end|>\n",
      "<|im_start|>\n",
      "<|\n",
      "\u001b[22m\u001b[39mConversation ID: 40eacc48-170f-4a35-a2a1-da784e43d2e4\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: <|im_start|>user\n",
      "What is 7*7?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_assistant|>\n",
      "<|im_assistant|>\n",
      "<|im_assistant|>\n",
      "<|im_assistant|>\n",
      "<|\n",
      "\u001b[22m\u001b[39mConversation ID: a09602fc-18c3-41b6-8823-8210e9980baa\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: <|im_start|>user\n",
      "What is 3*3?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|\n",
      "\u001b[22m\u001b[39mConversation ID: e40eeab4-bd7b-4dfd-a5eb-416da746d52f\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: <|im_start|>user\n",
      "What is 5*5?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|\n",
      "--------------------------------------------------\n",
      "Running model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Average response time for TinyLlama/TinyLlama-1.1B-Chat-v1.0: 3.73 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 1f1a6661-f113-4143-874f-4b1f6cb37ce2\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 7*7? \n",
      "<|assistant|>\n",
      "7*7 = 49\n",
      "\n",
      "7 multiplied by 7 equals 49.\n",
      "\u001b[22m\u001b[39mConversation ID: 38d6f2e5-0a34-424c-a5dd-257baa621b41\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 5*5? \n",
      "<|assistant|>\n",
      "5*5 is a mathematical expression that represents the product of 5 and 5. It is written as 5 x 5.\n",
      "\u001b[22m\u001b[39mConversation ID: 647e1b69-2733-4ece-bb1e-32772b3d2ad9\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 4*4? \n",
      "<|assistant|>\n",
      "4*4 is a mathematical expression that represents the product of four 4-digit numbers. It is written as 4 x 4 x \n",
      "\u001b[22m\u001b[39mConversation ID: aaa79885-1bef-4ee4-972f-905555b5587b\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 3*3? \n",
      "<|assistant|>\n",
      "3*3 is the product of the numbers 3 and 3. It is equal to 9.\n",
      "\u001b[22m\u001b[39mConversation ID: fbc4ad8c-9d8e-4988-8f00-6bdcd527576f\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: <|user|>\n",
      "What is 6*6? \n",
      "<|assistant|>\n",
      "Yes, 6*6 is a common shorthand notation for multiplying two numbers by 6. It is written as 6 x \n",
      "--------------------------------------------------\n",
      "Running model: princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT: 3.17 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 06b1f37a-30ff-4592-8bf4-8429496e6bb4\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 6*6? [/INST]\n",
      "\n",
      "6*6 is equal to 6x6 = 66.\n",
      "\u001b[22m\u001b[39mConversation ID: 22efae3f-0bf8-4129-a150-9c77355415ea\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 7*7? [/INST]\n",
      "\n",
      "7*7 is equal to 7 x 7 = 77.\n",
      "\u001b[22m\u001b[39mConversation ID: 35813c4e-e90a-4f09-9934-e8732cd0d1f2\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 4*4? [/INST]\n",
      "\n",
      "4*4 is equal to 4 times 4, which is 16.\n",
      "\u001b[22m\u001b[39mConversation ID: 91ca9bfc-af37-4685-a18e-2255bb25448f\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 5*5? [/INST]\n",
      "\n",
      "I'm sorry, I'm not sure what you're asking. Could you please provide more context or clarify your question?\n",
      "\u001b[22m\u001b[39mConversation ID: d5c25d93-03e4-4e58-9508-7aed7e2f6cfb\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 3*3? [/INST]\n",
      "\n",
      "3*3 is equal to 3x3.\n",
      "--------------------------------------------------\n",
      "Running model: princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b048c8e7874ea08a017ee5115b866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT: 8.54 seconds\n",
      "\n",
      "\u001b[22m\u001b[39mConversation ID: 21e5b6f2-abc3-4f76-abba-2b19a98fa28c\n",
      "\u001b[1m\u001b[34muser: What is 4*4?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 4*4? [/INST]\n",
      "\n",
      "[INST]What is 4^4? [/INST]\n",
      "\n",
      "[INST]What is 4^4? [/\n",
      "\u001b[22m\u001b[39mConversation ID: 5768ec92-b036-4b5b-a518-f3eeea7bc18a\n",
      "\u001b[1m\u001b[34muser: What is 5*5?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 5*5? [/INST]\n",
      "\n",
      "[INST]What is 5*5? [/INST]\n",
      "\n",
      "[INST]What is 5*5? [/\n",
      "\u001b[22m\u001b[39mConversation ID: 7712a301-0164-4271-8251-c6f862e21fe3\n",
      "\u001b[1m\u001b[34muser: What is 6*6?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 6*6? [/INST]\n",
      "\n",
      "[INST] What is 6*6? [/INST]\n",
      "\n",
      "[INST] What is 6*6? [/\n",
      "\u001b[22m\u001b[39mConversation ID: aed614ec-74bd-4fcc-abc8-eab53f89fb39\n",
      "\u001b[1m\u001b[34muser: What is 3*3?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 3*3? [/INST]\n",
      "\n",
      "[INST]What is 3*3? [/INST]\n",
      "\n",
      "[INST]What is 3*3? [/\n",
      "\u001b[22m\u001b[39mConversation ID: c058888c-7176-4141-a128-15c8b4116171\n",
      "\u001b[1m\u001b[34muser: What is 7*7?\n",
      "\u001b[22m\u001b[33massistant: [INST] What is 7*7? [/INST]\n",
      "\n",
      "[INST]What is 7*7? [/INST]\n",
      "\n",
      "[INST]What is 7*7? [/\n",
      "--------------------------------------------------\n",
      "Model Average Response Times:\n",
      "microsoft/Phi-3-mini-4k-instruct: 4.89 seconds\n",
      "HuggingFaceTB/SmolLM-135M-Instruct: 3.09 seconds\n",
      "HuggingFaceTB/SmolLM-360M-Instruct: 3.31 seconds\n",
      "HuggingFaceTB/SmolLM-1.7B-Instruct: 5.87 seconds\n",
      "Qwen/Qwen2-0.5B-Instruct: 1.38 seconds\n",
      "Qwen/Qwen2-1.5B-Instruct: 2.96 seconds\n",
      "stabilityai/stablelm-2-zephyr-1_6b: 5.31 seconds\n",
      "stabilityai/stablelm-zephyr-3b: 8.37 seconds\n",
      "google/gemma-2b: 7.87 seconds\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0: 3.73 seconds\n",
      "princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT: 3.17 seconds\n",
      "princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT: 8.54 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyrit.prompt_target import HuggingFaceChatTarget  \n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "\n",
    "# List of models to iterate through\n",
    "model_list = [\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"HuggingFaceTB/SmolLM-135M-Instruct\",\n",
    "    \"HuggingFaceTB/SmolLM-360M-Instruct\",\n",
    "    \"HuggingFaceTB/SmolLM-1.7B-Instruct\",\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    \"stabilityai/stablelm-2-zephyr-1_6b\",\n",
    "    \"stabilityai/stablelm-zephyr-3b\",\n",
    "    \"google/gemma-2b\",\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT\",\n",
    "    \"princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT\"\n",
    "]\n",
    "   \n",
    "# List of prompts to send\n",
    "prompt_list = [\n",
    "        \"What is 3*3?\",\n",
    "        \"What is 4*4?\",\n",
    "        \"What is 5*5?\",\n",
    "        \"What is 6*6?\",\n",
    "        \"What is 7*7?\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Dictionary to store average response times\n",
    "model_times = {}\n",
    "    \n",
    "for model_id in model_list:\n",
    "    print(f\"Running model: {model_id}\")\n",
    "        \n",
    "    try:\n",
    "        # Initialize HuggingFaceChatTarget with the current model\n",
    "        target = HuggingFaceChatTarget(\n",
    "            model_id=model_id,          # Use the desired model ID\n",
    "            use_cuda=False,             # Set to True if using CUDA\n",
    "            tensor_format=\"pt\",\n",
    "            verbose=False,              # Set to True for detailed logs\n",
    "            max_new_tokens=30\n",
    "        )\n",
    "            \n",
    "        # Initialize the orchestrator\n",
    "        orchestrator = PromptSendingOrchestrator(\n",
    "            prompt_target=target,\n",
    "            verbose=False\n",
    "        )\n",
    "            \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "            \n",
    "        # Send prompts asynchronously\n",
    "        responses = await orchestrator.send_prompts_async(prompt_list=prompt_list)\n",
    "            \n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "            \n",
    "        # Calculate total and average response time\n",
    "        total_time = end_time - start_time\n",
    "        avg_time = total_time / len(prompt_list)\n",
    "        model_times[model_id] = avg_time\n",
    "            \n",
    "        print(f\"Average response time for {model_id}: {avg_time:.2f} seconds\\n\")\n",
    "            \n",
    "        # Print the conversations\n",
    "        orchestrator.print_conversations()\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with model {model_id}: {e}\\n\")\n",
    "        model_times[model_id] = None\n",
    "        continue\n",
    "    \n",
    "# Print all model average times\n",
    "print(\"Model Average Response Times:\")\n",
    "for model_id, avg_time in model_times.items():\n",
    "    if avg_time is not None:\n",
    "        print(f\"{model_id}: {avg_time:.2f} seconds\")\n",
    "    else:\n",
    "        print(f\"{model_id}: Error occurred, no average time calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55610f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
