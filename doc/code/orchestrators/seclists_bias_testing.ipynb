{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520e0b78",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# SecLists Bias Testing - optional\n",
    "\n",
    "This notebook demonstrates the process of using SecLists AI LLM Bias Testing examples to evaluate the bias of an AI language model.\n",
    "It includes fetching bias testing prompts from the SecLists repository, sending them to the model, and scoring the model's responses based on predefined fairness and bias scales.\n",
    "The goal is to assess how well the AI model handles various biases in its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c6a01",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from pyrit.common import default_values\n",
    "from pyrit.datasets import fetch_seclists_bias_testing_examples\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import SelfAskLikertScorer, LikertScalePaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebe213",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "default_values.load_environment_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e7db7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "prompt_target = OpenAIChatTarget()\n",
    "examples_source = \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Ai/LLM_Testing/Bias_Testing/nationality_geographic_bias.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d8e2f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Note:\n",
    "# The dataset sources can be found at:\n",
    "# - GitHub repository: https://github.com/danielmiessler/SecLists/tree/master/Ai/LLM_Testing/Bias_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22df8d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set up the Bias Scorer with the same prompt target to ensure consistency\n",
    "bias_scorer = SelfAskLikertScorer(\n",
    "    likert_scale_path=LikertScalePaths.FAIRNESS_BIAS_SCALE.value, chat_target=prompt_target\n",
    ")\n",
    "\n",
    "# Create the orchestrator with scorer\n",
    "orchestrator = PromptSendingOrchestrator(prompt_target=prompt_target, scorers=[bias_scorer])\n",
    "\n",
    "# Fetch examples from SecLists Bias Testing dataset\n",
    "prompt_dataset = fetch_seclists_bias_testing_examples(examples_source, source_type=\"public_url\")\n",
    "\n",
    "# Use the first 5 examples\n",
    "prompt_list = prompt_dataset.prompts[:4]\n",
    "\n",
    "# Send prompts using the orchestrator\n",
    "responses = await orchestrator.send_prompts_async(prompt_list=prompt_list)  # type: ignore\n",
    "\n",
    "# Retrieve the memory to print scoring results\n",
    "orchestrator.print_conversations()  # Use built-in method to display conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde0433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
