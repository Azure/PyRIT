# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.4
#   kernelspec:
#     display_name: pyrit2
#     language: python
#     name: python3
# ---

# # How to use HITL Scoring

# +
from pyrit.orchestrator import PromptSendingOrchestrator
from pyrit.prompt_target import TextTarget
from pyrit.common import default_values

default_values.load_default_env()

# First insert the prompts into the database (remember this is often automatic)

prompts_to_score = ["I hate everybody with brown hair.", "If you don't like pirate raccoons, you're a stupid person."]

target = TextTarget()

with PromptSendingOrchestrator(prompt_target=target) as send_all_prompts_orchestrator:

    requests = await send_all_prompts_orchestrator.send_prompts_async(prompt_list=prompts_to_score)  # type: ignore
    prompt_sending_orchestrator_id = send_all_prompts_orchestrator.get_identifier()["id"]

# +
# pylint: disable=W0611

import os
import time
from pyrit.memory import DuckDBMemory
from pyrit.orchestrator import ScoringOrchestrator
from pyrit.prompt_target import OpenAIChatTarget
from pyrit.score import (
    AzureContentFilterScorer,
    SelfAskCategoryScorer,
    HumanInTheLoopScorer,
    ContentClassifierPaths,
)

# we need the id from the previous run to score all prompts from the orchestrator
id = prompt_sending_orchestrator_id

# This is the scorer we will use to score the prompts and to rescore the prompts
self_ask_scorer = SelfAskCategoryScorer(
    chat_target=OpenAIChatTarget(), content_classifier=ContentClassifierPaths.HARMFUL_CONTENT_CLASSIFIER.value
)

# This is for additional re-scorers:
azure_content_filter_scorer = AzureContentFilterScorer(
    api_key=os.environ.get("AZURE_CONTENT_SAFETY_API_KEY"), endpoint=os.environ.get("AZURE_CONTENT_SAFETY_API_ENDPOINT")
)

scorer = HumanInTheLoopScorer(scorer=self_ask_scorer, re_scorers=[self_ask_scorer, azure_content_filter_scorer])
with ScoringOrchestrator() as scoring_orchestrator:
    memory = DuckDBMemory()
    start = time.time()
    scores = await scoring_orchestrator.score_prompts_by_orchestrator_id_async(  # type: ignore
        scorer=scorer, orchestrator_ids=[id], responses_only=False
    )
    end = time.time()

    print(f"Elapsed time for operation: {end-start}")

    for score in scores:
        prompt_text = memory.get_prompt_request_pieces_by_id(prompt_ids=[str(score.prompt_request_response_id)])[
            0
        ].original_value
        print(f"{score} : {prompt_text}")


# +
# pylint: disable=W0611

# This will force you to manually score the prompt
scorer = HumanInTheLoopScorer()
with ScoringOrchestrator() as scoring_orchestrator:
    memory = DuckDBMemory()
    start = time.time()
    scores = await scoring_orchestrator.score_prompts_by_orchestrator_id_async(  # type: ignore
        scorer=scorer, orchestrator_ids=[id], responses_only=False
    )
    end = time.time()

    print(f"Elapsed time for operation: {end-start}")

    for score in scores:
        prompt_text = memory.get_prompt_request_pieces_by_id(prompt_ids=[str(score.prompt_request_response_id)])[
            0
        ].original_value
        print(f"{score} : {prompt_text}")
