{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Orchestrator\n",
    "\n",
    "A very common scenario is wanting to score multiple prompts. Here is a general strategy to do that using `ScoringOrchestrator`.\n",
    "\n",
    "1. Get the `PromptRequestPiece`s into the database. This is done automatically if using any targets (e.g. running any of the demos), but even if you manually entered the prompts outside of PyRIT, you can import them using `TextTarget`s or csvs as described [here](../memory/memory.ipynb)\n",
    "1. Scoring all prompts in the database that meet any criteria\n",
    "\n",
    "The following example demonstrates this by manually entering prompts in the database and then scoring them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__type__': 'TextTarget', '__module__': 'pyrit.prompt_target.text_target'}: user: I'm sorry, I can't do that.\n",
      "{'__type__': 'TextTarget', '__module__': 'pyrit.prompt_target.text_target'}: user: I hate you.\n",
      "{'__type__': 'TextTarget', '__module__': 'pyrit.prompt_target.text_target'}: user: nothing wrong with this.\n",
      "{'__type__': 'TextTarget', '__module__': 'pyrit.prompt_target.text_target'}: user: test\n"
     ]
    }
   ],
   "source": [
    "from pyrit.orchestrator.prompt_sending_orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.prompt_target import TextTarget\n",
    "\n",
    "\n",
    "# First insert the prompts into the database (remember this is often automatic)\n",
    "\n",
    "prompts_to_score = [\n",
    "    \"I'm sorry, I can't do that.\",\n",
    "    \"I hate you.\",\n",
    "    \"nothing wrong with this.\"\n",
    "]\n",
    "\n",
    "target = TextTarget()\n",
    "send_all_prompts_orchestrator = PromptSendingOrchestrator(prompt_target=target)\n",
    "\n",
    "requests = await send_all_prompts_orchestrator.send_prompts_async(prompt_list=prompts_to_score) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the prompts are in the database (which again, is often automatic) we can use `ScoringOrchestrator` to score them with whatever scorers we want. It works in parallel with batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureContentFilterScorer: Hate: 0.0 : test\n",
      "AzureContentFilterScorer: SelfHarm: 0.0 : test\n",
      "AzureContentFilterScorer: Sexual: 0.0 : test\n",
      "AzureContentFilterScorer: Violence: 0.0 : test\n",
      "AzureContentFilterScorer: Hate: 0.0 : I'm sorry, I can't do that.\n",
      "AzureContentFilterScorer: SelfHarm: 0.0 : I'm sorry, I can't do that.\n",
      "AzureContentFilterScorer: Sexual: 0.0 : I'm sorry, I can't do that.\n",
      "AzureContentFilterScorer: Violence: 0.0 : I'm sorry, I can't do that.\n",
      "AzureContentFilterScorer: Hate: 0.0 : nothing wrong with this.\n",
      "AzureContentFilterScorer: SelfHarm: 0.0 : nothing wrong with this.\n",
      "AzureContentFilterScorer: Sexual: 0.0 : nothing wrong with this.\n",
      "AzureContentFilterScorer: Violence: 0.0 : nothing wrong with this.\n",
      "AzureContentFilterScorer: Hate: 0.2857142857142857 : I hate you.\n",
      "AzureContentFilterScorer: SelfHarm: 0.0 : I hate you.\n",
      "AzureContentFilterScorer: Sexual: 0.0 : I hate you.\n",
      "AzureContentFilterScorer: Violence: 0.0 : I hate you.\n"
     ]
    }
   ],
   "source": [
    "from pyrit.memory import DuckDBMemory\n",
    "from pyrit.orchestrator.scoring_orchestrator import ScoringOrchestrator\n",
    "from pyrit.prompt_target.prompt_chat_target.openai_chat_target import AzureOpenAIChatTarget\n",
    "from pyrit.score import AzureContentFilterScorer, SelfAskCategoryScorer, HumanInTheLoopScorer, ContentClassifierPaths\n",
    "\n",
    "# we need the id from the previous run to score all prompts from the orchestrator\n",
    "id = send_all_prompts_orchestrator.get_identifier()[\"id\"]\n",
    "\n",
    "# The scorer is interchangeable with other scorers\n",
    "scorer = AzureContentFilterScorer()\n",
    "# scorer = HumanInTheLoopScorer()\n",
    "# scorer = SelfAskCategoryScorer(chat_target=AzureOpenAIChatTarget(), content_classifier=ContentClassifierPaths.HARMFUL_CONTENT_CLASSIFIER.value)\n",
    "\n",
    "scoring_orchestrator = ScoringOrchestrator()\n",
    "scores = await scoring_orchestrator.score_prompts_by_orchestrator_id_async(scorer=scorer, orchestrator_ids=[id], responses_only=False)\n",
    "\n",
    "memory = DuckDBMemory()\n",
    "\n",
    "for score in scores:\n",
    "    prompt_text = memory.get_prompt_request_pieces_by_id(prompt_ids=[score.prompt_request_response_id])[0].original_value\n",
    "    print(f\"{score} : {prompt_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the `ScoringOrchestrator` class can be extended to score any SQL query in parallel. If you have a scenario, any set of prompts you want to be scored, this is a great place to open an issue or contribute to the project :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
