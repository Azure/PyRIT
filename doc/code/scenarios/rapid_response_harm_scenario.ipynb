{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Rapid Response Harm Testing\n",
    "\n",
    "This notebook demonstrates the usage of the RapidResponseHarmScenario class to test model behavior with respect to various harm categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Initialize PyRIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pyrit.memory import CentralMemory\n",
    "from pyrit.setup.initialization import IN_MEMORY, initialize_pyrit\n",
    "\n",
    "# Initialize PyRIT with SQLite storage\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
    "memory = CentralMemory.get_memory_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Loading the data into memory\n",
    "\n",
    "Before running the scenario, we need to ensure that the relevant datasets are loaded into memory. In the following cells, we will be testing datasets for hate speech, violence, and harassment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyrit.common.path import DATASETS_PATH\n",
    "from pyrit.models import SeedDataset\n",
    "\n",
    "# Import seed prompts\n",
    "for harm in [\"hate\", \"violence\", \"harassment\"]:\n",
    "    seed_prompts = SeedDataset.from_yaml_file(\n",
    "        Path(DATASETS_PATH) / \"seed_prompts\" / \"harms\" / f\"{harm}.prompt\"\n",
    "    )\n",
    "    await memory.add_seeds_to_memory_async(prompts=[*seed_prompts.prompts, *seed_prompts.objectives], added_by=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Running Multiple Harm Strategies\n",
    "\n",
    "Now we run the strategies using the datasets we defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created scenario: Rapid Response Harm Scenario\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f942474e523f486ba369c27d8fbbb803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing Rapid Response Harm Scenario:   0%|          | 0/4 [00:00<?, ?attack/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\u001b[1m\u001b[36m                           ðŸ“Š SCENARIO RESULTS: RapidResponseHarmScenario                            \u001b[0m\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Scenario Information\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[1m  ðŸ“‹ Scenario Details\u001b[0m\n",
      "\u001b[36m    â€¢ Name: RapidResponseHarmScenario\u001b[0m\n",
      "\u001b[36m    â€¢ Scenario Version: 1\u001b[0m\n",
      "\u001b[36m    â€¢ PyRIT Version: 0.10.0.dev0\u001b[0m\n",
      "\u001b[36m    â€¢ Description:\u001b[0m\n",
      "\u001b[36m        Rapid Response Harm Scenario implementation for PyRIT. This scenario contains various harm-based checks that you\u001b[0m\n",
      "\u001b[36m        can run to get a quick idea about model behavior with respect to certain harm categories.\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸŽ¯ Target Information\u001b[0m\n",
      "\u001b[36m    â€¢ Target Type: OpenAIChatTarget\u001b[0m\n",
      "\u001b[36m    â€¢ Target Model: gpt-4o\u001b[0m\n",
      "\u001b[36m    â€¢ Target Endpoint: https://pyrit-sweden.openai.azure.com/openai/deployments/gpt-4o-unsafe/chat/completions\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ“Š Scorer Information\u001b[0m\n",
      "\u001b[36m    â€¢ Scorer Type: TrueFalseInverterScorer\u001b[0m\n",
      "\u001b[36m      â””â”€ Wraps:\u001b[0m\n",
      "\u001b[36m        â€¢ Scorer Type: SelfAskRefusalScorer\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Overall Statistics\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[1m  ðŸ“ˆ Summary\u001b[0m\n",
      "\u001b[32m    â€¢ Total Strategies: 4\u001b[0m\n",
      "\u001b[32m    â€¢ Total Attack Results: 19\u001b[0m\n",
      "\u001b[31m    â€¢ Overall Success Rate: 100%\u001b[0m\n",
      "\u001b[32m    â€¢ Unique Objectives: 16\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Per-Strategy Breakdown\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: baseline\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 3\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: hate\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 3\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: violence\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 10\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: harassment\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 3\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.scenarios.printer.console_printer import ConsoleScenarioResultPrinter\n",
    "from pyrit.scenarios.scenarios.ai_rt.rapid_response_harm_scenario import (\n",
    "    RapidResponseHarmScenario,\n",
    "    RapidResponseHarmStrategy,\n",
    ")\n",
    "\n",
    "printer = ConsoleScenarioResultPrinter()\n",
    "\n",
    "# Create RapidResponseHarmScenario instance for hate speech testing\n",
    "rapid_response_harm_scenario = RapidResponseHarmScenario(\n",
    "    objective_target= OpenAIChatTarget(\n",
    "            endpoint=os.environ.get(\"AZURE_OPENAI_GPT4O_UNSAFE_ENDPOINT\"),\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_GPT4O_UNSAFE_CHAT_KEY\"),\n",
    "        ),\n",
    "    scenario_strategies=[RapidResponseHarmStrategy.Hate,\n",
    "    RapidResponseHarmStrategy.Violence,\n",
    "    RapidResponseHarmStrategy.Harassment],\n",
    ")\n",
    "\n",
    "# Run hate speech tests\n",
    "print(f\"Created scenario: {rapid_response_harm_scenario.name}\")\n",
    "await rapid_response_harm_scenario.initialize_async()\n",
    "\n",
    "# Execute the entire scenario\n",
    "rapid_response_harm_results = await rapid_response_harm_scenario.run_async() # type: ignore\n",
    "await printer.print_summary_async(rapid_response_harm_results) # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit-dev-10-14",
   "language": "python",
   "name": "pyrit-dev-10-14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
