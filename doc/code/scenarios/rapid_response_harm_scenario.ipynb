{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Rapid Response Harm Testing\n",
    "\n",
    "This notebook demonstrates the usage of the RapidResponseHarmScenario class to test model behavior with respect to various harm categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Initialize PyRIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pyrit.memory import CentralMemory\n",
    "from pyrit.setup.initialization import IN_MEMORY, initialize_pyrit\n",
    "\n",
    "# Initialize PyRIT with IN_MEMORY storage\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
    "memory = CentralMemory.get_memory_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Loading the data into memory\n",
    "\n",
    "Before running the scenario, we need to ensure that the relevant datasets are loaded into memory. We have provided a sample set of harm-related seed prompts and are loading them into memory in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyrit.common.path import DATASETS_PATH\n",
    "from pyrit.models import SeedDataset\n",
    "\n",
    "# Import seed prompts\n",
    "for harm in [\"hate\", \"violence\", \"harassment\", \"leakage\", \"sexual\", \"fairness\", \"misinformation\"]:\n",
    "    seed_prompts = SeedDataset.from_yaml_file(Path(DATASETS_PATH) / \"seed_prompts\" / \"harms\" / f\"{harm}.prompt\")\n",
    "    await memory.add_seeds_to_memory_async(prompts=[*seed_prompts.prompts, *seed_prompts.objectives], added_by=\"test\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Running Multiple Harm Strategies\n",
    "\n",
    "Now we can run the strategies using the datasets we defined above! In this first example, we'll run all the strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created scenario: Rapid Response Harm Scenario\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badc54dba1334b858a481da8a1b1e07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing Rapid Response Harm Scenario:   0%|          | 0/15 [00:00<?, ?attack/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\u001b[1m\u001b[36m                           ðŸ“Š SCENARIO RESULTS: RapidResponseHarmScenario                            \u001b[0m\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Scenario Information\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[1m  ðŸ“‹ Scenario Details\u001b[0m\n",
      "\u001b[36m    â€¢ Name: RapidResponseHarmScenario\u001b[0m\n",
      "\u001b[36m    â€¢ Scenario Version: 1\u001b[0m\n",
      "\u001b[36m    â€¢ PyRIT Version: 0.10.0.dev0\u001b[0m\n",
      "\u001b[36m    â€¢ Description:\u001b[0m\n",
      "\u001b[36m        Rapid Response Harm Scenario implementation for PyRIT. This scenario contains various harm-based checks that you\u001b[0m\n",
      "\u001b[36m        can run to get a quick idea about model behavior with respect to certain harm categories.\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸŽ¯ Target Information\u001b[0m\n",
      "\u001b[36m    â€¢ Target Type: OpenAIChatTarget\u001b[0m\n",
      "\u001b[36m    â€¢ Target Model: gpt-4o\u001b[0m\n",
      "\u001b[36m    â€¢ Target Endpoint: https://pyrit-sweden.openai.azure.com/openai/deployments/gpt-4o-unsafe/chat/completions\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ“Š Scorer Information\u001b[0m\n",
      "\u001b[36m    â€¢ Scorer Type: TrueFalseInverterScorer\u001b[0m\n",
      "\u001b[36m      â””â”€ Wraps:\u001b[0m\n",
      "\u001b[36m        â€¢ Scorer Type: SelfAskRefusalScorer\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Overall Statistics\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[1m  ðŸ“ˆ Summary\u001b[0m\n",
      "\u001b[32m    â€¢ Total Strategies: 8\u001b[0m\n",
      "\u001b[32m    â€¢ Total Attack Results: 74\u001b[0m\n",
      "\u001b[31m    â€¢ Overall Success Rate: 95%\u001b[0m\n",
      "\u001b[32m    â€¢ Unique Objectives: 34\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Per-Strategy Breakdown\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: baseline\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 6\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 83%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: leakage\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 12\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 91%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: violence\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 20\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 95%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: misinformation\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 12\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: hate\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 6\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: fairness\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 2\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: sexual\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 10\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: harassment\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 6\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.scenarios.printer.console_printer import ConsoleScenarioResultPrinter\n",
    "from pyrit.scenarios.scenarios.ai_rt.rapid_response_harm_scenario import (\n",
    "    RapidResponseHarmScenario,\n",
    "    RapidResponseHarmStrategy,\n",
    ")\n",
    "\n",
    "printer = ConsoleScenarioResultPrinter()\n",
    "\n",
    "# Create RapidResponseHarmScenario instance for all harm strategies\n",
    "rapid_response_harm_scenario = RapidResponseHarmScenario(\n",
    "    objective_target=OpenAIChatTarget(\n",
    "        endpoint=os.environ.get(\"AZURE_OPENAI_GPT4O_UNSAFE_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_GPT4O_UNSAFE_CHAT_KEY\"),\n",
    "    ),\n",
    "    scenario_strategies=[\n",
    "        RapidResponseHarmStrategy.ALL\n",
    "    ],\n",
    "    # Uncomment the following line to use a custom dataset prefix, we're using the default here\n",
    "    # seed_dataset_prefix==\"custom_prefix\", \n",
    ")\n",
    "\n",
    "# Run strategies\n",
    "print(f\"Created scenario: {rapid_response_harm_scenario.name}\")\n",
    "await rapid_response_harm_scenario.initialize_async()  # type: ignore\n",
    "\n",
    "# Execute the entire scenario\n",
    "rapid_response_harm_results = await rapid_response_harm_scenario.run_async()  # type: ignore\n",
    "await printer.print_summary_async(rapid_response_harm_results)  # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "\n",
    "We can also selectively choose which strategies to run. In this next example, we'll only run the Hate, Violence, and Harassment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created scenario: Rapid Response Harm Scenario\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4fc5c58c234623a2aa6498266be73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing Rapid Response Harm Scenario:   0%|          | 0/7 [00:00<?, ?attack/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\u001b[1m\u001b[36m                           ðŸ“Š SCENARIO RESULTS: RapidResponseHarmScenario                            \u001b[0m\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Scenario Information\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[1m  ðŸ“‹ Scenario Details\u001b[0m\n",
      "\u001b[36m    â€¢ Name: RapidResponseHarmScenario\u001b[0m\n",
      "\u001b[36m    â€¢ Scenario Version: 1\u001b[0m\n",
      "\u001b[36m    â€¢ PyRIT Version: 0.10.0.dev0\u001b[0m\n",
      "\u001b[36m    â€¢ Description:\u001b[0m\n",
      "\u001b[36m        Rapid Response Harm Scenario implementation for PyRIT. This scenario contains various harm-based checks that you\u001b[0m\n",
      "\u001b[36m        can run to get a quick idea about model behavior with respect to certain harm categories.\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸŽ¯ Target Information\u001b[0m\n",
      "\u001b[36m    â€¢ Target Type: OpenAIChatTarget\u001b[0m\n",
      "\u001b[36m    â€¢ Target Model: gpt-4o\u001b[0m\n",
      "\u001b[36m    â€¢ Target Endpoint: https://pyrit-sweden.openai.azure.com/openai/deployments/gpt-4o-unsafe/chat/completions\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ“Š Scorer Information\u001b[0m\n",
      "\u001b[36m    â€¢ Scorer Type: TrueFalseInverterScorer\u001b[0m\n",
      "\u001b[36m      â””â”€ Wraps:\u001b[0m\n",
      "\u001b[36m        â€¢ Scorer Type: SelfAskRefusalScorer\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Overall Statistics\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[1m  ðŸ“ˆ Summary\u001b[0m\n",
      "\u001b[32m    â€¢ Total Strategies: 4\u001b[0m\n",
      "\u001b[32m    â€¢ Total Attack Results: 35\u001b[0m\n",
      "\u001b[31m    â€¢ Overall Success Rate: 100%\u001b[0m\n",
      "\u001b[32m    â€¢ Unique Objectives: 16\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36mâ–¼ Per-Strategy Breakdown\u001b[0m\n",
      "\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: baseline\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 3\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: hate\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 6\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: violence\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 20\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[1m  ðŸ”¸ Strategy: harassment\u001b[0m\n",
      "\u001b[33m    â€¢ Number of Results: 6\u001b[0m\n",
      "\u001b[31m    â€¢ Success Rate: 100%\u001b[0m\n",
      "\n",
      "\u001b[36mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create RapidResponseHarmScenario instance for hate, violence, and harassment testing\n",
    "rapid_response_harm_scenario = RapidResponseHarmScenario(\n",
    "    objective_target=OpenAIChatTarget(\n",
    "        endpoint=os.environ.get(\"AZURE_OPENAI_GPT4O_UNSAFE_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_GPT4O_UNSAFE_CHAT_KEY\"),\n",
    "    ),\n",
    "    scenario_strategies=[\n",
    "        RapidResponseHarmStrategy.Hate,\n",
    "        RapidResponseHarmStrategy.Violence,\n",
    "        RapidResponseHarmStrategy.Harassment,\n",
    "    ],\n",
    "    # Uncomment the following line to use a custom dataset prefix, we're using the default here\n",
    "    # seed_dataset_prefix==\"custom_prefix\", \n",
    ")\n",
    "\n",
    "# Run strategies\n",
    "print(f\"Created scenario: {rapid_response_harm_scenario.name}\")\n",
    "await rapid_response_harm_scenario.initialize_async()  # type: ignore\n",
    "\n",
    "# Execute the entire scenario\n",
    "rapid_response_harm_results = await rapid_response_harm_scenario.run_async()  # type: ignore\n",
    "await printer.print_summary_async(rapid_response_harm_results)  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
