{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Configuring Scenarios\n",
    "\n",
    "This notebook demonstrates how to use a composite strategies — the `FoundryStrategy` — to test a target with multiple\n",
    "attack strategies.\n",
    "A \"composite strategy\"  This class encapsulates a collection of ScenarioStrategy instances along with\n",
    "an auto-generated descriptive name, making it easy to represent both single strategies\n",
    "and composed multi-strategy attacks.\n",
    "\n",
    "The `Foundry` scenario provides a comprehensive testing approach that includes:\n",
    "- **Converter-based attacks**: Apply various encoding/obfuscation techniques (Base64, Caesar cipher, etc.)\n",
    "- **Multi-turn attacks**: Complex conversational attack strategies (Crescendo, RedTeaming)\n",
    "- **Strategy composition**: Combine multiple converters together\n",
    "- **Difficulty levels**: Organized into EASY, MODERATE, and DIFFICULT categories\n",
    "\n",
    "Note that this is not the easiest way to run the Foundry scenario (or any scenario). This is meant to show how you can configure all the components.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we'll initialize PyRIT and configure the target we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.scenario import ScenarioCompositeStrategy\n",
    "from pyrit.scenario.foundry import Foundry, FoundryStrategy\n",
    "from pyrit.scenario.printer.console_printer import ConsoleScenarioResultPrinter\n",
    "from pyrit.setup import IN_MEMORY, initialize_pyrit_async\n",
    "\n",
    "await initialize_pyrit_async(memory_db_type=IN_MEMORY, initializers=[])  # type: ignore\n",
    "\n",
    "objective_target = OpenAIChatTarget()\n",
    "printer = ConsoleScenarioResultPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from pyrit.executor.attack.printer import ConsoleAttackResultPrinter\n",
    "\n",
    "# E2E Testing for Scam Scenario, remove when validation is done\n",
    "from pyrit.scenario import ScenarioResult\n",
    "from pyrit.setup.initializers.scenarios.load_default_datasets import LoadDefaultDatasets\n",
    "\n",
    "await LoadDefaultDatasets().initialize_async() # type: ignore\n",
    "\n",
    "scam_scenario = ScamScenario(include_baseline=False)\n",
    "await scam_scenario.initialize_async(  # type: ignore\n",
    "    objective_target=objective_target,\n",
    ")\n",
    "\n",
    "print(f\"Created scenario: {scam_scenario.name}\")\n",
    "print(f\"Number of atomic attacks: {scam_scenario.atomic_attack_count}\")\n",
    "\n",
    "scam_results:  ScenarioResult = await scam_scenario.run_async() # type: ignore\n",
    "\n",
    "# Flatten and print all attack results\n",
    "attack_results = list(itertools.chain.from_iterable(scam_results.attack_results.values()))\n",
    "\n",
    "for result in attack_results:\n",
    "    await ConsoleAttackResultPrinter().print_result_async(result=result)  # type: ignore\n",
    "\n",
    "await printer.print_summary_async(scam_results)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define Seed Groups\n",
    "\n",
    "By default, `Foundry` selects four random objectives from HarmBench. Here we'll retrieve only two for demonstration. If you didn't pass any `seed_groups`, the default would be almost the same except with `max_dataset_size=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.datasets import SeedDatasetProvider\n",
    "from pyrit.models import SeedGroup\n",
    "from pyrit.scenario import DatasetConfiguration\n",
    "\n",
    "datasets = await SeedDatasetProvider.fetch_datasets_async(dataset_names=[\"harmbench\"])  # type: ignore\n",
    "seed_groups: list[SeedGroup] = datasets[0].seed_groups  # type: ignore\n",
    "dataset_config = DatasetConfiguration(seed_groups=seed_groups, max_dataset_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Select Attack Strategies\n",
    "\n",
    "You can specify individual strategies or compose multiple converters together.\n",
    "The scenario supports three types of strategy specifications:\n",
    "\n",
    "1. **Simple strategies**: Individual converter or attack strategies (e.g., `FoundryStrategy.Base64`)\n",
    "2. **Aggregate strategies**: Tag-based groups (e.g., `FoundryStrategy.EASY` expands to all easy strategies)\n",
    "3. **Composite strategies**: Multiple converters applied together (e.g., Caesar + CharSwap)\n",
    "\n",
    "If not selected, there are always defaults. In this case, the default is `FoundryStrategy.EASY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_strategies = [\n",
    "    FoundryStrategy.Base64,  # Simple strategy (auto-wrapped internally)\n",
    "    FoundryStrategy.Binary,  # Simple strategy (auto-wrapped internally)\n",
    "    ScenarioCompositeStrategy(strategies=[FoundryStrategy.Caesar, FoundryStrategy.CharSwap]),  # Composed strategy\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Create and Initialize the Scenario\n",
    "\n",
    "The scenario needs to be initialized before execution. This builds the atomic attacks based on the selected strategies. Most of these have defaults, but the one thing that needs to be supplied is an `objective_target` so the scenario knows what we're attacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundry_scenario = Foundry()\n",
    "await foundry_scenario.initialize_async(  # type: ignore\n",
    "    objective_target=objective_target,\n",
    "    scenario_strategies=scenario_strategies,\n",
    "    max_concurrency=10,\n",
    "    dataset_config=dataset_config,\n",
    ")\n",
    "\n",
    "print(f\"Created scenario: {foundry_scenario.name}\")\n",
    "print(f\"Number of atomic attacks: {foundry_scenario.atomic_attack_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Execute the Scenario\n",
    "\n",
    "Now we'll run the scenario and print the results. The scenario will:\n",
    "1. Execute each atomic attack sequentially\n",
    "2. Apply the attack strategy to all objectives\n",
    "3. Score the results using the configured scorer\n",
    "4. Aggregate all results into a `ScenarioResult`\n",
    "\n",
    "The below example actually executes the scenario, and stores the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_result = await foundry_scenario.run_async()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Printing Scenarios\n",
    "\n",
    "The `ScenarioResult` object aggregates all results from a scenario execution. It contains a `scenario_identifier` (with name, description, and version), an `objective_target_identifier` describing the target tested, and an `attack_results` dictionary that maps each atomic attack strategy name to a list of `AttackResult` objects. Key properties include `scenario_run_state` (which can be \"CREATED\", \"IN_PROGRESS\", \"COMPLETED\", or \"FAILED\"), `labels` for metadata tagging, and `completion_time`. The class provides helper methods like `get_strategies_used()` to list all attack strategies, `get_objectives()` to retrieve unique objectives tested, and `objective_achieved_rate()` to calculate the success rate as a percentage. You can filter these methods by a specific `atomic_attack_name` or aggregate across all attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "await printer.print_summary_async(scenario_result)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Then to look at all the individual results, you can look at the `attack_results` property of the `ScenarioResult`. Additionally, although, the scenario result is returned from `run_async`, you can also retrieve it from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.executor.attack import ConsoleAttackResultPrinter\n",
    "from pyrit.memory.central_memory import CentralMemory\n",
    "\n",
    "memory = CentralMemory.get_memory_instance()\n",
    "scenario_result_from_memory = memory.get_scenario_results(scenario_name=\"Foundry\")[0]\n",
    "\n",
    "# Flatten all attack results from all strategies\n",
    "all_results = [result for results in scenario_result_from_memory.attack_results.values() for result in results]\n",
    "\n",
    "successful_attacks = [r for r in all_results if r.outcome == \"success\"]\n",
    "non_successful_attacks = [r for r in all_results if r.outcome != \"success\"]\n",
    "\n",
    "if len(successful_attacks) > 0:\n",
    "    print(\"\\nSuccessful Attacks:\")\n",
    "    for result in successful_attacks:\n",
    "        await ConsoleAttackResultPrinter().print_result_async(result=result)  # type: ignore\n",
    "else:\n",
    "    print(\"\\nNo successful attacks. Here is the first non success...\\n\")\n",
    "    await ConsoleAttackResultPrinter().print_result_async(result=non_successful_attacks[0])  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Alternative: Using Difficulty Levels\n",
    "\n",
    "Instead of specifying individual strategies, you can use aggregate tags like `EASY`, `MODERATE`, or `DIFFICULT` to test multiple strategies at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test all EASY strategies\n",
    "# easy_scenario = Foundry(\n",
    "#     objective_target=objective_target,\n",
    "#     scenario_strategies=[FoundryStrategy.EASY],  # Expands to all easy strategies\n",
    "#     objectives=objectives,\n",
    "# )\n",
    "# await easy_scenario.initialize_async()\n",
    "# easy_results = await easy_scenario.run_async()\n",
    "# await printer.print_summary_async(easy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Scenario Resiliency\n",
    "\n",
    "The `Foundry` scenario supports automatic resume and retry mechanisms:\n",
    "\n",
    "- **Automatic Resume**: If execution is interrupted, re-running the scenario will continue from where it left off\n",
    "- **Retry Mechanism**: Set `max_retries` to automatically retry on transient failures\n",
    "- **Progress Tracking**: The scenario tracks completed objectives in memory\n",
    "\n",
    "For more details on resiliency features, see the [resiliency documentation](../setup/2_resiliency.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
