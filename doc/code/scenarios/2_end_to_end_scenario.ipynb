{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Content Harm Testing\n",
    "\n",
    "This notebook demonstrates the usage of the ContentHarmScenario class to test model behavior with respect to various harm categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Initialize PyRIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pyrit.memory import CentralMemory\n",
    "from pyrit.setup.initialization import IN_MEMORY, initialize_pyrit\n",
    "\n",
    "# Initialize PyRIT with IN_MEMORY storage\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
    "memory = CentralMemory.get_memory_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Loading the data into memory\n",
    "\n",
    "Before running the scenario, we need to ensure that the relevant datasets are loaded into memory. We have provided a sample set of harm-related seed prompts and are loading them into memory in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyrit.common.path import DATASETS_PATH\n",
    "from pyrit.models import SeedDataset\n",
    "\n",
    "# Import seed prompts\n",
    "for harm in [\"hate\", \"violence\", \"harassment\", \"leakage\", \"sexual\", \"fairness\", \"misinformation\"]:\n",
    "    seed_prompts = SeedDataset.from_yaml_file(Path(DATASETS_PATH) / \"seed_prompts\" / \"harms\" / f\"{harm}.prompt\")\n",
    "    await memory.add_seeds_to_memory_async(prompts=[*seed_prompts.prompts, *seed_prompts.objectives], added_by=\"test\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Running Multiple Harm Strategies\n",
    "\n",
    "Now we can run the strategies using the datasets we defined above! We can selectively choose which strategies to run. In this example, we'll only run the Hate, Violence, and Harassment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.scenarios.printer.console_printer import ConsoleScenarioResultPrinter\n",
    "from pyrit.scenarios.scenarios.harms import (\n",
    "    ContentHarmScenario,\n",
    "    ContentHarmStrategy,\n",
    ")\n",
    "from pyrit.score.true_false.self_ask_refusal_scorer import SelfAskRefusalScorer\n",
    "from pyrit.score.true_false.true_false_inverter_scorer import TrueFalseInverterScorer\n",
    "\n",
    "printer = ConsoleScenarioResultPrinter()\n",
    "objective_target = OpenAIChatTarget()\n",
    "\n",
    "# Create ContentHarmScenario instance for hate, violence, and harassment testing\n",
    "content_harm_scenario = ContentHarmScenario(\n",
    "    objective_scorer=TrueFalseInverterScorer(scorer=SelfAskRefusalScorer(chat_target=OpenAIChatTarget())),\n",
    "    # Uncomment the following line to use a custom dataset prefix, we're using the default here\n",
    "    # seed_dataset_prefix==\"custom_prefix\",\n",
    ")\n",
    "await content_harm_scenario.initialize_async( # type: ignore\n",
    "    scenario_strategies=[\n",
    "        ContentHarmStrategy.Hate,\n",
    "        ContentHarmStrategy.Violence,\n",
    "        ContentHarmStrategy.Harassment,\n",
    "    ],\n",
    "    objective_target=objective_target,\n",
    ")\n",
    "print(f\"Created scenario: {content_harm_scenario.name}\")\n",
    "\n",
    "# Execute the entire scenario\n",
    "content_harm_results = await content_harm_scenario.run_async()  # type: ignore\n",
    "await printer.print_summary_async(content_harm_results)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can then inspect individual results from the different harm categories. Below is an example of printing out the results from the Violence strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.executor.attack.printer.console_printer import ConsoleAttackResultPrinter\n",
    "\n",
    "for result in content_harm_results.attack_results[\"violence\"]:\n",
    "    await ConsoleAttackResultPrinter().print_summary_async(result=result)  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
