{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ee13da",
   "metadata": {},
   "source": [
    "## Azure OpenAI GPT4-o Target Demo\n",
    "This notebook demonstrates how to use the Azure OpenAI GPT4-o target to accept multimodal input (text+image) and generate text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a4b9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:47:40.839505Z",
     "iopub.status.busy": "2024-07-26T03:47:40.839505Z",
     "iopub.status.idle": "2024-07-26T03:47:44.098303Z",
     "shell.execute_reply": "2024-07-26T03:47:44.098303Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "\n",
    "from pyrit.models import PromptRequestPiece, PromptRequestResponse\n",
    "from pyrit.prompt_target import AzureOpenAIGPT4OChatTarget\n",
    "from pyrit.common import default_values\n",
    "import pathlib\n",
    "from pyrit.common.path import HOME_PATH\n",
    "import uuid\n",
    "\n",
    "default_values.load_default_env()\n",
    "test_conversation_id = str(uuid.uuid4())\n",
    "\n",
    "# use the image from our docs\n",
    "image_path = pathlib.Path(HOME_PATH) / \"assets\" / \"pyrit_architecture.png\"\n",
    "\n",
    "request_pieces = [\n",
    "    PromptRequestPiece(\n",
    "        role=\"user\",\n",
    "        conversation_id=test_conversation_id,\n",
    "        original_value=\"Describe this picture:\",\n",
    "        original_value_data_type=\"text\",\n",
    "        converted_value_data_type=\"text\",\n",
    "    ),\n",
    "    PromptRequestPiece(\n",
    "        role=\"user\",\n",
    "        conversation_id=test_conversation_id,\n",
    "        original_value=str(image_path),\n",
    "        original_value_data_type=\"image_path\",\n",
    "        converted_value_data_type=\"image_path\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63359a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:47:44.098303Z",
     "iopub.status.busy": "2024-07-26T03:47:44.098303Z",
     "iopub.status.idle": "2024-07-26T03:47:44.106634Z",
     "shell.execute_reply": "2024-07-26T03:47:44.106634Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_request_response = PromptRequestResponse(request_pieces=request_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad937ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:47:44.111638Z",
     "iopub.status.busy": "2024-07-26T03:47:44.106634Z",
     "iopub.status.idle": "2024-07-26T03:47:48.006250Z",
     "shell.execute_reply": "2024-07-26T03:47:48.005100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None: assistant: The image is a structured chart detailing the components of PyRIT with two primary columns: \"Interface\" and \"Implementation.\"\n",
      "\n",
      "- **Target**:\n",
      "  - Local: local model (e.g., ONNX)\n",
      "  - Remote: API or web app\n",
      "\n",
      "- **Datasets**:\n",
      "  - Static: Prompts\n",
      "  - Dynamic: Prompt templates\n",
      "\n",
      "- **Scoring Engine**:\n",
      "  - PyRIT Itself: Self Evaluation\n",
      "  - API: Existing content classifiers\n",
      "\n",
      "- **Attack Strategy**:\n",
      "  - Single Turn: Using static prompts\n",
      "  - Multi Turn: Multiple conversations using prompt templates\n",
      "\n",
      "- **Memory**:\n",
      "  - Storage: JSON, Database\n",
      "  - Utils: Conversation, retrieval and storage, memory sharing, data analysis\n"
     ]
    }
   ],
   "source": [
    "with AzureOpenAIGPT4OChatTarget() as azure_openai_chat_target:\n",
    "    resp = await azure_openai_chat_target.send_prompt_async(prompt_request=prompt_request_response)  # type: ignore\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a799e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0901b69c",
   "metadata": {},
   "source": [
    "## Multimodal Demo using AzureOpenAIGPT4OChatTarget and PromptSendingOrchestrator\n",
    "This demo showcases the capabilities of AzureOpenAIGPT4OChatTarget for generating text based on multimodal inputs, including both text and images using PromptSendingOrchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91518dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:47:48.006704Z",
     "iopub.status.busy": "2024-07-26T03:47:48.006704Z",
     "iopub.status.idle": "2024-07-26T03:47:48.552887Z",
     "shell.execute_reply": "2024-07-26T03:47:48.552887Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyrit.common import default_values\n",
    "import pathlib\n",
    "from pyrit.common.path import HOME_PATH\n",
    "\n",
    "from pyrit.prompt_target import AzureOpenAIGPT4OChatTarget\n",
    "from pyrit.prompt_normalizer.normalizer_request import NormalizerRequestPiece\n",
    "from pyrit.prompt_normalizer.normalizer_request import NormalizerRequest\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "\n",
    "default_values.load_default_env()\n",
    "\n",
    "azure_openai_gpt4o_chat_target = AzureOpenAIGPT4OChatTarget()\n",
    "\n",
    "image_path = pathlib.Path(HOME_PATH) / \"assets\" / \"pyrit_architecture.png\"\n",
    "data = [\n",
    "    [\n",
    "        {\"prompt_text\": \"Describe this picture:\", \"prompt_data_type\": \"text\"},\n",
    "        {\"prompt_text\": str(image_path), \"prompt_data_type\": \"image_path\"},\n",
    "    ],\n",
    "    [{\"prompt_text\": \"Tell me about something?\", \"prompt_data_type\": \"text\"}],\n",
    "    [{\"prompt_text\": str(image_path), \"prompt_data_type\": \"image_path\"}],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35816c",
   "metadata": {},
   "source": [
    "Construct list of NormalizerRequest objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab70dc73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:47:48.552887Z",
     "iopub.status.busy": "2024-07-26T03:47:48.552887Z",
     "iopub.status.idle": "2024-07-26T03:47:48.561971Z",
     "shell.execute_reply": "2024-07-26T03:47:48.561971Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "normalizer_requests = []\n",
    "\n",
    "for piece_data in data:\n",
    "    request_pieces = []\n",
    "\n",
    "    for item in piece_data:\n",
    "        prompt_text = item.get(\"prompt_text\", \"\")  # type: ignore\n",
    "        prompt_data_type = item.get(\"prompt_data_type\", \"\")\n",
    "        converters = []  # type: ignore\n",
    "        request_piece = NormalizerRequestPiece(\n",
    "            prompt_value=prompt_text, prompt_data_type=prompt_data_type, request_converters=converters  # type: ignore\n",
    "        )\n",
    "        request_pieces.append(request_piece)\n",
    "\n",
    "    normalizer_request = NormalizerRequest(request_pieces)\n",
    "    normalizer_requests.append(normalizer_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6522f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:47:48.561971Z",
     "iopub.status.busy": "2024-07-26T03:47:48.561971Z",
     "iopub.status.idle": "2024-07-26T03:47:48.573397Z",
     "shell.execute_reply": "2024-07-26T03:47:48.573397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalizer_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14579342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:47:48.573397Z",
     "iopub.status.busy": "2024-07-26T03:47:48.573397Z",
     "iopub.status.idle": "2024-07-26T03:47:53.969405Z",
     "shell.execute_reply": "2024-07-26T03:47:53.969405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureOpenAIGPT4OChatTarget: user: E:\\OSS_Tools\\PyRIT-internal\\PyRIT\\assets\\pyrit_architecture.png\n",
      "AzureOpenAIGPT4OChatTarget: assistant: The image provides a detailed breakdown of the components of PyRIT, an AI-based system. Here is a summary of each component and its implementation:\n",
      "\n",
      "### Interface: \n",
      "#### Target\n",
      "- **Local:** local model (e.g., ONNX)\n",
      "- **Remote:** API or web app\n",
      "\n",
      "### Datasets\n",
      "- **Static:** prompts\n",
      "- **Dynamic:** Prompt templates\n",
      "\n",
      "### Scoring Engine\n",
      "- **PyRIT Itself:** Self Evaluation\n",
      "- **API:** Existing content classifiers\n",
      "\n",
      "### Attack Strategy\n",
      "- **Single Turn:** Using static prompts\n",
      "- **Multi Turn:** Multiple conversations using prompt templates\n",
      "\n",
      "### Memory\n",
      "- **Storage:** JSON, Database\n",
      "- **Utils:** Conversation, retrieval and storage, memory sharing, data analysis\n",
      "\n",
      "Each section defines key functionalities and methods that PyRIT leverages for processing and interaction within its framework.\n",
      "AzureOpenAIGPT4OChatTarget: user: Tell me about something?\n",
      "AzureOpenAIGPT4OChatTarget: assistant: Sure, I'd be happy to share information on a topic of interest to you! Is there something specific you're curious about? It could range from science and technology, culture, history, current events, hobbies, or anything else. Just let me know what you're interested in!\n",
      "AzureOpenAIGPT4OChatTarget: user: Describe this picture:\n",
      "AzureOpenAIGPT4OChatTarget: user: E:\\OSS_Tools\\PyRIT-internal\\PyRIT\\assets\\pyrit_architecture.png\n",
      "AzureOpenAIGPT4OChatTarget: assistant: This image is a diagram titled \"PyRIT Components\" and depicts various elements of the PyRIT system organized into two columns under the headings \"Interface\" and \"Implementation.\"\n",
      "\n",
      "### Components:\n",
      "1. **Target**\n",
      "   - **Local:** Local model (e.g., ONNX)\n",
      "   - **Remote:** API or web app\n",
      "2. **Datasets**\n",
      "   - **Static:** Prompts\n",
      "   - **Dynamic:** Prompt templates\n",
      "3. **Scoring Engine**\n",
      "   - **PyRIT Itself:** Self Evaluation\n",
      "   - **API:** Existing content classifiers\n",
      "4. **Attack Strategy**\n",
      "   - **Single Turn:** Using static prompts\n",
      "   - **Multi Turn:** Multiple conversations using prompt templates\n",
      "5. **Memory**\n",
      "   - **Storage:** JSON, Database\n",
      "   - **Utils:** Conversation, retrieval and storage, memory sharing, data analysis\n",
      "\n",
      "The components are identified in the left column under \"Interface,\" and their corresponding implementations are described on the right side under \"Implementation.\" The sections use different background colors to visually differentiate between them.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with PromptSendingOrchestrator(prompt_target=azure_openai_gpt4o_chat_target) as orchestrator:\n",
    "\n",
    "    await orchestrator.send_normalizer_requests_async(prompt_request_list=normalizer_requests)  # type: ignore\n",
    "\n",
    "    memory = orchestrator.get_memory()\n",
    "\n",
    "    for entry in memory:\n",
    "        print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d43c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pyrit-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
