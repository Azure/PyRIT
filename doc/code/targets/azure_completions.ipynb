{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4725cd0b",
   "metadata": {},
   "source": [
    "## Azure OpenAI Completions\n",
    "\n",
    "Before you begin, ensure you are setup with the correct version of PyRIT and have the applicable secrets configured as described [here](../../setup/).\n",
    "\n",
    "Once you are configured, then you will be able to get completions for your text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd1e909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T22:38:27.789323Z",
     "iopub.status.busy": "2024-04-28T22:38:27.789323Z",
     "iopub.status.idle": "2024-04-28T22:38:34.005106Z",
     "shell.execute_reply": "2024-04-28T22:38:34.005106Z"
    }
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Unsupported data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m target \u001b[38;5;241m=\u001b[39m AzureOpenAICompletionTarget(api_key\u001b[38;5;241m=\u001b[39mapi_key, endpoint\u001b[38;5;241m=\u001b[39mapi_base, deployment_name\u001b[38;5;241m=\u001b[39mdeployment_name)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PromptSendingOrchestrator(prompt_target\u001b[38;5;241m=\u001b[39mtarget) \u001b[38;5;28;01mas\u001b[39;00m orchestrator:\n\u001b[0;32m---> 17\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m orchestrator\u001b[38;5;241m.\u001b[39msend_prompts_async(prompt_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello! Who are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyrit/orchestrator/prompt_sending_orchestrator.py:105\u001b[0m, in \u001b[0;36mPromptSendingOrchestrator.send_prompts_async\u001b[0;34m(self, prompt_list, prompt_type, memory_labels, metadata)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompt_list:\n\u001b[1;32m     96\u001b[0m     requests\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_normalizer_request(\n\u001b[1;32m     98\u001b[0m             prompt_text\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m     )\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_normalizer_requests_async(\n\u001b[1;32m    106\u001b[0m     prompt_request_list\u001b[38;5;241m=\u001b[39mrequests,\n\u001b[1;32m    107\u001b[0m     memory_labels\u001b[38;5;241m=\u001b[39mmemory_labels,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyrit/orchestrator/prompt_sending_orchestrator.py:160\u001b[0m, in \u001b[0;36mPromptSendingOrchestrator.send_normalizer_requests_async\u001b[0;34m(self, prompt_request_list, memory_labels)\u001b[0m\n\u001b[1;32m    156\u001b[0m     request\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Normalizer is responsible for storing the requests in memory\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# The labels parameter may allow me to stash class information for each kind of prompt.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m responses: \u001b[38;5;28mlist\u001b[39m[PromptRequestResponse] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_normalizer\u001b[38;5;241m.\u001b[39msend_prompt_batch_to_target_async(\n\u001b[1;32m    161\u001b[0m     requests\u001b[38;5;241m=\u001b[39mprompt_request_list,\n\u001b[1;32m    162\u001b[0m     target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_target,\n\u001b[1;32m    163\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_with_global_memory_labels(memory_labels),\n\u001b[1;32m    164\u001b[0m     orchestrator_identifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_identifier(),\n\u001b[1;32m    165\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# These are the responses from the target\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# print(responses[0].request_pieces)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scorers:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyrit/prompt_normalizer/prompt_normalizer.py:130\u001b[0m, in \u001b[0;36mPromptNormalizer.send_prompt_batch_to_target_async\u001b[0;34m(self, requests, target, labels, orchestrator_identifier, batch_size)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts_batch:\n\u001b[1;32m    121\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_prompt_async(\n\u001b[1;32m    123\u001b[0m                 normalizer_request\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m             )\n\u001b[1;32m    128\u001b[0m         )\n\u001b[0;32m--> 130\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    131\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(batch_results)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyrit/prompt_normalizer/prompt_normalizer.py:63\u001b[0m, in \u001b[0;36mPromptNormalizer.send_prompt_async\u001b[0;34m(self, normalizer_request, target, conversation_id, sequence, labels, orchestrator_identifier)\u001b[0m\n\u001b[1;32m     60\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m target\u001b[38;5;241m.\u001b[39msend_prompt_async(prompt_request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory\u001b[38;5;241m.\u001b[39madd_request_response_to_memory(request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Ensure request to memory before processing exception\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyrit/prompt_target/azure_openai_completion_target.py:114\u001b[0m, in \u001b[0;36mAzureOpenAICompletionTarget.send_prompt_async\u001b[0;34m(self, prompt_request)\u001b[0m\n\u001b[1;32m    110\u001b[0m request \u001b[38;5;241m=\u001b[39m prompt_request\u001b[38;5;241m.\u001b[39mrequest_pieces[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    112\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending the following prompt to the prompt target: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m text_response: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_client\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    115\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    116\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mconverted_value,\n\u001b[1;32m    117\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top_p,\n\u001b[1;32m    118\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temperature,\n\u001b[1;32m    119\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frequency_penalty,\n\u001b[1;32m    120\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_presence_penalty,\n\u001b[1;32m    121\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_tokens,\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    123\u001b[0m prompt_response \u001b[38;5;241m=\u001b[39m PromptResponse(\n\u001b[1;32m    124\u001b[0m     completion\u001b[38;5;241m=\u001b[39mtext_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    125\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mconverted_value,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39mtext_response\u001b[38;5;241m.\u001b[39mobject,\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    133\u001b[0m response_entry \u001b[38;5;241m=\u001b[39m construct_response_from_request(\n\u001b[1;32m    134\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[1;32m    135\u001b[0m     response_text_pieces\u001b[38;5;241m=\u001b[39m[prompt_response\u001b[38;5;241m.\u001b[39mcompletion],\n\u001b[1;32m    136\u001b[0m     prompt_metadata\u001b[38;5;241m=\u001b[39mprompt_response\u001b[38;5;241m.\u001b[39mto_json(),\n\u001b[1;32m    137\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/completions.py:1081\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1080\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m AsyncStream[Completion]:\n\u001b[0;32m-> 1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1083\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1084\u001b[0m             {\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1086\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m   1087\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_of,\n\u001b[1;32m   1088\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecho\u001b[39m\u001b[38;5;124m\"\u001b[39m: echo,\n\u001b[1;32m   1089\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1090\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1091\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1092\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1093\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1094\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1095\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1097\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1098\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1099\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: suffix,\n\u001b[1;32m   1100\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1101\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1102\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1103\u001b[0m             },\n\u001b[1;32m   1104\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1105\u001b[0m         ),\n\u001b[1;32m   1106\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1107\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1108\u001b[0m         ),\n\u001b[1;32m   1109\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mCompletion,\n\u001b[1;32m   1110\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1111\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[Completion],\n\u001b[1;32m   1112\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1816\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1804\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1812\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1813\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1814\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1815\u001b[0m     )\n\u001b[0;32m-> 1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1510\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1503\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1511\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1512\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1515\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1516\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1611\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1610\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1614\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1615\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1620\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Unsupported data type"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyrit.common import default_values\n",
    "\n",
    "from pyrit.orchestrator.prompt_sending_orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.prompt_target import AzureOpenAICompletionTarget\n",
    "\n",
    "default_values.load_default_env()\n",
    "\n",
    "api_base = os.environ.get(\"AZURE_OPENAI_COMPLETION_ENDPOINT\")\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_COMPLETION_KEY\")\n",
    "deployment_name = os.environ.get(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT\")\n",
    "\n",
    "\n",
    "target = AzureOpenAICompletionTarget(api_key=api_key, endpoint=api_base, deployment_name=deployment_name)\n",
    "\n",
    "with PromptSendingOrchestrator(prompt_target=target) as orchestrator:\n",
    "    response = await orchestrator.send_prompts_async(prompt_list=[\"Hello! Who are you?\"]) # type: ignore\n",
    "    print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25b2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pyrit-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
