{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130a41a5",
   "metadata": {},
   "source": [
    "## Azure OpenAI GPT-V Target Demo\n",
    "This notebook demonstrates how to use the Azure OpenAI GPT-V target to accept multimodal input (text+image) and generate text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74f5930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T19:35:20.497173Z",
     "iopub.status.busy": "2024-05-16T19:35:20.496177Z",
     "iopub.status.idle": "2024-05-16T19:35:32.892964Z",
     "shell.execute_reply": "2024-05-16T19:35:32.891955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "\n",
    "from pyrit.models import PromptRequestPiece, PromptRequestResponse\n",
    "from pyrit.prompt_target import AzureOpenAIGPTVChatTarget\n",
    "from pyrit.common import default_values\n",
    "import pathlib\n",
    "from pyrit.common.path import HOME_PATH\n",
    "import uuid\n",
    "\n",
    "default_values.load_default_env()\n",
    "test_conversation_id = str(uuid.uuid4())\n",
    "\n",
    "# use the image from our docs\n",
    "image_path = pathlib.Path(HOME_PATH) / \"assets\" / \"pyrit_architecture.png\"\n",
    "\n",
    "request_pieces = [\n",
    "    PromptRequestPiece(\n",
    "        role=\"user\",\n",
    "        conversation_id=test_conversation_id,\n",
    "        original_value=\"Describe this picture:\",\n",
    "        original_value_data_type=\"text\",\n",
    "        converted_value_data_type=\"text\",\n",
    "    ),\n",
    "    PromptRequestPiece(\n",
    "        role=\"user\",\n",
    "        conversation_id=test_conversation_id,\n",
    "        original_value=str(image_path),\n",
    "        original_value_data_type=\"image_path\",\n",
    "        converted_value_data_type=\"image_path\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b4c596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T19:35:32.898501Z",
     "iopub.status.busy": "2024-05-16T19:35:32.897497Z",
     "iopub.status.idle": "2024-05-16T19:35:32.905229Z",
     "shell.execute_reply": "2024-05-16T19:35:32.904284Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_request_response = PromptRequestResponse(request_pieces=request_pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581d394",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2c237c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T19:35:32.912243Z",
     "iopub.status.busy": "2024-05-16T19:35:32.911242Z",
     "iopub.status.idle": "2024-05-16T19:35:49.616018Z",
     "shell.execute_reply": "2024-05-16T19:35:49.616018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None: assistant: The image displays a chart titled \"PyRIT Components,\" which appears to outline the structure of a system or framework named PyRIT. The chart is divided into two columns: Interface and Implementation.\n",
      "\n",
      "Under the Interface column, there are five categories:\n",
      "1. Target\n",
      "2. Datasets\n",
      "3. Scoring Engine\n",
      "4. Attack Strategy\n",
      "5. Memory\n",
      "\n",
      "Adjacent to each Interface category, there's an Implementation description:\n",
      "- For Target, there are two types: Local (local model e.g., ONNX) and Remote (API or web app).\n",
      "- Datasets can be either Static (prompts) or Dynamic (Prompt templates).\n",
      "- The Scoring Engine's implementation is described as PyRIT Itself: Self Evaluation and API: Existing content classifiers.\n",
      "- Attack Strategy can be Single Turn (Using static prompts) or Multi Turn (Multiple conversations using prompt templates).\n",
      "- Memory is detailed with Storage (JSON, Database) and Utils (Conversation, retrieval and storage, memory sharing, data analysis).\n",
      "\n",
      "Overall, the image seems to summarize the components of PyRIT in terms of how it interfaces with users or other systems alongside specific implementation methods for its functionalities.\n"
     ]
    }
   ],
   "source": [
    "with AzureOpenAIGPTVChatTarget() as azure_openai_chat_target:\n",
    "    resp = await azure_openai_chat_target.send_prompt_async(prompt_request=prompt_request_response)  # type: ignore\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba99d61",
   "metadata": {},
   "source": [
    "## Multimodal Demo using AzureOpenAIGPTVChatTarget and PromptSendingOrchestrator\n",
    "This demo showcases the capabilities of AzureOpenAIGPTVChatTarget for generating text based on multimodal inputs, including both text and images using PromptSendingOrchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.common import default_values\n",
    "import pathlib\n",
    "from pyrit.common.path import HOME_PATH\n",
    "\n",
    "from pyrit.prompt_target import AzureOpenAIGPTVChatTarget\n",
    "from pyrit.prompt_normalizer.normalizer_request import NormalizerRequestPiece\n",
    "from pyrit.prompt_normalizer.normalizer_request import NormalizerRequest\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "\n",
    "default_values.load_default_env()\n",
    "\n",
    "azure_openai_gptv_chat_target = AzureOpenAIGPTVChatTarget()\n",
    "\n",
    "image_path = pathlib.Path(HOME_PATH) / \"assets\" / \"pyrit_architecture.png\"\n",
    "data = [\n",
    "    [\n",
    "        {\"prompt_text\": \"Describe this picture:\", \"prompt_data_type\": \"text\"},\n",
    "        {\"prompt_text\": str(image_path), \"prompt_data_type\": \"image_path\"},\n",
    "    ],\n",
    "    [{\"prompt_text\": \"Tell me about something?\", \"prompt_data_type\": \"text\"}],\n",
    "    [{\"prompt_text\": str(image_path), \"prompt_data_type\": \"image_path\"}],\n",
    "]\n",
    "\n",
    "\n",
    "normalizer_requests = []\n",
    "\n",
    "for piece_data in data:\n",
    "    request_pieces = []\n",
    "\n",
    "    for item in piece_data:\n",
    "        prompt_text = item.get(\"prompt_text\", \"\")  # type: ignore\n",
    "        prompt_data_type = item.get(\"prompt_data_type\", \"\")\n",
    "        converters = []  # type: ignore\n",
    "        request_piece = NormalizerRequestPiece(\n",
    "            prompt_value=prompt_text, prompt_data_type=prompt_data_type, request_converters=converters  # type: ignore\n",
    "        )\n",
    "        request_pieces.append(request_piece)\n",
    "\n",
    "    normalizer_request = NormalizerRequest(request_pieces)\n",
    "    normalizer_requests.append(normalizer_request)\n",
    "\n",
    "\n",
    "    \n",
    "with PromptSendingOrchestrator(prompt_target=azure_openai_gptv_chat_target) as orchestrator:\n",
    "\n",
    "    await orchestrator.send_normalizer_requests_async(prompt_request_list=normalizer_requests)  # type: ignore\n",
    "\n",
    "    memory = orchestrator.get_memory()\n",
    "\n",
    "    for entry in memory:\n",
    "        print(entry)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pyrit-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
