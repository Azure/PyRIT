{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de35022",
   "metadata": {},
   "source": [
    "## Azure OpenAI GPT-V Target Demo\n",
    "This notebook demonstrates how to use the Azure OpenAI GPT-V target to accept multimodal input (text+image) and generate text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc94a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T02:42:51.647147Z",
     "iopub.status.busy": "2024-04-29T02:42:51.647147Z",
     "iopub.status.idle": "2024-04-29T02:42:56.808308Z",
     "shell.execute_reply": "2024-04-29T02:42:56.808308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "\n",
    "from pyrit.models import PromptRequestPiece, PromptRequestResponse\n",
    "from pyrit.prompt_target import AzureOpenAIGPTVChatTarget\n",
    "from pyrit.common import default_values\n",
    "import pathlib\n",
    "from pyrit.common.path import HOME_PATH\n",
    "import uuid\n",
    "\n",
    "default_values.load_default_env()\n",
    "test_conversation_id = str(uuid.uuid4())\n",
    "\n",
    "# use the image from our docs\n",
    "image_path = pathlib.Path(HOME_PATH) / \"assets\" / \"pyrit_architecture.png\"\n",
    "\n",
    "request_pieces = [\n",
    "    PromptRequestPiece(\n",
    "        role=\"user\",\n",
    "        conversation_id=test_conversation_id,\n",
    "        original_value=\"Describe this picture:\",\n",
    "        original_value_data_type=\"text\",\n",
    "        converted_value_data_type=\"text\",\n",
    "    ),\n",
    "    PromptRequestPiece(\n",
    "        role=\"user\",\n",
    "        conversation_id=test_conversation_id,\n",
    "        original_value=str(image_path),\n",
    "        original_value_data_type=\"image_path\",\n",
    "        converted_value_data_type=\"image_path\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30eaddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T02:42:56.812314Z",
     "iopub.status.busy": "2024-04-29T02:42:56.811312Z",
     "iopub.status.idle": "2024-04-29T02:42:56.814589Z",
     "shell.execute_reply": "2024-04-29T02:42:56.814589Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_request_response = PromptRequestResponse(request_pieces=request_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c8eabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T02:42:56.817594Z",
     "iopub.status.busy": "2024-04-29T02:42:56.817594Z",
     "iopub.status.idle": "2024-04-29T02:43:11.905794Z",
     "shell.execute_reply": "2024-04-29T02:43:11.905794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None: assistant: The picture shows a chart with the title \"PyRIT Components\" at the top. The chart is divided into two columns, with \"Interface\" on the left and \"Implementation\" on the right. Underneath the columns, there are five rows, each labeled with a different component of PyRIT. \n",
      "\n",
      "The first row is labeled \"Target\" and includes two types of implementation: \"Local,\" which specifies local model (e.g., ONNX), and \"Remote,\" which mentions API or web app.\n",
      "\n",
      "The second row is labeled \"Datasets\" and includes two types of implementation as well: \"Static,\" which specifies prompts, and \"Dynamic,\" which mentions prompt templates.\n",
      "\n",
      "The third row is labeled \"Scoring Engine\" and includes only one type of implementation: PyRIT itself for self-evaluation and API for existing content classifiers.\n",
      "\n",
      "The fourth row is labeled \"Attack Strategy\" and also has two implementations: \"Single Turn,\" which uses static prompts, and \"Multi-Turn,\" which involves multiple conversations using prompt templates.\n",
      "\n",
      "The last row is labeled “Memory”  with “Storage” specifying JSON and database.  There is also “Utils,” mentioning conversation, retrieval and storage, memory sharing, data analysis.\n"
     ]
    }
   ],
   "source": [
    "with AzureOpenAIGPTVChatTarget() as azure_openai_chat_target:\n",
    "    resp = await azure_openai_chat_target.send_prompt_async(prompt_request=prompt_request_response)  # type: ignore\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bcefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pyrit-311",
   "language": "python",
   "name": "pyrit-311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
