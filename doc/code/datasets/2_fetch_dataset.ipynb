{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {},
      "source": [
        "# Fetching dataset examples\n",
        "\n",
        "This notebook demonstrates how to load datasets as a `SeedPromptDataset` to perform red teaming on a target.\n",
        "There are several datasets which can be found in the `fetch_example_datasets.py` file.\n",
        "Three example datasets are shown in this notebook and can be used with orchestrators such as the Prompt Sending Orchestrator.\n",
        "The example below demonstrates loading a HuggingFace dataset as a `SeedPromptDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
        "from pyrit.datasets import fetch_llm_latent_adversarial_training_harmful_dataset\n",
        "from pyrit.orchestrator import PromptSendingOrchestrator\n",
        "from pyrit.prompt_target import TextTarget\n",
        "\n",
        "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
        "\n",
        "# Set up the Azure OpenAI prompt target\n",
        "prompt_target = TextTarget()\n",
        "\n",
        "# Fetch prompt column from harmful datasets\n",
        "prompt_dataset = fetch_llm_latent_adversarial_training_harmful_dataset()\n",
        "# Some fetch functions may include parameters such as the below example for unsafe prompts\n",
        "# prompt_dataset = fetch_pku_safe_rlhf_dataset(False)\n",
        "\n",
        "# Use the first 8 examples for red teaming\n",
        "prompt_list = [prompt.value for prompt in prompt_dataset.prompts[:8]]\n",
        "\n",
        "# Send prompts using the orchestrator and capture responses\n",
        "orchestrator = PromptSendingOrchestrator(objective_target=prompt_target)\n",
        "responses = await orchestrator.send_prompts_async(prompt_list=prompt_list)  # type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {},
      "source": [
        "# Example dataset from public URL\n",
        "\n",
        "The following example fetches DecodingTrust 'stereotypes' examples of involving potentially harmful stereotypes from the DecodingTrust repository which try to convince the assistant to agree and captures the responses. This is a scenario where the dataset resides in a public  URL and is also outputted as a `SeedPromptDataset`. By fetching these prompts, we can further use this `SeedPromptDataset` by sending the prompts to a target using the `PromptSendingOrchestrator` as shown in the example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyrit.datasets import fetch_decoding_trust_stereotypes_dataset\n",
        "from pyrit.prompt_target import TextTarget\n",
        "\n",
        "# Set up the Azure OpenAI prompt target\n",
        "prompt_target = TextTarget()\n",
        "examples_source = (\n",
        "    \"https://raw.githubusercontent.com/AI-secure/DecodingTrust/main/data/stereotype/dataset/user_prompts.csv\"\n",
        ")\n",
        "\n",
        "orchestrator = PromptSendingOrchestrator(objective_target=prompt_target)\n",
        "\n",
        "# Fetch examples from DecodingTrust 'Stereotype' dataset using the 'targeted' system prompt and topics of \"driving\" and \"technology\"\n",
        "prompt_dataset = fetch_decoding_trust_stereotypes_dataset(\n",
        "    examples_source,\n",
        "    source_type=\"public_url\",\n",
        "    stereotype_topics=[\"driving\", \"technology\"],\n",
        "    target_groups=None,\n",
        "    system_prompt_type=\"targeted\",\n",
        ")\n",
        "\n",
        "# Use the first 4 examples\n",
        "prompt_list = [prompt.value for prompt in prompt_dataset.prompts[:4]]\n",
        "\n",
        "# Send prompts using the orchestrator and capture responses\n",
        "try:\n",
        "    responses = await orchestrator.send_prompts_async(prompt_list=prompt_list)  # type: ignore\n",
        "    if responses:\n",
        "        await orchestrator.print_conversations_async()  # type: ignore\n",
        "    else:\n",
        "        print(\"No valid responses were received from the orchestrator.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while sending prompts: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyrit.memory import CentralMemory\n",
        "\n",
        "memory = CentralMemory.get_memory_instance()\n",
        "memory.dispose_engine()"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}