{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b27acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found default environment files: ['C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env', 'C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env.local']\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env.local\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import SelfAskRefusalScorer, SelfAskTrueFalseScorer\n",
    "from pyrit.setup import IN_MEMORY, initialize_pyrit_async\n",
    "\n",
    "await initialize_pyrit_async(memory_db_type=IN_MEMORY)  # type: ignore\n",
    "target = OpenAIChatTarget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4d79d",
   "metadata": {},
   "source": [
    "## Running a Scorer Evaluation\n",
    "\n",
    "The simplest way to evaluate a scorer is to call `evaluate_async()` directly on the scorer instance. This method:\n",
    "- Automatically detects the scorer type (objective vs harm)\n",
    "- Uses standard column names from the CSV\n",
    "- Returns metrics immediately without saving files\n",
    "- Optionally includes raw trial scores for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6db266b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Path not found: C:\\git\\PyRIT\\doc\\code\\scoring\\pyrit\\score\\scorer_evals\\true_false\\mini_refusal.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m refusal_scorer = SelfAskRefusalScorer(chat_target=target)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Run evaluation on mini dataset (10 examples)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m metrics = \u001b[38;5;28;01mawait\u001b[39;00m refusal_scorer.evaluate_async(\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyrit/score/scorer_evals/true_false/mini_refusal.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     num_scorer_trials=\u001b[32m1\u001b[39m,  \u001b[38;5;66;03m# Number of times to score each response\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\git\\PyRIT\\pyrit\\score\\scorer.py:305\u001b[39m, in \u001b[36mScorer.evaluate_async\u001b[39m\u001b[34m(self, csv_path, num_scorer_trials, add_to_registry)\u001b[39m\n\u001b[32m    302\u001b[39m scorer_evaluator = ScorerEvaluator.from_scorer(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    304\u001b[39m \u001b[38;5;66;03m# Use standard column names\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m metrics = \u001b[38;5;28;01mawait\u001b[39;00m scorer_evaluator.run_evaluation_from_csv_async(\n\u001b[32m    306\u001b[39m     csv_path=csv_path,\n\u001b[32m    307\u001b[39m     human_label_col_names=[\u001b[33m\"\u001b[39m\u001b[33mhuman_score\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    308\u001b[39m     objective_or_harm_col_name=\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    309\u001b[39m     assistant_response_col_name=\u001b[33m\"\u001b[39m\u001b[33massistant_response\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    310\u001b[39m     assistant_response_data_type_col_name=\u001b[33m\"\u001b[39m\u001b[33mdata_type\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    311\u001b[39m     num_scorer_trials=num_scorer_trials,\n\u001b[32m    312\u001b[39m     add_to_registry=add_to_registry,\n\u001b[32m    313\u001b[39m )\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\git\\PyRIT\\pyrit\\score\\scorer_evaluation\\scorer_evaluator.py:463\u001b[39m, in \u001b[36mObjectiveScorerEvaluator.run_evaluation_from_csv_async\u001b[39m\u001b[34m(self, csv_path, human_label_col_names, objective_or_harm_col_name, assistant_response_col_name, assistant_response_data_type_col_name, num_scorer_trials, dataset_name, version, add_to_registry)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_evaluation_from_csv_async\u001b[39m(\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    445\u001b[39m     csv_path: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    453\u001b[39m     add_to_registry: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    454\u001b[39m ) -> ObjectiveScorerMetrics:\n\u001b[32m    455\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[33;03m    Evaluate an objective scorer against a CSV dataset containing binary labels (0 or 1).\u001b[39;00m\n\u001b[32m    457\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    461\u001b[39m \u001b[33;03m        ObjectiveScorerMetrics: Metrics including accuracy, F1 score, precision, and recall.\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     labeled_dataset = \u001b[43mHumanLabeledDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetricsType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOBJECTIVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43massistant_response_col_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_response_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhuman_label_col_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhuman_label_col_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobjective_or_harm_col_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjective_or_harm_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43massistant_response_data_type_col_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_response_data_type_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     metrics = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_evaluation_async(\n\u001b[32m    474\u001b[39m         labeled_dataset=labeled_dataset,\n\u001b[32m    475\u001b[39m         num_scorer_trials=num_scorer_trials,\n\u001b[32m    476\u001b[39m         add_to_registry=add_to_registry,\n\u001b[32m    477\u001b[39m     )\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\git\\PyRIT\\pyrit\\score\\scorer_evaluation\\human_labeled_dataset.py:153\u001b[39m, in \u001b[36mHumanLabeledDataset.from_csv\u001b[39m\u001b[34m(cls, csv_path, metrics_type, human_label_col_names, objective_or_harm_col_name, assistant_response_col_name, assistant_response_data_type_col_name, dataset_name, version)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_csv\u001b[39m(\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m     version: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    122\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mHumanLabeledDataset\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    123\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m    Load a human-labeled dataset from a CSV file. This only allows for single turn scored text responses.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m    You can optionally include a # comment line at the top of the CSV file to specify the dataset version\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[33;03m        ValueError: If version is not provided and not found in the CSV file via comment line \"# version=\".\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     csv_path = \u001b[43mverify_and_resolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# Read the first line to check for version info\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\git\\PyRIT\\pyrit\\common\\utils.py:36\u001b[39m, in \u001b[36mverify_and_resolve_path\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     34\u001b[39m path_obj: Path = Path(path).resolve() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m path.resolve()\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path_obj.exists():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(path_obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path_obj\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Path not found: C:\\git\\PyRIT\\doc\\code\\scoring\\pyrit\\score\\scorer_evals\\true_false\\mini_refusal.csv"
     ]
    }
   ],
   "source": [
    "# Evaluate a refusal scorer\n",
    "refusal_scorer = SelfAskRefusalScorer(chat_target=target)\n",
    "\n",
    "# Run evaluation on mini dataset (10 examples)\n",
    "metrics = await refusal_scorer.evaluate_async(\n",
    "    \"pyrit/score/scorer_evals/true_false/mini_refusal.csv\",\n",
    "    num_scorer_trials=1,  # Number of times to score each response\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {metrics.accuracy:.2%}\")\n",
    "print(f\"Precision: {metrics.precision:.3f}\")\n",
    "print(f\"Recall: {metrics.recall:.3f}\")\n",
    "print(f\"F1 Score: {metrics.f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12102bb",
   "metadata": {},
   "source": [
    "## Accessing All Metrics\n",
    "\n",
    "The returned `ObjectiveScorerMetrics` object contains all evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb14166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "# View all metrics (excluding trial_scores for cleaner output)\n",
    "all_metrics = {k: v for k, v in asdict(metrics).items() if k != 'trial_scores'}\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf37efd",
   "metadata": {},
   "source": [
    "## Analyzing Trial Scores\n",
    "\n",
    "When `num_scorer_trials > 1`, you can analyze score variance across trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with multiple trials to measure scorer consistency\n",
    "metrics_multi = await refusal_scorer.evaluate_async(\n",
    "    \"pyrit/score/scorer_evals/true_false/mini_refusal.csv\",\n",
    "    num_scorer_trials=3,\n",
    ")\n",
    "\n",
    "# Access raw trial scores\n",
    "trial_scores = metrics_multi.trial_scores\n",
    "print(f\"Trial scores shape: {trial_scores.shape}\")  # (num_trials, num_responses)\n",
    "print(f\"First response scored across trials: {trial_scores[:, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f918ba4",
   "metadata": {},
   "source": [
    "## Evaluating Different Scorers\n",
    "\n",
    "You can evaluate any true/false scorer using the same API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyrit.common.path import DATASETS_PATH\n",
    "\n",
    "# Evaluate a custom true/false scorer\n",
    "custom_scorer = SelfAskTrueFalseScorer(\n",
    "    true_false_question_path=DATASETS_PATH / \"score\" / \"true_false_question\" / \"task_achieved.yaml\",\n",
    "    chat_target=target\n",
    ")\n",
    "\n",
    "# Use a different evaluation dataset\n",
    "metrics = await custom_scorer.evaluate_async(\n",
    "    \"pyrit/score/scorer_evals/true_false/privacy.csv\"\n",
    ")\n",
    "\n",
    "print(f\"Privacy Detection - Accuracy: {metrics.accuracy:.2%}, F1: {metrics.f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16fc2f",
   "metadata": {},
   "source": [
    "## Auto-Discovery of Objective Datasets\n",
    "\n",
    "By default, objective scorers automatically evaluate against all CSV files in their dataset directory and return a combined metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-discovery happens automatically - no need to manually list files\n",
    "results = await custom_scorer.evaluate_async()\n",
    "\n",
    "# Get combined metrics across all objective datasets\n",
    "combined = results[\"combined\"]\n",
    "print(f\"Combined metrics across all objective datasets:\")\n",
    "print(f\"  Accuracy: {combined.accuracy:.1%}\")\n",
    "print(f\"  Precision: {combined.precision:.3f}\")\n",
    "print(f\"  Recall: {combined.recall:.3f}\")\n",
    "print(f\"  F1 Score: {combined.f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6194b7f",
   "metadata": {},
   "source": [
    "## Browsing Official Scorer Performance\n",
    "\n",
    "The PyRIT team maintains a registry of scorer evaluation results on official consolidated datasets. You can browse these to compare scorer performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92436dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.score.scorer_evaluation.scorer_metrics_utility import load_all_objective_metrics\n",
    "\n",
    "# Load all registered objective scorer evaluations\n",
    "all_entries = load_all_objective_metrics()\n",
    "\n",
    "print(f\"Total registered evaluations: {len(all_entries)}\")\n",
    "\n",
    "# Browse first few entries\n",
    "for i, entry in enumerate(all_entries[:3], 1):\n",
    "    scorer_type = entry.get('__type__', 'Unknown')\n",
    "    metrics = entry.get('metrics', {})\n",
    "    accuracy = metrics.get('accuracy', 'N/A')\n",
    "    \n",
    "    print(f\"\\nEntry {i}:\")\n",
    "    print(f\"  Scorer: {scorer_type}\")\n",
    "    print(f\"  Accuracy: {accuracy:.2%}\" if isinstance(accuracy, float) else f\"  Accuracy: {accuracy}\")\n",
    "    print(f\"  Dataset Version: {entry.get('dataset_version', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc4f100",
   "metadata": {},
   "source": [
    "## Checking if Your Scorer Configuration Has Registry Metrics\n",
    "\n",
    "You can check if your specific scorer configuration has been evaluated on official datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this scorer configuration is in the registry\n",
    "registry_metrics = refusal_scorer.get_scorer_metrics_from_registry()\n",
    "\n",
    "if registry_metrics:\n",
    "    print(\"This scorer configuration has official metrics:\")\n",
    "    print(f\"  Accuracy: {registry_metrics.accuracy:.2%}\")\n",
    "    print(f\"  F1 Score: {registry_metrics.f1_score:.3f}\")\n",
    "else:\n",
    "    print(\"No official metrics found for this scorer configuration.\")\n",
    "    print(\"Run evaluate_async() to generate metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3104acc",
   "metadata": {},
   "source": [
    "## Adding to the Official Registry (PyRIT Team Only)\n",
    "\n",
    "When evaluating scorers on official consolidated datasets, PyRIT maintainers can add results to the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f06d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only set add_to_registry=True when using official consolidated datasets\n",
    "# metrics = await refusal_scorer.evaluate_async(\n",
    "#     \"pyrit/score/scorer_evals/true_false/CONSOLIDATED_true_false_objective_dataset.csv\",\n",
    "#     add_to_registry=True,  # Appends to official registry JSONL\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6aec3f",
   "metadata": {},
   "source": [
    "## Understanding Scorer Identifiers\n",
    "\n",
    "Each scorer has a unique identifier based on its configuration. This enables comparing different scorer setups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View scorer identifier\n",
    "identifier = refusal_scorer.get_identifier()\n",
    "print(f\"Scorer Type: {identifier.get('__type__')}\")\n",
    "print(f\"Configuration Hash: {identifier.get('hash')}\")\n",
    "print(f\"\\nFull identifier (compact):\")\n",
    "for key, value in list(identifier.items())[:5]:  # Show first 5 fields\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda6d0b",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Start with small datasets**: Use mini_*.csv files (10-30 examples) for quick iteration\n",
    "2. **Multiple trials for production**: Use `num_scorer_trials=3` or higher to measure scorer variance\n",
    "3. **Check trial_scores**: Analyze `metrics.trial_scores` to identify inconsistent examples\n",
    "4. **Compare configurations**: Evaluate different scorer setups (prompts, models, temperatures) to find optimal settings\n",
    "5. **Registry lookup first**: Check `get_scorer_metrics_from_registry()` before running expensive evaluations\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Create custom evaluation datasets with your own human labels\n",
    "- Compare multiple scorer configurations systematically\n",
    "- Use evaluation results to tune scorer prompts and parameters\n",
    "- Integrate scorer evaluation into your CI/CD pipeline for regression testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
