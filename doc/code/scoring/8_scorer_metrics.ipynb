{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8775f11a",
   "metadata": {},
   "source": [
    "# 8. Scorer Metrics\n",
    "\n",
    "PyRIT includes an evaluation framework to measure how well automated scorers align with human judgment. This is critical for understanding scorer reliability before using them in production scenarios.\n",
    "\n",
    "**Why evaluate scorers?**\n",
    "- Different scorer configurations (prompts, models, temperatures) produce different results\n",
    "- Metrics help you choose the best configuration for your use case. And helps us create better scorers\n",
    "- Understanding accuracy/precision/recall trade-offs guides how you interpret scorer outputs\n",
    "\n",
    "**Two types of scorer evaluations:**\n",
    "1. **Objective scorers** (true/false): Measured with accuracy, precision, recall, F1 score\n",
    "2. **Harm scorers** (Likert scale 0.0-1.0): Measured with mean absolute error, t-test, Krippendorff's alpha\n",
    "\n",
    "This notebook covers how to run evaluations, interpret metrics, retrieve cached results, and iterate on scorer configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f3405",
   "metadata": {},
   "source": [
    "## Objective Scorer Evaluation\n",
    "\n",
    "Objective scorers produce true/false outputs, usually whether an objective was met (e.g. did this response have instructions on \"how to make a Molotov cocktail?). We evaluate them using standard classification metrics by comparing model predictions against human-labeled ground truth.\n",
    "\n",
    "### Understanding Objective Metrics\n",
    "\n",
    "- **Accuracy**: Proportion of predictions matching human labels. Simple but can be misleading with imbalanced datasets.\n",
    "- **Precision**: Of all \"true\" predictions, how many were correct? High precision = few false positives.\n",
    "- **Recall**: Of all actual \"true\" cases, how many did we catch? High recall = few false negatives.\n",
    "- **F1 Score**: Harmonic mean of precision and recall. Balances both concerns.\n",
    "- **Accuracy Standard Error**: Statistical uncertainty in accuracy estimate, useful for confidence intervals.\n",
    "\n",
    "**Which metric matters most?**\n",
    "- If false positives are costly (e.g., flagging safe content as harmful) → prioritize **precision**\n",
    "- If false negatives are costly (e.g., missing actual jailbreaks) → prioritize **recall**\n",
    "- For balanced scenarios → use **F1 score**\n",
    "\n",
    "### Running an Objective Evaluation\n",
    "\n",
    "Call `evaluate_async()` on any scorer instance. The scorer's identity (including system prompt, model, temperature) determines which cached results apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd0793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found default environment files: ['C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env', 'C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env.local']\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env.local\n",
      "Evaluation Metrics:\n",
      "Dataset Name: refusal_scorer/test_refusal_evaluation_results.jsonl\n",
      "Dataset Version: 1.0\n",
      "F1 Score: 1.0\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyparsing import cast\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import SelfAskRefusalScorer\n",
    "from pyrit.score.scorer_evaluation.scorer_evaluator import ScorerEvalDatasetFiles\n",
    "from pyrit.score.scorer_evaluation.scorer_metrics import ObjectiveScorerMetrics\n",
    "from pyrit.setup import IN_MEMORY, initialize_pyrit_async\n",
    "\n",
    "await initialize_pyrit_async(memory_db_type=IN_MEMORY)\n",
    "\n",
    "# Create a refusal scorer - uses the chat target to determine if responses are refusals\n",
    "refusal_scorer = SelfAskRefusalScorer(chat_target=OpenAIChatTarget())\n",
    "\n",
    "\n",
    "# REAL usage would simply be the next line\n",
    "# but we're configuring a smaller eval for demo purposes\n",
    "# metrics = await refusal_scorer.evaluate_async()\n",
    "\n",
    "# For demonstration, use a smaller evaluation file (normally you'd use the full dataset)\n",
    "# The evaluation_file_mapping tells the evaluator which human-labeled CSV files to use\n",
    "refusal_scorer.evaluation_file_mapping = ScorerEvalDatasetFiles(\n",
    "    human_labeled_datasets_files=[\"refusal_scorer/mini_refusal.csv\"],\n",
    "    result_file=\"refusal_scorer/test_refusal_evaluation_results.jsonl\",\n",
    ")\n",
    "# Run evaluation with:\n",
    "# - num_scorer_trials=1: Score each response once (use 3+ for production to measure variance)\n",
    "# - add_to_evaluation_results=False: Don't save to the official registry (for testing only)\n",
    "metrics = await refusal_scorer.evaluate_async(num_scorer_trials=1, add_to_evaluation_results=False)\n",
    "\n",
    "if metrics:\n",
    "    objective_metrics = cast(ObjectiveScorerMetrics, metrics)\n",
    "    print (\"Evaluation Metrics:\")\n",
    "    print (f\"Dataset Name: {objective_metrics.dataset_name}\")\n",
    "    print (f\"Dataset Version: {objective_metrics.dataset_version}\")\n",
    "    print (f\"F1 Score: {objective_metrics.f1_score}\")\n",
    "    print (f\"Accuracy: {objective_metrics.accuracy}\")\n",
    "    print (f\"Precision: {objective_metrics.precision}\")\n",
    "    print (f\"Recall: {objective_metrics.recall}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Evaluation failed, no metrics returned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484d0d8",
   "metadata": {},
   "source": [
    "## Harm Scorer Evaluation\n",
    "\n",
    "Harm scorers produce float scores (0.0-1.0) representing severity. Since these are continuous values, we use different metrics that capture how close the model's scores are to human judgments.\n",
    "\n",
    "### Understanding Harm Metrics\n",
    "\n",
    "**Error Metrics:**\n",
    "- **Mean Absolute Error (MAE)**: Average absolute difference between model and human scores. An MAE of 0.15 means the model is off by ~15% on average.\n",
    "- **MAE Standard Error**: Uncertainty in the MAE estimate.\n",
    "\n",
    "**Statistical Significance:**\n",
    "- **t-statistic**: From a one-sample t-test. Positive = model scores higher than humans; negative = lower.\n",
    "- **p-value**: If < 0.05, the difference between model and human scores is statistically significant (not due to chance).\n",
    "\n",
    "**Inter-Rater Reliability (Krippendorff's Alpha):**\n",
    "Measures agreement between evaluators, ranging from -1.0 to 1.0:\n",
    "- **1.0**: Perfect agreement\n",
    "- **0.8+**: Strong agreement (typically acceptable for production)\n",
    "- **0.6-0.8**: Moderate agreement\n",
    "- **< 0.6**: Weak agreement (scorer may need tuning)\n",
    "\n",
    "Three alpha values are reported:\n",
    "- **`krippendorff_alpha_humans`**: Agreement among human evaluators (baseline quality of labels)\n",
    "- **`krippendorff_alpha_model`**: Agreement across multiple scoring trials (model consistency)\n",
    "- **`krippendorff_alpha_combined`**: Overall agreement between humans and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b096152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: test_hate_speech_evaluation_results\n",
      "  MAE:                0.167\n",
      "  MAE Std Error:      0.0314\n",
      "  t-statistic:        -0.612\n",
      "  p-value:            0.5501\n",
      "  Krippendorff Alpha: 0.678\n"
     ]
    }
   ],
   "source": [
    "from pyrit.score import SelfAskLikertScorer, LikertScalePaths\n",
    "\n",
    "# Create a harm scorer using the hate speech Likert scale\n",
    "likert_scorer = SelfAskLikertScorer(\n",
    "    chat_target=OpenAIChatTarget(),\n",
    "    likert_scale_path=LikertScalePaths.HATE_SPEECH_SCALE.value\n",
    ")\n",
    "\n",
    "# Configure evaluation to use a small sample dataset\n",
    "likert_scorer.evaluation_file_mapping = ScorerEvalDatasetFiles(\n",
    "        human_labeled_datasets_files=[\"harm/mini_hate_speech.csv\"],\n",
    "        result_file=\"harm/test_hate_speech_evaluation_results.jsonl\",\n",
    "        harm_category=\"hate_speech\",  # Required for harm evaluations\n",
    "    )\n",
    "\n",
    "harm_results = await likert_scorer.evaluate_async(num_scorer_trials=1, add_to_evaluation_results=False)\n",
    "\n",
    "for name, metrics in harm_results.items():\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"  MAE:                {metrics.mean_absolute_error:.3f}\")\n",
    "    print(f\"  MAE Std Error:      {metrics.mae_standard_error:.4f}\")\n",
    "    print(f\"  t-statistic:        {metrics.t_statistic:.3f}\")\n",
    "    print(f\"  p-value:            {metrics.p_value:.4f}\")\n",
    "    print(f\"  Krippendorff Alpha: {metrics.krippendorff_alpha_combined:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5ea0e",
   "metadata": {},
   "source": [
    "## Retrieving Cached Metrics\n",
    "\n",
    "Once scorer metrics are calculated with `evaluate_async()`, they're saved to JSONL registry files and can be retrieved without re-running the evaluation. The PyRIT team has pre-computed metrics for common scorer configurations which you can access immediately.\n",
    "\n",
    "Use `get_scorer_metrics()` to retrieve cached results for a scorer configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ee5063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached results for: refusal_evaluation_results\n",
      "  Accuracy:  100.00%\n",
      "  F1 Score:  1.000\n",
      "  Precision: 1.000\n",
      "  Recall:    1.000\n"
     ]
    }
   ],
   "source": [
    "# Create a new scorer instance with the same configuration\n",
    "# get_scorer_metrics() looks up results by scorer identity hash\n",
    "refusal_scorer_for_lookup = SelfAskRefusalScorer(chat_target=OpenAIChatTarget())\n",
    "\n",
    "# Retrieve pre-computed metrics (from PyRIT team's evaluation runs)\n",
    "cached_metrics = refusal_scorer_for_lookup.get_scorer_metrics()\n",
    "\n",
    "if cached_metrics:\n",
    "    for dataset_name, metrics in cached_metrics.items():\n",
    "        print(f\"Cached results for: {dataset_name}\")\n",
    "        print(f\"  Accuracy:  {metrics.accuracy:.2%}\")\n",
    "        print(f\"  F1 Score:  {metrics.f1_score:.3f}\")\n",
    "        print(f\"  Precision: {metrics.precision:.3f}\")\n",
    "        print(f\"  Recall:    {metrics.recall:.3f}\")\n",
    "else:\n",
    "    print(\"No cached metrics found for this scorer configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ceaa5",
   "metadata": {},
   "source": [
    "## Scorer Identity and Caching\n",
    "\n",
    "Evaluation results are cached by a hash of the scorer's full identity, which includes:\n",
    "- Scorer type (e.g., `SelfAskRefusalScorer`)\n",
    "- System and user prompt templates\n",
    "- Target model information (endpoint, model name)\n",
    "- Temperature and other generation parameters\n",
    "- Any scorer-specific configuration\n",
    "\n",
    "This means changing *any* of these values creates a new scorer identity, requiring fresh evaluation. It is important to note that the reason these are variables are because these are things that _might_ change the performance. Does changing the temperature increase or decrease accuracy? This allows users to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the scorer's full identity - this determines the metrics retrieved using get_scorer_metrics()\n",
    "scorer_identity = refusal_scorer.scorer_identifier\n",
    "print(\"Scorer Identity:\")\n",
    "print(f\"  Type: {scorer_identity.scorer_type}\")\n",
    "print(f\"  System Prompt Hash: {scorer_identity.system_prompt_template[:50] if scorer_identity.system_prompt_template else 'None'}...\")\n",
    "print(f\"  Target Info: {scorer_identity.target_info}\")\n",
    "print(f\"  Identity Hash: {scorer_identity.compute_hash()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc063dda",
   "metadata": {},
   "source": [
    "## Comparing Scorer Configurations\n",
    "\n",
    "The evaluation registry stores metrics for all tested scorer configurations. You can load all entries to compare which configurations perform best for different metrics.\n",
    "\n",
    "Use `get_all_objective_metrics()` or `get_all_harm_metrics()` to load evaluation results. These return properly typed `ScorerMetricsWithIdentity` objects with clean attribute access to both the scorer identity and its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe31fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 scorer configurations in the registry\n",
      "\n",
      "Top 5 configurations by F1 Score:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. F1=0.814  Accuracy=78.38%  Precision=0.714  Recall=0.946\n",
      "   Scorer: {'__type__': 'TrueFalseInverterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'SelfAskRefusalScorer', 'system_prompt_template': 'sha256:8e8ddfa0aa41578a', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-unsafe', 'temperature': 0.9}, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': None, 'hash': 'a13631702f000907fa0b7b6a0f62b05aed9effe291674448a958b34658893575'}\n",
      "\n",
      "2. F1=0.802  Accuracy=77.03%  Precision=0.704  Recall=0.932\n",
      "   Scorer: {'__type__': 'TrueFalseInverterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'SelfAskRefusalScorer', 'system_prompt_template': 'sha256:8e8ddfa0aa41578a', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-unsafe'}, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': None, 'hash': '1eebe11cf0ff95f1b8872cdbae697c81723b44e3c383c84935df64ebb330ffc3'}\n",
      "\n",
      "3. F1=0.435  Accuracy=56.08%  Precision=0.610  Recall=0.338\n",
      "   Scorer: {'__type__': 'FloatScaleThresholdScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'AzureContentFilterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': None, 'target_info': None, 'score_aggregator': None, 'scorer_specific_params': {'score_categories': ['Hate', 'SelfHarm', 'Sexual', 'Violence']}}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': {'threshold': 0.1, 'float_scale_aggregator': 'MAX_'}, 'hash': 'ef97483941e22275ac9d9744e8ef94d1497e15f38a37c7d216ad0732005a58ea'}\n",
      "\n",
      "4. F1=0.227  Accuracy=54.05%  Precision=0.714  Recall=0.135\n",
      "   Scorer: {'__type__': 'TrueFalseCompositeScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'FloatScaleThresholdScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'AzureContentFilterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': None, 'target_info': None, 'score_aggregator': None, 'scorer_specific_params': {'score_categories': ['Hate', 'SelfHarm', 'Sexual', 'Violence']}}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': {'threshold': 0.5, 'float_scale_aggregator': 'MAX_'}}, {'__type__': 'TrueFalseInverterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'SelfAskRefusalScorer', 'system_prompt_template': 'sha256:8e8ddfa0aa41578a', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-unsafe', 'temperature': 0.9}, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'AND_', 'scorer_specific_params': None, 'hash': 'cc98484f3e53c827b48bae5d277c1f6a46fc8f6c58b34b706e30a2d1ffa3830d'}\n",
      "\n",
      "5. F1=0.225  Accuracy=53.38%  Precision=0.667  Recall=0.135\n",
      "   Scorer: {'__type__': 'FloatScaleThresholdScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'AzureContentFilterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': None, 'target_info': None, 'score_aggregator': None, 'scorer_specific_params': {'score_categories': ['Hate', 'SelfHarm', 'Sexual', 'Violence']}}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': {'threshold': 0.5, 'float_scale_aggregator': 'MAX_'}, 'hash': '70a6be133c6afd9af9dcc59a4b521833605a92ad977c5b894f88cd0ea39ef2cb'}\n",
      "\n",
      "================================================================================\n",
      "Best Accuracy:  78.38%\n",
      "Best Precision: 0.714\n",
      "Best Recall:    0.946\n"
     ]
    }
   ],
   "source": [
    "from pyrit.score import get_all_objective_metrics\n",
    "\n",
    "# Load all objective scorer metrics - returns ScorerMetricsWithIdentity[ObjectiveScorerMetrics]\n",
    "all_scorers = get_all_objective_metrics()\n",
    "\n",
    "print(f\"Found {len(all_scorers)} scorer configurations in the registry\\n\")\n",
    "\n",
    "# Sort by F1 score - type checker knows entry.metrics is ObjectiveScorerMetrics\n",
    "sorted_by_f1 = sorted(all_scorers, key=lambda x: x.metrics.f1_score, reverse=True)\n",
    "\n",
    "print(\"Top 5 configurations by F1 Score:\")\n",
    "print(\"-\" * 80)\n",
    "for i, entry in enumerate(sorted_by_f1[:5], 1):\n",
    "    m = entry.metrics  # ObjectiveScorerMetrics - no cast needed!\n",
    "    print(f\"\\n{i}. F1={m.f1_score:.3f}  Accuracy={m.accuracy:.2%}  Precision={m.precision:.3f}  Recall={m.recall:.3f}\")\n",
    "    print(f\"   Scorer Type: {entry.scorer_identifier.type}\")\n",
    "\n",
    "# Show best by each metric\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Best Accuracy:  {max(all_scorers, key=lambda x: x.metrics.accuracy).metrics.accuracy:.2%}\")\n",
    "print(f\"Best Precision: {max(all_scorers, key=lambda x: x.metrics.precision).metrics.precision:.3f}\")\n",
    "print(f\"Best Recall:    {max(all_scorers, key=lambda x: x.metrics.recall).metrics.recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a52059",
   "metadata": {},
   "source": [
    "## Custom Evaluation Datasets\n",
    "\n",
    "You can evaluate scorers against your own human-labeled datasets using `ScorerEvalDatasetFiles`:\n",
    "\n",
    "```python\n",
    "# Configure custom datasets\n",
    "scorer.evaluation_file_mapping = [\n",
    "    ScorerEvalDatasetFiles(\n",
    "        # Glob patterns relative to SCORER_EVALS_PATH\n",
    "        human_labeled_datasets_files=[\"my_datasets/*.csv\"],\n",
    "        # Where to store results\n",
    "        result_file=\"my_datasets/evaluation_results.jsonl\",\n",
    "        # Required for harm scorers, ignored for objective scorers\n",
    "        harm_category=\"violence\",\n",
    "    )\n",
    "]\n",
    "```\n",
    "\n",
    "### CSV Format\n",
    "\n",
    "**Required columns:**\n",
    "- `assistant_response`: The text to score\n",
    "- `objective`: Required for **objective scorers** - the scoring question (e.g., \"Did the response refuse?\")\n",
    "- `harm_category`: Required for **harm scorers** - the harm category (e.g., \"hate_speech\", \"violence\")\n",
    "- `human_score` or `human_score_1`, `human_score_2`, etc.: Ground truth labels from human raters\n",
    "  - For objective scorers: 0 or 1 (converted to bool)\n",
    "  - For harm scorers: 0.0-1.0 float values\n",
    "- `data_type`: Type of content (defaults to \"text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
