{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963cb838",
   "metadata": {},
   "source": [
    "## Understanding Scorer Metrics\n",
    "\n",
    "PyRIT has metrics for many scorers checked in. Before diving into how to retrieve or create metrics, it's important to understand what they measure and how scorer identity determines which metrics apply.\n",
    "\n",
    "### Scorer Identity\n",
    "\n",
    "Every scorer has a unique **identity hash** computed from its complete configuration:\n",
    "- Scorer type (e.g., `SelfAskRefusalScorer`)\n",
    "- System and user prompt templates\n",
    "- Target model information (endpoint, model name)\n",
    "- Temperature and other generation parameters\n",
    "- Any scorer-specific configuration\n",
    "\n",
    "This means changing *any* of these values creates a new scorer identity. The reason these are variables is because they _might_ change performance—does changing the temperature increase or decrease accuracy? Metrics let you experiment and find out.\n",
    "\n",
    "Metrics are stored and retrieved by this identity hash, so the same scorer configuration will always get the same cached metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c820c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found default environment files: ['C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env', 'C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env.local']\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env.local\n",
      "Scorer Identity:\n",
      "  Type: SelfAskRefusalScorer\n",
      "  System Prompt Hash: You are an evaluator tasked with determining if th...\n",
      "  Target Info: {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-40'}\n",
      "  Identity Hash: d9007ba254383ed04e7240f5bcf6ac6d5e34d79dd7653a800d2f0ed4a26a84ba\n"
     ]
    }
   ],
   "source": [
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import SelfAskRefusalScorer\n",
    "from pyrit.setup import IN_MEMORY, initialize_pyrit_async\n",
    "\n",
    "await initialize_pyrit_async(memory_db_type=IN_MEMORY)\n",
    "\n",
    "# Create a refusal scorer\n",
    "refusal_scorer = SelfAskRefusalScorer(chat_target=OpenAIChatTarget())\n",
    "\n",
    "# View the scorer's full identity - this determines which metrics apply\n",
    "scorer_identity = refusal_scorer.scorer_identifier\n",
    "print(\"Scorer Identity:\")\n",
    "print(f\"  Type: {scorer_identity.type}\")\n",
    "print(f\"  System Prompt Hash: {scorer_identity.system_prompt_template[:50] if scorer_identity.system_prompt_template else 'None'}...\")\n",
    "print(f\"  Target Info: {scorer_identity.target_info}\")\n",
    "print(f\"  Identity Hash: {scorer_identity.compute_hash()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dba2e",
   "metadata": {},
   "source": [
    "### Objective Metrics\n",
    "\n",
    "Objective scorers produce true/false outputs, usually whether an objective was met (e.g., did this response have instructions on \"how to make a Molotov cocktail?\"). We evaluate them using standard classification metrics by comparing model predictions against human-labeled ground truth.\n",
    "\n",
    "- **Accuracy**: Proportion of predictions matching human labels. Simple but can be misleading with imbalanced datasets.\n",
    "- **Precision**: Of all \"true\" predictions, how many were correct? High precision = few false positives.\n",
    "- **Recall**: Of all actual \"true\" cases, how many did we catch? High recall = few false negatives.\n",
    "- **F1 Score**: Harmonic mean of precision and recall. Balances both concerns.\n",
    "- **Accuracy Standard Error**: Statistical uncertainty in accuracy estimate, useful for confidence intervals.\n",
    "\n",
    "**Which metric matters most?**\n",
    "- If false positives are costly (e.g., flagging safe content as harmful) → prioritize **precision**\n",
    "- If false negatives are costly (e.g., missing actual jailbreaks) → prioritize **recall**\n",
    "- For balanced scenarios → use **F1 score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08773ad",
   "metadata": {},
   "source": [
    "### Harm Metrics\n",
    "\n",
    "Harm scorers produce float scores (0.0-1.0) representing severity. Since these are continuous values, we use different metrics that capture how close the model's scores are to human judgments.\n",
    "\n",
    "**Error Metrics:**\n",
    "- **Mean Absolute Error (MAE)**: Average absolute difference between model and human scores. An MAE of 0.15 means the model is off by ~15% on average.\n",
    "- **MAE Standard Error**: Uncertainty in the MAE estimate.\n",
    "\n",
    "**Statistical Significance:**\n",
    "- **t-statistic**: From a one-sample t-test. Positive = model scores higher than humans; negative = lower.\n",
    "- **p-value**: If < 0.05, the difference between model and human scores is statistically significant (not due to chance).\n",
    "\n",
    "**Inter-Rater Reliability (Krippendorff's Alpha):**\n",
    "Measures agreement between evaluators, ranging from -1.0 to 1.0:\n",
    "- **1.0**: Perfect agreement\n",
    "- **0.8+**: Strong agreement\n",
    "- **0.6-0.8**: Moderate agreement\n",
    "- **< 0.6**: Weak agreement (scorer may need tuning)\n",
    "\n",
    "Three alpha values are reported:\n",
    "- **`krippendorff_alpha_humans`**: Agreement among human evaluators (baseline quality of labels)\n",
    "- **`krippendorff_alpha_model`**: Agreement across multiple scoring trials (model consistency)\n",
    "- **`krippendorff_alpha_combined`**: Overall agreement between humans and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165b7c4",
   "metadata": {},
   "source": [
    "## Retrieving Scorer Metrics\n",
    "\n",
    "Once scorer metrics are calculated with `evaluate_async()`, they're saved to JSONL registry files and can be retrieved without re-running the evaluation. The PyRIT team has pre-computed metrics for common scorer configurations which you can access immediately.\n",
    "\n",
    "### Retrieving Cached Metrics for a Scorer\n",
    "\n",
    "Use `get_scorer_metrics()` on any scorer instance to retrieve cached results matching its identity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f572996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "  Dataset Name: objective/objective_achieved_metrics.jsonl\n",
      "  Dataset Version: 1.0\n",
      "  F1 Score: 0.8395061728395061\n",
      "  Accuracy: 0.8243243243243243\n",
      "  Precision: 0.7727272727272727\n",
      "  Recall: 0.918918918918919\n",
      "  Avg Score time: 0.43015325405403027 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyrit.score import TrueFalseInverterScorer, SelfAskRefusalScorer, ConsoleScorerPrinter\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "\n",
    "# This is a simple objective scorer that only detects whether the response was a refusal\n",
    "objective_scorer = TrueFalseInverterScorer(\n",
    "        scorer=SelfAskRefusalScorer(\n",
    "            chat_target=OpenAIChatTarget(\n",
    "                endpoint=os.environ.get(\"AZURE_OPENAI_GPT4O_ENDPOINT\"),\n",
    "                api_key=os.environ.get(\"AZURE_OPENAI_GPT4O_KEY\"),\n",
    "                model_name=os.environ.get(\"AZURE_OPENAI_GPT4O_MODEL\"),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Retrieve pre-computed metrics (from PyRIT team's evaluation runs)\n",
    "# using the scorer's identity hash\n",
    "cached_metrics = objective_scorer.get_scorer_metrics()\n",
    "\n",
    "if cached_metrics:\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"  Dataset Name: {cached_metrics.dataset_name}\")\n",
    "    print(f\"  Dataset Version: {cached_metrics.dataset_version}\")\n",
    "    print(f\"  F1 Score: {cached_metrics.f1_score}\")\n",
    "    print(f\"  Accuracy: {cached_metrics.accuracy}\")\n",
    "    print(f\"  Precision: {cached_metrics.precision}\")\n",
    "    print(f\"  Recall: {cached_metrics.recall}\")\n",
    "    print(f\"  Avg Score time: {cached_metrics.average_score_time_seconds} seconds\")\n",
    "else:\n",
    "    print(\"No cached metrics found for this scorer configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83115c62",
   "metadata": {},
   "source": [
    "Harm scorer metrics are retrieved similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcb5795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "  Dataset Name: harm/exploits_metrics.jsonl\n",
      "  Dataset Version: 1.0\n",
      "  Krippendorff Alpha: 0.458\n",
      "  P value: 0.0002\n",
      "  Avg Score time: 0.7639685599999557 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyrit.score import SelfAskLikertScorer, LikertScalePaths\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "\n",
    "harm_scorer = SelfAskLikertScorer(\n",
    "        chat_target=OpenAIChatTarget(\n",
    "            endpoint=os.environ.get(\"AZURE_OPENAI_GPT4O_ENDPOINT\"),\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_GPT4O_KEY\"),\n",
    "            model_name=os.environ.get(\"AZURE_OPENAI_GPT4O_MODEL\"),\n",
    "        ),\n",
    "        likert_scale=LikertScalePaths.EXPLOITS_SCALE,\n",
    "    )\n",
    "\n",
    "# Retrieve pre-computed metrics using the scorer's identity hash\n",
    "harm_metrics = harm_scorer.get_scorer_metrics()\n",
    "\n",
    "if harm_metrics:\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"  Dataset Name: {harm_metrics.dataset_name}\")\n",
    "    print(f\"  Dataset Version: {harm_metrics.dataset_version}\")\n",
    "    print(f\"  Krippendorff Alpha: {harm_metrics.krippendorff_alpha_combined:.3f}\")\n",
    "    print(f\"  P value: {harm_metrics.p_value:.4f}\")\n",
    "    print(f\"  Avg Score time: {harm_metrics.average_score_time_seconds} seconds\")\n",
    "\n",
    "else:\n",
    "    print(\"No cached metrics found for this scorer configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636d683",
   "metadata": {},
   "source": [
    "### Comparing All Scorer Configurations\n",
    "\n",
    "The evaluation registry stores metrics for all tested scorer configurations. You can load all entries to compare which configurations perform best.\n",
    "\n",
    "Use `get_all_objective_metrics()` or `get_all_harm_metrics()` to load evaluation results. These return `ScorerMetricsWithIdentity` objects with clean attribute access to both the scorer identity and its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36f3728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 scorer configurations in the registry\n",
      "\n",
      "Top 5 configurations by F1 Score:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. F1=0.840  Accuracy=82.43%  Precision=0.773  Recall=0.919\n",
      "   Scorer Type: {'__type__': 'TrueFalseInverterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'SelfAskRefusalScorer', 'system_prompt_template': 'sha256:8e8ddfa0aa41578a', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-japan-nilfilter'}, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': None, 'hash': '1db406024b6cb2bb32a89dc7208f765668fc0596a872cab50747a58da2fa8aa0'}\n",
      "\n",
      "2. F1=0.826  Accuracy=79.73%  Precision=0.724  Recall=0.959\n",
      "   Scorer Type: {'__type__': 'TrueFalseInverterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'SelfAskRefusalScorer', 'system_prompt_template': 'sha256:8e8ddfa0aa41578a', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-unsafe'}, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': None, 'hash': '1eebe11cf0ff95f1b8872cdbae697c81723b44e3c383c84935df64ebb330ffc3'}\n",
      "\n",
      "3. F1=0.812  Accuracy=78.38%  Precision=0.719  Recall=0.932\n",
      "   Scorer Type: {'__type__': 'TrueFalseInverterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'SelfAskRefusalScorer', 'system_prompt_template': 'sha256:8e8ddfa0aa41578a', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-unsafe', 'temperature': 0.9}, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': None, 'hash': 'a13631702f000907fa0b7b6a0f62b05aed9effe291674448a958b34658893575'}\n",
      "\n",
      "4. F1=0.435  Accuracy=56.08%  Precision=0.610  Recall=0.338\n",
      "   Scorer Type: {'__type__': 'FloatScaleThresholdScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'AzureContentFilterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': None, 'target_info': None, 'score_aggregator': None, 'scorer_specific_params': {'score_categories': ['Hate', 'SelfHarm', 'Sexual', 'Violence']}}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': {'threshold': 0.1, 'float_scale_aggregator': 'MAX_'}, 'hash': 'ef97483941e22275ac9d9744e8ef94d1497e15f38a37c7d216ad0732005a58ea'}\n",
      "\n",
      "5. F1=0.227  Accuracy=54.05%  Precision=0.714  Recall=0.135\n",
      "   Scorer Type: {'__type__': 'TrueFalseCompositeScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'FloatScaleThresholdScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'AzureContentFilterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': None, 'target_info': None, 'score_aggregator': None, 'scorer_specific_params': {'score_categories': ['Hate', 'SelfHarm', 'Sexual', 'Violence']}}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': {'threshold': 0.5, 'float_scale_aggregator': 'MAX_'}}, {'__type__': 'TrueFalseInverterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': [{'__type__': 'SelfAskRefusalScorer', 'system_prompt_template': 'sha256:8e8ddfa0aa41578a', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-unsafe', 'temperature': 0.9}, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'OR_', 'scorer_specific_params': None}], 'target_info': None, 'score_aggregator': 'AND_', 'scorer_specific_params': None, 'hash': 'cc98484f3e53c827b48bae5d277c1f6a46fc8f6c58b34b706e30a2d1ffa3830d'}\n",
      "\n",
      "================================================================================\n",
      "Best Accuracy:  82.43%\n",
      "Best Precision: 0.773\n",
      "Best Recall:    0.959\n",
      "Fastest:        0.118 seconds\n",
      "Slowest:        0.759 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyrit.score import get_all_objective_metrics\n",
    "\n",
    "# Load all objective scorer metrics - returns ScorerMetricsWithIdentity[ObjectiveScorerMetrics]\n",
    "all_scorers = get_all_objective_metrics()\n",
    "\n",
    "print(f\"Found {len(all_scorers)} scorer configurations in the registry\\n\")\n",
    "\n",
    "# Sort by F1 score - type checker knows entry.metrics is ObjectiveScorerMetrics\n",
    "sorted_by_f1 = sorted(all_scorers, key=lambda x: x.metrics.f1_score, reverse=True)\n",
    "\n",
    "print(\"Top 5 configurations by F1 Score:\")\n",
    "print(\"-\" * 80)\n",
    "for i, entry in enumerate(sorted_by_f1[:5], 1):\n",
    "    m = entry.metrics  # ObjectiveScorerMetrics - no cast needed!\n",
    "    print(f\"\\n{i}. F1={m.f1_score:.3f}  Accuracy={m.accuracy:.2%}  Precision={m.precision:.3f}  Recall={m.recall:.3f}\")\n",
    "    print(f\"   Scorer Type: {entry.scorer_identifier.to_compact_dict()}\")\n",
    "\n",
    "# Show best by each metric\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Best Accuracy:  {max(all_scorers, key=lambda x: x.metrics.accuracy).metrics.accuracy:.2%}\")\n",
    "print(f\"Best Precision: {max(all_scorers, key=lambda x: x.metrics.precision).metrics.precision:.3f}\")\n",
    "print(f\"Best Recall:    {max(all_scorers, key=lambda x: x.metrics.recall).metrics.recall:.3f}\")\n",
    "print(f\"Fastest:        {min(all_scorers, key=lambda x: x.metrics.average_score_time_seconds).metrics.average_score_time_seconds:.3f} seconds\")\n",
    "print(f\"Slowest:        {max(all_scorers, key=lambda x: x.metrics.average_score_time_seconds).metrics.average_score_time_seconds:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b5f2f",
   "metadata": {},
   "source": [
    "Similarly, you can look at the best harm scorers for a given category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca36f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 harm scorer configurations for violence\n",
      "\n",
      "Top configurations by Krippendorff's Alpha:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. K Alpha=0.868  MAE=0.171  p-value=0.3052  Avg Score time: 0.8147548685714442 seconds\n",
      "   Scorer Type: {'__type__': 'SelfAskLikertScorer', 'system_prompt_template': 'sha256:c8e2c8367b8f24a3', 'user_prompt_template': None, 'sub_identifier': None, 'target_info': {'__type__': 'OpenAIChatTarget', 'model_name': 'gpt-4o-japan-nilfilter'}, 'score_aggregator': None, 'scorer_specific_params': None, 'hash': '1969e7b7f18b6d7016d5c7184042c58474ee3ba4033d2e5ec9ea8e0d100b2a1b'}\n",
      "\n",
      "2. K Alpha=0.850  MAE=0.213  p-value=0.4873  Avg Score time: 0.11533883523800233 seconds\n",
      "   Scorer Type: {'__type__': 'AzureContentFilterScorer', 'system_prompt_template': None, 'user_prompt_template': None, 'sub_identifier': None, 'target_info': None, 'score_aggregator': None, 'scorer_specific_params': {'score_categories': ['Violence']}, 'hash': '3f87858a39d506c29e7bd63011248a5b29d3ea260c398a840409810c56db851d'}\n"
     ]
    }
   ],
   "source": [
    "from pyrit.score import get_all_harm_metrics\n",
    "\n",
    "# Load all harm scorer metrics for a specific category\n",
    "all_harm_scorers = get_all_harm_metrics(harm_category=\"violence\")\n",
    "\n",
    "print(f\"Found {len(all_harm_scorers)} harm scorer configurations for violence\\n\")\n",
    "\n",
    "# Sort by Krippendorff's alpha (higher is better)\n",
    "sorted_by_alpha = sorted(all_harm_scorers, key=lambda x: x.metrics.krippendorff_alpha_combined, reverse=True)\n",
    "\n",
    "print(\"Top configurations by Krippendorff's Alpha:\")\n",
    "print(\"-\" * 80)\n",
    "for i, entry in enumerate(sorted_by_alpha[:5], 1):\n",
    "    m = entry.metrics\n",
    "    print(f\"\\n{i}. K Alpha={m.krippendorff_alpha_combined:.3f}  MAE={m.mean_absolute_error:.3f}  p-value={m.p_value:.4f}  Avg Score time: {m.average_score_time_seconds} seconds\")\n",
    "    print(f\"   Scorer Type: {entry.scorer_identifier.to_compact_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff31e46",
   "metadata": {},
   "source": [
    "## Creating Scorer Metrics\n",
    "\n",
    "This section covers how to create new metrics by running evaluations against human-labeled datasets.\n",
    "\n",
    "### Caching and Skip Logic\n",
    "\n",
    "When you call `evaluate_async()` on a scorer, the evaluation framework follows a smart caching strategy to avoid redundant work. It checks the metrics registry (a JSONL file) for an existing entry matching the scorer's identity hash. The decision to skip or run evaluation depends on:\n",
    "\n",
    "1. **No existing entry**: Run the full evaluation\n",
    "2. **Dataset version changed**: Re-run and replace the old entry (assumes newer dataset is authoritative)\n",
    "3. **Same version, sufficient trials**: Skip if existing `num_scorer_trials >= requested` (existing metrics are good enough)\n",
    "4. **Same version, fewer trials**: Re-run with more trials and replace (higher fidelity needed)\n",
    "\n",
    "During evaluation, the scorer processes each entry from human-labeled CSV dataset(s). For each `assistant_response` in the CSV, the scorer generates predictions which are compared against the `human_score` column(s). For objective scorers, this produces accuracy/precision/recall/F1 metrics. For harm scorers, it calculates MAE, t-statistics, and Krippendorff's alpha.\n",
    "\n",
    "Setting `add_to_evaluation_results=False` bypasses caching entirely—always running fresh evaluations without reading from or writing to the registry. This is useful for testing custom configurations without polluting the official metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c8d74",
   "metadata": {},
   "source": [
    "### Running an Objective Evaluation\n",
    "\n",
    "Call `evaluate_async()` on any scorer instance. The scorer's identity (including system prompt, model, temperature) determines which cached results apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e928bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics created refusal_scorer/test_refusal_metrics.jsonl\n"
     ]
    }
   ],
   "source": [
    "from typing import cast\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import SelfAskRefusalScorer, ScorerEvalDatasetFiles, ObjectiveScorerMetrics, RegistryUpdateBehavior\n",
    "\n",
    "# Create a refusal scorer - uses the chat target to determine if responses are refusals\n",
    "refusal_scorer = SelfAskRefusalScorer(chat_target=OpenAIChatTarget())\n",
    "\n",
    "# REAL usage would simply be:\n",
    "# metrics = await refusal_scorer.evaluate_async()\n",
    "\n",
    "# For demonstration, use a smaller evaluation file (normally you'd use the full dataset)\n",
    "# The evaluation_file_mapping tells the evaluator which human-labeled CSV files to use\n",
    "refusal_scorer.evaluation_file_mapping = ScorerEvalDatasetFiles(\n",
    "    human_labeled_datasets_files=[\"refusal_scorer/mini_refusal.csv\"],\n",
    "    result_file=\"refusal_scorer/test_refusal_metrics.jsonl\",\n",
    ")\n",
    "\n",
    "# Run evaluation with:\n",
    "# - num_scorer_trials=1: Score each response once (use 3+ for production to measure variance)\n",
    "# - RegistryUpdateBehavior.NEVER_UPDATE: Don't save to the official registry (for testing only)\n",
    "metrics = await refusal_scorer.evaluate_async(num_scorer_trials=1, update_registry_behavior=RegistryUpdateBehavior.NEVER_UPDATE)\n",
    "\n",
    "if metrics:\n",
    "    objective_metrics = cast(ObjectiveScorerMetrics, metrics)\n",
    "    print(f\"Evaluation Metrics created {objective_metrics.dataset_name}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Evaluation failed, no metrics returned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d30f3",
   "metadata": {},
   "source": [
    "### Running a Harm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b488cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: harm/test_hate_speech_metrics.jsonl created\n"
     ]
    }
   ],
   "source": [
    "from typing import cast\n",
    "from pyrit.score import SelfAskLikertScorer, LikertScalePaths\n",
    "from pyrit.score.scorer_evaluation.scorer_evaluator import ScorerEvalDatasetFiles\n",
    "from pyrit.score.scorer_evaluation.scorer_metrics import HarmScorerMetrics\n",
    "\n",
    "# Create a harm scorer using the hate speech Likert scale\n",
    "likert_scorer = SelfAskLikertScorer(\n",
    "    chat_target=OpenAIChatTarget(),\n",
    "    likert_scale=LikertScalePaths.HATE_SPEECH_SCALE\n",
    ")\n",
    "\n",
    "# Configure evaluation to use a small sample dataset\n",
    "likert_scorer.evaluation_file_mapping = ScorerEvalDatasetFiles(\n",
    "    human_labeled_datasets_files=[\"harm/mini_hate_speech.csv\"],\n",
    "    result_file=\"harm/test_hate_speech_metrics.jsonl\",\n",
    "    harm_category=\"hate_speech\",  # Required for harm evaluations\n",
    ")\n",
    "\n",
    "# This can be called without parameters to update the registry\n",
    "metrics = await likert_scorer.evaluate_async(num_scorer_trials=1, update_registry_behavior=RegistryUpdateBehavior.NEVER_UPDATE)\n",
    "\n",
    "if metrics:\n",
    "    harm_metrics = cast(HarmScorerMetrics, metrics)\n",
    "    print(f\"Dataset: {harm_metrics.dataset_name} created\")\n",
    "else:\n",
    "    raise RuntimeError(\"Evaluation failed, no metrics returned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce652b",
   "metadata": {},
   "source": [
    "### Custom Evaluation Datasets\n",
    "\n",
    "You can evaluate scorers against your own human-labeled datasets using `ScorerEvalDatasetFiles`:\n",
    "\n",
    "```python\n",
    "# Configure custom datasets\n",
    "scorer.evaluation_file_mapping = ScorerEvalDatasetFiles(\n",
    "    # Glob patterns relative to SCORER_EVALS_PATH\n",
    "    human_labeled_datasets_files=[\"my_datasets/*.csv\"],\n",
    "    # Where to store results\n",
    "    result_file=\"my_datasets/evaluation_metrics.jsonl\",\n",
    "    # Required for harm scorers, ignored for objective scorers\n",
    "    harm_category=\"violence\",\n",
    ")\n",
    "```\n",
    "\n",
    "### CSV Human Evaluation Files\n",
    "\n",
    "Many human scored dataset csv files are available in the `pyrit/datasets/scorer_evals/` directory. These include datasets for refusal detection, hate speech, violence, and other harm categories. You can reference these as templates for creating your own evaluation datasets.\n",
    "\n",
    "**Required columns:**\n",
    "- `assistant_response`: The text to score\n",
    "- `objective`: Required for **objective scorers** - the scoring question (e.g., \"Did the response refuse?\")\n",
    "- `harm_category`: Required for **harm scorers** - the harm category (e.g., \"hate_speech\", \"violence\")\n",
    "- `human_score` or `human_score_1`, `human_score_2`, etc.: Ground truth labels from human raters\n",
    "  - For objective scorers: 0 or 1 (converted to bool)\n",
    "  - For harm scorers: 0.0-1.0 float values\n",
    "- `data_type`: Type of content (defaults to \"text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
