{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8775f11a",
   "metadata": {},
   "source": [
    "# 8. Scorer Metrics\n",
    "\n",
    "PyRIT includes metrics on scorers to evaluate the efficacy of different scorer configurations. They are caculated by comparing our automated scorers against human labeled datasets. You see these numbers automatically as part of Scenario output. This documentation goes through how to run new evaluations on scorers or to get existing metrics if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3ad689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found default environment files: ['C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env', 'C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env.local']\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env.local\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import SelfAskRefusalScorer\n",
    "from pyrit.setup import IN_MEMORY, initialize_pyrit_async\n",
    "\n",
    "await initialize_pyrit_async(memory_db_type=IN_MEMORY)\n",
    "target = OpenAIChatTarget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f3405",
   "metadata": {},
   "source": [
    "## Running a Scorer Evaluation\n",
    "\n",
    "The simplest way to evaluate a scorer is to call `evaluate_async()` on the scorer instance. A scorer has many parts to its identity that may influence the accuracy of a scorer. These include the following, which are part of the `ScorerIdentifier` class:\n",
    "\n",
    "```python\n",
    "    type: str\n",
    "    system_prompt_template: Optional[str] = None\n",
    "    user_prompt_template: Optional[str] = None\n",
    "    sub_identifier: Optional[List[ScorerIdentifier]] = None\n",
    "    target_info: Optional[Dict[str, Any]] = None\n",
    "    score_aggregator: Optional[str] = None\n",
    "    scorer_specific_params: Optional[Dict[str, Any]] = None\n",
    "```\n",
    "\n",
    "A `Scorer` also has an optional evaluation_file_mapping attribute. This configures what human datasets (and optionally harm categories) a `Scorer` will run against. Below is a snippet that runs an evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and evaluate a refusal scorer\n",
    "from pyrit.score.scorer_evaluation.scorer_evaluator import ScorerEvalDatasetFiles\n",
    "\n",
    "\n",
    "refusal_scorer = SelfAskRefusalScorer(chat_target=target)\n",
    "\n",
    "# Run evaluation - uses the scorer's default evaluation_file_mapping\n",
    "\n",
    "# Real usage would run the evaluation on the whole dataset and save the results\n",
    "# results = await refusal_scorer.evaluate_async()\n",
    "\n",
    "# For demonstration, we will use a smaller evaluation file mapping\n",
    "# run only one trial, and not add to the evaluation results\n",
    "\n",
    "refusal_scorer.evaluation_file_mapping = [\n",
    "    ScorerEvalDatasetFiles(\n",
    "        human_labeled_datasets_files=[\"refusal_scorer/mini_refusal.csv\"],\n",
    "        result_file=\"refusal_scorer/test_refusal_evaluation_results.jsonl\",\n",
    "    )\n",
    "]\n",
    "\n",
    "results = await refusal_scorer.evaluate_async(num_scorer_trials=1, add_to_evaluation_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f4f3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: refusal_evaluation_results\n",
      "  Accuracy: 100.00%\n",
      "  Precision: 1.000\n",
      "  Recall: 1.000\n",
      "  F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Results is a dict mapping dataset name to metrics\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"  Accuracy: {metrics.accuracy:.2%}\")\n",
    "    print(f\"  Precision: {metrics.precision:.3f}\")\n",
    "    print(f\"  Recall: {metrics.recall:.3f}\")\n",
    "    print(f\"  F1 Score: {metrics.f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484d0d8",
   "metadata": {},
   "source": [
    "## Retrieving Scorer metrics\n",
    "\n",
    "Once scorer metrics for a given scorer are calculated with `evaluate_async`, those metrics are saved (and checked in). We have run quite a few and checked them in already; see `build_scripts\\evaluate_scorers.py`. You can then retrieve them for any scorer using `get_scorer_metrics`, which is what you see when scenarios are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b096152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refusal_evaluation_results': ObjectiveScorerMetrics(num_responses=174, num_human_raters=1, num_scorer_trials=1, dataset_name='refusal_scorer/refusal_evaluation_results.jsonl', dataset_version='1.0', trial_scores=None, accuracy=1.0, accuracy_standard_error=0.0, f1_score=1.0, precision=1.0, recall=1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Note these are not from our sample run above, they are retrieved from a full run that is checked in.\n",
    "\n",
    "refusal_scorer2 = SelfAskRefusalScorer(chat_target=target)\n",
    "metrics = refusal_scorer2.get_scorer_metrics()\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5ea0e",
   "metadata": {},
   "source": [
    "## Understanding the Metrics\n",
    "\n",
    "For objective (true/false) scorers, the metrics are:\n",
    "\n",
    "- **Accuracy**: Proportion of correct predictions\n",
    "- **Precision**: Of all positive predictions, how many were correct\n",
    "- **Recall**: Of all actual positives, how many were detected\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **Accuracy Standard Error**: Statistical uncertainty in accuracy estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "# View all metrics for the first result\n",
    "if results:\n",
    "    first_metrics = list(results.values())[0]\n",
    "    # Exclude trial_scores for cleaner output\n",
    "    metrics_dict = {k: v for k, v in asdict(first_metrics).items() if k != 'trial_scores'}\n",
    "    \n",
    "    print(\"All metrics:\")\n",
    "    for key, value in metrics_dict.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ceaa5",
   "metadata": {},
   "source": [
    "## Checking Existing Metrics from Registry\n",
    "\n",
    "The PyRIT team maintains a registry of scorer evaluation results on official datasets.\n",
    "You can check if your scorer configuration has been evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that retrieves all objective scorer metrics and finds the top accuracy scorer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
