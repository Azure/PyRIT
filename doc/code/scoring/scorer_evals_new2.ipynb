{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3ad689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found default environment files: ['C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env', 'C:\\\\Users\\\\rlundeen\\\\.pyrit\\\\.env.local']\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env\n",
      "Loaded environment file: C:\\Users\\rlundeen\\.pyrit\\.env.local\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import SelfAskRefusalScorer\n",
    "from pyrit.setup import IN_MEMORY, initialize_pyrit_async\n",
    "\n",
    "await initialize_pyrit_async(memory_db_type=IN_MEMORY)\n",
    "target = OpenAIChatTarget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f3405",
   "metadata": {},
   "source": [
    "## Running a Scorer Evaluation\n",
    "\n",
    "The simplest way to evaluate a scorer is to call `evaluate_async()` on the scorer instance.\n",
    "This uses the scorer's configured `evaluation_file_mapping` to find the appropriate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and evaluate a refusal scorer\n",
    "refusal_scorer = SelfAskRefusalScorer(chat_target=target)\n",
    "\n",
    "# Run evaluation - uses the scorer's default evaluation_file_mapping\n",
    "results = await refusal_scorer.evaluate_async(num_scorer_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results is a dict mapping dataset name to metrics\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"  Accuracy: {metrics.accuracy:.2%}\")\n",
    "    print(f\"  Precision: {metrics.precision:.3f}\")\n",
    "    print(f\"  Recall: {metrics.recall:.3f}\")\n",
    "    print(f\"  F1 Score: {metrics.f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815892b",
   "metadata": {},
   "source": [
    "## Understanding Evaluation File Mapping\n",
    "\n",
    "Each scorer can define an `evaluation_file_mapping` that specifies which CSV datasets to use.\n",
    "This is a list of `ScorerEvalDatasetFiles` objects that map glob patterns to result files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b52277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the scorer's configured evaluation file mapping\n",
    "print(\"Refusal scorer's evaluation_file_mapping:\")\n",
    "for mapping in refusal_scorer.evaluation_file_mapping:\n",
    "    print(f\"  Datasets: {mapping.human_labeled_datasets_files}\")\n",
    "    print(f\"  Result file: {mapping.result_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba45ff",
   "metadata": {},
   "source": [
    "## Running with Multiple Trials\n",
    "\n",
    "To measure scorer consistency, run multiple trials. The `trial_scores` array contains\n",
    "the raw scores from each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea044181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with multiple trials to measure scorer variance\n",
    "results = await refusal_scorer.evaluate_async(num_scorer_trials=3)\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"  Accuracy: {metrics.accuracy:.2%}\")\n",
    "    \n",
    "    # Access raw trial scores\n",
    "    if metrics.trial_scores is not None:\n",
    "        print(f\"  Trial scores shape: {metrics.trial_scores.shape}\")\n",
    "        print(f\"  First response across trials: {metrics.trial_scores[:, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b6d79",
   "metadata": {},
   "source": [
    "## Using Custom File Mappings\n",
    "\n",
    "You can override the default evaluation by providing a custom `file_mapping`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.score.scorer_evaluation import ScorerEvalDatasetFiles\n",
    "\n",
    "# Define custom file mapping\n",
    "custom_mapping = [\n",
    "    ScorerEvalDatasetFiles(\n",
    "        human_labeled_datasets_files=[\"refusal_scorer/mini_refusal.csv\"],\n",
    "        result_file=\"mini_refusal_results.jsonl\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run with custom mapping\n",
    "results = await refusal_scorer.evaluate_async(\n",
    "    file_mapping=custom_mapping,\n",
    "    num_scorer_trials=1\n",
    ")\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Accuracy = {metrics.accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9ada1",
   "metadata": {},
   "source": [
    "## Checking Existing Metrics from Registry\n",
    "\n",
    "The PyRIT team maintains a registry of scorer evaluation results on official datasets.\n",
    "You can check if your scorer configuration has been evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9028ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this scorer has metrics in the registry\n",
    "existing_metrics = refusal_scorer.get_scorer_metrics()\n",
    "\n",
    "if existing_metrics:\n",
    "    print(\"Found existing metrics for this scorer configuration:\")\n",
    "    for name, metrics in existing_metrics.items():\n",
    "        print(f\"  {name}: Accuracy = {metrics.accuracy:.2%}\")\n",
    "else:\n",
    "    print(\"No existing metrics found in registry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961af9d",
   "metadata": {},
   "source": [
    "## Browsing All Registered Evaluations\n",
    "\n",
    "You can browse all registered scorer evaluations to compare performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.score.scorer_evaluation.scorer_metrics_io import load_all_objective_metrics\n",
    "\n",
    "# Load all registered objective scorer evaluations\n",
    "all_entries = load_all_objective_metrics()\n",
    "\n",
    "print(f\"Total registered evaluations: {len(all_entries)}\")\n",
    "\n",
    "# Display summary of each entry\n",
    "for entry in all_entries[:5]:  # Show first 5\n",
    "    scorer_type = entry.get('__type__', 'Unknown')\n",
    "    metrics = entry.get('metrics', {})\n",
    "    accuracy = metrics.get('accuracy', 'N/A')\n",
    "    \n",
    "    if isinstance(accuracy, float):\n",
    "        print(f\"  {scorer_type}: {accuracy:.2%}\")\n",
    "    else:\n",
    "        print(f\"  {scorer_type}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26367ed3",
   "metadata": {},
   "source": [
    "## Using ScorerEvaluator Directly\n",
    "\n",
    "For more control, you can use `ScorerEvaluator` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrit.score import ScorerEvaluator\n",
    "\n",
    "# Create evaluator from scorer\n",
    "evaluator = ScorerEvaluator.from_scorer(refusal_scorer)\n",
    "\n",
    "# Run evaluation with file mappings\n",
    "results = await evaluator.run_evaluation_async(\n",
    "    dataset_files=custom_mapping,\n",
    "    num_scorer_trials=1,\n",
    "    add_to_registry=False  # Set True only for official evaluations\n",
    ")\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Accuracy = {metrics.accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5ea0e",
   "metadata": {},
   "source": [
    "## Understanding the Metrics\n",
    "\n",
    "For objective (true/false) scorers, the metrics are:\n",
    "\n",
    "- **Accuracy**: Proportion of correct predictions\n",
    "- **Precision**: Of all positive predictions, how many were correct\n",
    "- **Recall**: Of all actual positives, how many were detected\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **Accuracy Standard Error**: Statistical uncertainty in accuracy estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "# View all metrics for the first result\n",
    "if results:\n",
    "    first_metrics = list(results.values())[0]\n",
    "    # Exclude trial_scores for cleaner output\n",
    "    metrics_dict = {k: v for k, v in asdict(first_metrics).items() if k != 'trial_scores'}\n",
    "    \n",
    "    print(\"All metrics:\")\n",
    "    for key, value in metrics_dict.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ae5b7",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Start small**: Use `mini_*.csv` datasets for quick iteration\n",
    "2. **Multiple trials**: Use `num_scorer_trials=3` or higher for production evaluations\n",
    "3. **Check registry first**: Use `get_scorer_metrics()` before running expensive evaluations\n",
    "4. **Compare configurations**: Evaluate different models/prompts to find optimal settings\n",
    "5. **Analyze trial_scores**: Identify inconsistent examples where the scorer disagrees with itself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

