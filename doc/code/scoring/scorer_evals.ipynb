{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Scorer Evaluations\n",
    "This notebook demonstrates how to retrieve metrics for a Scorer, as well as how to run evaluations on a Scorer using a dataset of sample assistant responses and manual human scores. For now, this is only implemented for the `SelfAskLikertScorer`, specifically for the `hate_speech` and `violence` scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Running evaluation on a Scorer\n",
    "The following cell demonstrates how to run a custom evaluation on a `Scorer` using a dataset of LLM assistant responses and human-labeled scores. In the following example, we use a small sample `hate_speech` dataset of assistant responses and human scores to demonstrate configuration and running of the evaluation on the `SelfAskLikertScorer` using the PyRIT `HATE_SPEECH_SCALE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HarmScorerMetrics(mean_absolute_error=0.1388888888888889, mae_standard_error=0.02683588286331377, t_statistic=-1.1262576862507647, p_value=0.2789925067396749, krippendorff_alpha_combined=0.6768534283903045, krippendorff_alpha_humans=0.6594786524638434, krippendorff_alpha_model=None, type='harm')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
    "from pyrit.common.path import (\n",
    "    SCORER_EVALS_PATH,\n",
    ")\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import LikertScalePaths, ScorerEvaluator, SelfAskLikertScorer\n",
    "\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
    "\n",
    "target = OpenAIChatTarget()\n",
    "likert_scorer = SelfAskLikertScorer(chat_target=target, likert_scale_path=LikertScalePaths.HATE_SPEECH_SCALE.value)\n",
    "\n",
    "evaluator = ScorerEvaluator(scorer=likert_scorer)\n",
    "csv_path = f\"{str(SCORER_EVALS_PATH)}/eval_datasets/likert_hate_speech_sample_dataset.csv\"\n",
    "metrics = await evaluator.run_evaluation_from_csv_async(\n",
    "    csv_path=csv_path, type=\"harm\", assistant_response_col=\"assistant_response\", gold_label_col_names=[\"human_likert_score_1\", \"human_likert_score_2\", \"human_likert_score_3\"], top_level_harm=\"hate_speech\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HarmScorerMetrics(mean_absolute_error=0.1388888888888889, mae_standard_error=0.02683588286331377, t_statistic=-1.1262576862507647, p_value=0.2789925067396749, krippendorff_alpha_combined=0.6768534283903045, krippendorff_alpha_humans=0.6594786524638434, krippendorff_alpha_model=None, type='harm')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Either work for fetching the hate_speech metrics\n",
    "evaluator.get_scorer_metrics(\"hate_speech\")\n",
    "likert_scorer.get_scorer_metrics(file_name = \"hate_speech\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
