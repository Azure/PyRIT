{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Scorer Evaluations\n",
    "This notebook demonstrates how to retrieve metrics for a Scorer, as well as how to run evaluations on a Scorer using a dataset of sample assistant responses and manual human scores. For now, this is only implemented for the `SelfAskLikertScorer`, specifically for the `hate_speech` and `violence` scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Running evaluation on a Scorer\n",
    "The following cell demonstrates how to run a custom evaluation on a `Scorer` using a dataset of LLM assistant responses and human-labeled scores. In the following example, we use a small sample `hate_speech` dataset of assistant responses and human scores to demonstrate configuration and running of the evaluation on the `SelfAskLikertScorer` using the PyRIT `HATE_SPEECH_SCALE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ScorerEvaluator(scorer\u001b[38;5;241m=\u001b[39mlikert_scorer)\n\u001b[0;32m     14\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(SCORER_EVALS_PATH)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/eval_datasets/likert_hate_speech_sample_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluator\u001b[38;5;241m.\u001b[39mrun_evaluation_from_csv_async(\n\u001b[0;32m     16\u001b[0m     csv_path\u001b[38;5;241m=\u001b[39mcsv_path, dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhate_speech\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mharm\u001b[39m\u001b[38;5;124m\"\u001b[39m, assistant_response_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_response\u001b[39m\u001b[38;5;124m\"\u001b[39m, human_label_col_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_likert_score_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_likert_score_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_likert_score_3\u001b[39m\u001b[38;5;124m\"\u001b[39m], objective_or_harm_col_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, scorer_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m metrics\n",
      "File \u001b[1;32m~\\Documents\\PyRIT Clone\\PyRIT-internal\\PyRIT\\pyrit\\score\\scorer_evaluator.py:251\u001b[0m, in \u001b[0;36mScorerEvaluator.run_evaluation_from_csv_async\u001b[1;34m(self, csv_path, dataset_name, type, assistant_response_col, human_label_col_names, objective_or_harm_col_name, scorer_trials, save_results)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_evaluation_from_csv_async\u001b[39m(\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    231\u001b[0m     csv_path: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m     save_results: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScorerMetrics:\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    Run the evaluation for the scorer/policy combination on the passed in CSV file.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m        ScorerMetrics: The metrics for the scorer.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     labeled_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHumanLabeledDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43massistant_responses_col_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_response_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhuman_label_col_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuman_label_col_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective_or_harm_col_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_or_harm_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_evaluation_async(\n\u001b[0;32m    260\u001b[0m         labeled_dataset\u001b[38;5;241m=\u001b[39mlabeled_dataset, scorer_trials\u001b[38;5;241m=\u001b[39mscorer_trials, save_results\u001b[38;5;241m=\u001b[39msave_results\n\u001b[0;32m    261\u001b[0m     )\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[1;32m~\\Documents\\PyRIT Clone\\PyRIT-internal\\PyRIT\\pyrit\\score\\human_labeled_dataset.py:163\u001b[0m, in \u001b[0;36mHumanLabeledDataset.from_csv\u001b[1;34m(cls, csv_path, dataset_name, type, assistant_responses_col_name, human_label_col_names, objective_or_harm_col_name, assistant_responses_data_type_col_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m request_responses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    156\u001b[0m     PromptRequestResponse(\n\u001b[0;32m    157\u001b[0m         request_pieces\u001b[38;5;241m=\u001b[39m[PromptRequestPiece(role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m ]\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mharm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 163\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_harm_entry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_responses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_responses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mharm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_or_harm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhuman_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuman_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m    entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_objective_entry(\n\u001b[0;32m    170\u001b[0m         request_responses\u001b[38;5;241m=\u001b[39mrequest_responses,\n\u001b[0;32m    171\u001b[0m         objective\u001b[38;5;241m=\u001b[39mobjective_or_harm,\n\u001b[0;32m    172\u001b[0m         human_scores\u001b[38;5;241m=\u001b[39mhuman_scores,\n\u001b[0;32m    173\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\PyRIT Clone\\PyRIT-internal\\PyRIT\\pyrit\\score\\human_labeled_dataset.py:216\u001b[0m, in \u001b[0;36mHumanLabeledDataset._construct_harm_entry\u001b[1;34m(request_responses, harm, human_scores)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m harm:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHarm category missing for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_responses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 216\u001b[0m float_scores \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhuman_scores\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HarmHumanLabeledEntry(request_responses, float_scores, harm)\n",
      "File \u001b[1;32m~\\Documents\\PyRIT Clone\\PyRIT-internal\\PyRIT\\pyrit\\score\\human_labeled_dataset.py:216\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m harm:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHarm category missing for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_responses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 216\u001b[0m float_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m human_scores]\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HarmHumanLabeledEntry(request_responses, float_scores, harm)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
    "from pyrit.common.path import (\n",
    "    SCORER_EVALS_PATH,\n",
    ")\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score import LikertScalePaths, ScorerEvaluator, SelfAskLikertScorer\n",
    "\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
    "\n",
    "target = OpenAIChatTarget()\n",
    "likert_scorer = SelfAskLikertScorer(chat_target=target, likert_scale_path=LikertScalePaths.HATE_SPEECH_SCALE.value)\n",
    "\n",
    "evaluator = ScorerEvaluator(scorer=likert_scorer)\n",
    "csv_path = f\"{str(SCORER_EVALS_PATH)}/eval_datasets/likert_hate_speech_sample_dataset.csv\"\n",
    "metrics = await evaluator.run_evaluation_from_csv_async(\n",
    "    csv_path=csv_path, dataset_name = \"hate_speech\", type=\"harm\", assistant_response_col=\"assistant_response\", human_label_col_names=[\"human_likert_score_1\", \"human_likert_score_2\", \"human_likert_score_3\"], objective_or_harm_col_name= \"category\", scorer_trials=1)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HarmScorerMetrics(mean_absolute_error=0.13611111111111113, mae_standard_error=0.029779712485518802, t_statistic=-1.7335802125534576, p_value=0.10494893238493287, krippendorff_alpha_combined=0.6825948594921006, krippendorff_alpha_humans=0.6594786524638434, krippendorff_alpha_model=0.9182385229540918, type='harm')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Either work for fetching the hate_speech metrics\n",
    "evaluator.get_scorer_metrics(metrics_type = \"harm\", dataset_name = \"hate_speech\")\n",
    "likert_scorer.get_scorer_metrics(dataset_name = \"hate_speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d5c9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "any(np.isnan([1, 2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrit-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
