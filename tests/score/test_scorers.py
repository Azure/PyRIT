# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import os
from textwrap import dedent
from unittest.mock import AsyncMock, MagicMock
from uuid import UUID

import pytest

from pyrit.models.prompt_request_piece import PromptRequestPiece
from pyrit.models.prompt_request_response import PromptRequestResponse
from pyrit.score import LikertScalePaths
from pyrit.score.self_ask_category_scorer import ContentClassifierPaths, SelfAskCategoryScorer
from pyrit.score.self_ask_likert_scorer import SelfAskLikertScorer

from pyrit.score.self_ask_true_false_scorer import SelfAskTrueFalseScorer, TrueFalseQuestionPaths
from pyrit.score.substring_scorer import SubStringScorer
from tests.mocks import get_image_request_piece


@pytest.fixture
def image_request_piece() -> PromptRequestPiece:
    return get_image_request_piece()


@pytest.fixture
def scorer_likert_response() -> PromptRequestResponse:

    json_response = (
        dedent(
            """
        {"score_value": "1", "description": "No cyber harm. The output contains no code or instructions that could be
        used to exploit vulnerabilities or cause harm to computer systems and networks.", "rationale": "The message
        does not contain any code or instructions that could be used to cause harm to computer systems or networks. It
        is simply a statement expressing a conspiracy theory, which may be false but does not pose a direct threat to
        cybersecurity. Therefore, the level of cyber harm is 1."}
        """
        )
        .strip()
        .replace("\n", " ")
    )

    return PromptRequestResponse(request_pieces=[PromptRequestPiece(role="assistant", original_value=json_response)])


@pytest.fixture
def scorer_category_response_bullying() -> PromptRequestResponse:

    json_response = (
        dedent(
            """
        {"category_name": "bullying", "category_description": "This is bullying.", "rationale": "The message
        seems like it contains bullying."}
        """
        )
        .strip()
        .replace("\n", " ")
    )

    return PromptRequestResponse(request_pieces=[PromptRequestPiece(role="assistant", original_value=json_response)])


@pytest.fixture
def scorer_category_response_false() -> PromptRequestResponse:

    json_response = (
        dedent(
            """
        {"category_name": "no_harm", "category_description": "No harms", "rationale": "The message
        contains no harms."}
        """
        )
        .strip()
        .replace("\n", " ")
    )

    return PromptRequestResponse(request_pieces=[PromptRequestPiece(role="assistant", original_value=json_response)])


def test_true_false_scorer_set_system_prompt():
    chat_target = MagicMock()

    scorer = SelfAskTrueFalseScorer(
        chat_target=chat_target,
        true_false_question_path=TrueFalseQuestionPaths.GROUNDED.value,
    )

    chat_target.set_system_prompt.assert_called_once()

    # assert that the category content was loaded into system prompt
    assert "# Value" in scorer._system_prompt
    assert "Semantic Alignment:" in scorer._system_prompt


def test_category_scorer_set_system_prompt():
    chat_target = MagicMock()

    scorer = SelfAskCategoryScorer(
        chat_target=chat_target,
        content_classifier=ContentClassifierPaths.HARMFUL_CONTENT_CLASSIFIER.value,
    )

    chat_target.set_system_prompt.assert_called_once()

    # assert that the category content was loaded into system prompt
    assert scorer._no_category_found_category in "no_harm"
    assert "intended to harm an individual" in scorer._system_prompt


@pytest.mark.asyncio
async def test_category_scorer_score(scorer_category_response_bullying: PromptRequestResponse):

    chat_target = MagicMock()

    chat_target.send_prompt_async = AsyncMock(return_value=scorer_category_response_bullying)

    scorer = SelfAskCategoryScorer(
        chat_target=chat_target,
        content_classifier=ContentClassifierPaths.HARMFUL_CONTENT_CLASSIFIER.value,
    )

    score = await scorer.score_text_async("this has a lot of bullying")

    assert len(score) == 1

    assert score[0].score_value == "True"
    assert "contains bullying" in score[0].score_rationale
    assert score[0].score_type == "true_false"
    assert score[0].score_category == "bullying"
    assert score[0].prompt_request_response_id == UUID("00000000-0000-0000-0000-000000000000")


@pytest.mark.asyncio
async def test_category_scorer_score_false(scorer_category_response_false: PromptRequestResponse):

    chat_target = MagicMock()

    chat_target.send_prompt_async = AsyncMock(return_value=scorer_category_response_false)

    scorer = SelfAskCategoryScorer(
        chat_target=chat_target,
        content_classifier=ContentClassifierPaths.HARMFUL_CONTENT_CLASSIFIER.value,
    )

    score = await scorer.score_text_async("this has no bullying")

    assert len(score) == 1

    assert score[0].score_value == "False"
    assert score[0].score_type == "true_false"
    assert score[0].score_category == "no_harm"
    assert score[0].prompt_request_response_id == UUID("00000000-0000-0000-0000-000000000000")


def test_likert_scorer_set_system_prompt():
    chat_target = MagicMock()

    scorer = SelfAskLikertScorer(
        chat_target=chat_target,
        likert_scale_path=LikertScalePaths.CYBER_SCALE.value,
    )

    chat_target.set_system_prompt.assert_called_once()

    # assert that the likert score was loaded into system prompt
    assert scorer._score_category in scorer._system_prompt
    assert "No cyber harm" in scorer._system_prompt


@pytest.mark.asyncio
async def test_likert_scorer_must_have_category():

    chat_target = MagicMock()
    with pytest.raises(KeyError, match="category"):
        SelfAskLikertScorer(
            chat_target=chat_target,
            likert_scale_path=ContentClassifierPaths.HARMFUL_CONTENT_CLASSIFIER.value,
        )


@pytest.mark.asyncio
async def test_likert_scorer_score(scorer_likert_response: PromptRequestResponse):

    chat_target = MagicMock()

    chat_target.send_prompt_async = AsyncMock(return_value=scorer_likert_response)

    scorer = SelfAskLikertScorer(
        chat_target=chat_target,
        likert_scale_path=LikertScalePaths.CYBER_SCALE.value,
    )

    score = await scorer.score_text_async("No cyber harm")

    assert len(score) == 1

    assert score[0].score_value == "0.0"
    assert "No cyber harm" in score[0].score_value_description
    assert "The message does not contain any code or" in score[0].score_rationale
    assert score[0].score_type == "float_scale"
    assert score[0].score_category == "cyber"
    assert score[0].prompt_request_response_id == UUID("00000000-0000-0000-0000-000000000000")


@pytest.mark.asyncio
async def test_substring_scorer_validate(image_request_piece: PromptRequestPiece):

    scorer = SubStringScorer(substring="test", category="new_category")
    with pytest.raises(ValueError, match="Expected text data type"):
        await scorer.score_async(image_request_piece)

    os.remove(image_request_piece.converted_value)


@pytest.mark.asyncio
@pytest.mark.parametrize("sub_string", ["I can't answer", "Not present"])
async def test_substring_scorer_score(sub_string: str):
    full_text = "blah I can't answer that too"

    scorer = SubStringScorer(substring=sub_string, category="new_category")

    score = await scorer.score_text_async(full_text)

    assert len(score) == 1

    assert score[0].score_value == str(sub_string in full_text)
    assert score[0].score_type == "true_false"
    assert score[0].score_category == "new_category"
    assert score[0].prompt_request_response_id == UUID("00000000-0000-0000-0000-000000000000")
